{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cacf45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Quantum States\n",
    "\n",
    "import numpy as np\n",
    "import qutip as qt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pure_state():\n",
    "    eps = 1e-7\n",
    "    rand_ket = qt.rand_ket_haar(4)\n",
    "    rho_p = qt.ket2dm(qt.Qobj(rand_ket))\n",
    "    rho_p = (1-eps)*rho_p + (eps/4)*np.eye(4)\n",
    "    return np.array(rho_p)\n",
    "\n",
    "def mixed_state():\n",
    "    G = np.random.normal(0, 1, [4,4]) + 1j*np.random.normal(0, 1, [4,4])\n",
    "    G = np.matrix(G)\n",
    "    rho_m = (G*G.H)/np.trace(G*G.H)\n",
    "    return rho_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea72639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate Tomography Measurements\n",
    "\n",
    "#Noisy Measurement Projectors Matrix\n",
    "\n",
    "def U_noise(x, var):\n",
    "    \n",
    "    \"\"\" x = 2x2 quantum stat\n",
    "        var = scalar value. High var = high noise in state to be measured.\n",
    "    \"\"\"\n",
    "    a, b, c = np.random.normal(0, var, 3)\n",
    "    U = np.matrix(np.array([[np.exp(1j*b/2)*np.cos(a), -1j*np.exp(1j*c)*np.sin(a)],\n",
    "                  [-1j*np.exp(-1j*c)*np.sin(a), np.exp(-1j*b/2)*np.cos(a)]]))\n",
    "    return U.dot(x).dot(U.H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f9d1ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Simulate Tomography Measruements\n",
    "\n",
    "# state_N = np.tile(state, (36,1)).reshape(36,4,4)\n",
    "# st_mul_PN = [np.matmul(state, P_N[x]) for x in range(36)]\n",
    "\n",
    "def meas(state, P_N):\n",
    "    #state = 4x4\n",
    "    #P_N = 36x4x4\n",
    "    #returns trace of measurements M = 6x6\n",
    "    \n",
    "    st_mul_PN = np.zeros([36,4,4], dtype='complex')\n",
    "    for jk in range(36):\n",
    "        st_mul_PN[jk] = np.matmul(state, P_N[jk])\n",
    "\n",
    "    M = np.array([np.trace(st_mul_PN[x]) for x in range(36)], dtype='complex').reshape(6,6)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0f93a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stokes Reconstruction\n",
    "\n",
    "def stokes_recon(M):\n",
    "    s00 = M[0,0] + M[0,1] + M[0,3] + M[0,2] \n",
    "    s11 = M[2,3] - M[2,2] - M[2,0] + M[2,1]\n",
    "    s12 = M[2,3] - M[2,5] - M[3,1] + M[3,0]\n",
    "    s13 = M[3,5] - M[3,4] - M[3,2] + M[3,3]\n",
    "    s21 = M[5,2] - M[5,3] - M[5,5] + M[5,4]\n",
    "    s22 = M[5,1] - M[5,0] - M[4,4] + M[4,5]\n",
    "    s23 = M[4,0] - M[4,1] - M[4,3] + M[4,2]\n",
    "    s31 = M[1,2] - M[1,3] - M[1,5] + M[1,4]\n",
    "    s32 = M[1,1] - M[1,0] - M[0,4] + M[0,5]\n",
    "    s33 = M[0,0] - M[0,1] - M[0,3] + M[0,2]\n",
    "    s01 = M[2,3] - M[2,2] + M[2,0] - M[2,1]\n",
    "    s02 = M[5,1] + M[4,4] - M[5,0] - M[4,5]\n",
    "    s03 = M[0,0] - M[0,1] + M[0,3] - M[0,2]\n",
    "    s10 = M[2,3] + M[2,2] - M[2,0] - M[2,1]\n",
    "    s20 = M[5,1] - M[4,4] + M[5,0] - M[4,5]\n",
    "    s30 = M[0,0] + M[0,1] - M[0,3] - M[0,2]\n",
    "\n",
    "    I, X, Y, Z = qt.qeye(2), qt.sigmax(), qt.sigmay(), qt.sigmaz()\n",
    "\n",
    "    rho_est = (s00*np.kron(I,I) + s01*np.kron(I,X) +\n",
    "               s02*np.kron(I,Y) + s03*np.kron(I,Z) +\n",
    "               s10*np.kron(X,I) + s20*np.kron(Y,I) +\n",
    "               s30*np.kron(Z,I) + s11*np.kron(X,X) +\n",
    "               s12*np.kron(X,Y) + s13*np.kron(X,Z) +\n",
    "               s21*np.kron(Y,X) + s22*np.kron(Y,Y) +\n",
    "               s23*np.kron(Y,Z) + s31*np.kron(Z,X) +\n",
    "               s32*np.kron(Z,Y) + s33*np.kron(Z,Z)).reshape(4,4)/4\n",
    "\n",
    "    return rho_est\n",
    "# print(\"State = {} \\n\\nEstimate = {} \\n\\nFidelity = {}\".format(\n",
    "#     qt.Qobj(state),\n",
    "#     qt.Qobj(rho_est),\n",
    "#     qt.fidelity(qt.Qobj(rho_est), qt.Qobj(state))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca7a3df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1be6e2ec070>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#complete tomogrpahy using stokes params\n",
    "\n",
    "state = mixed_state()#Instantiating State\n",
    "\n",
    "F = np.zeros([10, 10])\n",
    "\n",
    "for jk in range(10):    \n",
    "    var = 0.01*jk\n",
    "    for ij in range(10):\n",
    "        P_N = noisy_meas_proj(var) #36 meas projectors with var\n",
    "        M = meas(state, P_N) #6x6 matrix of trace of projectors and state\n",
    "        est = stokes_recon(M) #stokes reconstruction\n",
    "        F[jk, ij] = qt.fidelity(qt.Qobj(state), qt.Qobj(est))\n",
    "    \n",
    "plt.plot(np.mean(F, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "341c7673",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation*}\\left(\\begin{array}{*{11}c}0.015 & (0.006+0.016j) & (0.042-0.113j) & (-0.005-0.004j)\\\\(0.006-0.016j) & 0.020 & (-0.105-0.089j) & (-0.006+0.004j)\\\\(0.042+0.113j) & (-0.105+0.089j) & 0.963 & (0.014-0.044j)\\\\(-0.005+0.004j) & (-0.006-0.004j) & (0.014+0.044j) & 0.002\\\\\\end{array}\\right)\\end{equation*}"
      ],
      "text/plain": [
       "Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\n",
       "Qobj data =\n",
       "[[ 0.01496033+0.j          0.00584247+0.01613467j  0.04160146-0.11259451j\n",
       "  -0.00457873-0.00358958j]\n",
       " [ 0.00584247-0.01613467j  0.01968285+0.j         -0.10518618-0.08883866j\n",
       "  -0.00565949+0.00353631j]\n",
       " [ 0.04160146+0.11259451j -0.10518618+0.08883866j  0.96309418+0.j\n",
       "   0.01428346-0.04444233j]\n",
       " [-0.00457873+0.00358958j -0.00565949-0.00353631j  0.01428346+0.04444233j\n",
       "   0.00226264+0.j        ]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.96752164e-05,  8.53588857e-05,  0.00000000e+00,  4.75672156e-02,\n",
       "       -3.85626849e-05, -1.28117884e-05, -1.15025441e-04, -2.08435766e-04,\n",
       "        3.00279506e-01,  9.34305896e-01, -8.80708436e-05,  2.71332722e-05,\n",
       "       -1.18978795e-01, -7.43434308e-02, -9.62581043e-02,  7.54633197e-02])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#targets states\n",
    "\n",
    "# state = mixed_state()\n",
    "\n",
    "state = np.matrix([[0.01496033+0.j, 0.00584247+0.01613467j, 0.04160146-0.11259451j, -0.00457873-0.00358958j],\n",
    "                   [ 0.00584247-0.01613467j, 0.01968285+0.j, -0.10518618-0.08883866j, -0.00565949+0.00353631j],\n",
    "                   [ 0.04160146+0.11259451j, -0.10518618+0.08883866j, 0.96309418+0.j, 0.01428346-0.04444233j],\n",
    "                   [-0.00457873+0.00358958j, -0.00565949-0.00353631j, 0.01428346+0.04444233j, 0.00226264+0.j]])\n",
    "display(qt.Qobj(state))\n",
    "\n",
    "def find_tau(state):\n",
    "    m_00 = np.linalg.det(state[1:,1:])\n",
    "    m_01 = np.linalg.det(np.delete(state[1:,:], slice(1,2), axis=1))\n",
    "\n",
    "    m_00_11 = np.linalg.det(state[2:,2:])\n",
    "    m_00_12 = np.linalg.det(np.delete(state[2:,:], slice(0,3,2), axis=1))\n",
    "    m_01_12 = np.linalg.det(np.delete(state[2:,:], slice(1,3), axis=1))\n",
    "\n",
    "    tau = np.zeros(16, dtype = 'complex')\n",
    "\n",
    "    tau[0] = np.sqrt(np.linalg.det(state)/m_00)\n",
    "    tau[1] = np.sqrt(m_00/m_00_11)\n",
    "    tau[2] = np.sqrt(m_00_11/state[3,3])\n",
    "    tau[3] = np.sqrt(state[3,3])\n",
    "    tau[4] = (m_01/np.sqrt(m_00*m_00_11)).real\n",
    "    tau[5] = (m_01/np.sqrt(m_00*m_00_11)).imag\n",
    "    tau[6] = (m_00_12/np.sqrt(state[3,3]*m_00_11)).real\n",
    "    tau[7] = (m_00_12/np.sqrt(state[3,3]*m_00_11)).imag\n",
    "    tau[8] = (state[3,2]/np.sqrt(state[3,3])).real\n",
    "    tau[9] = (state[3,2]/np.sqrt(state[3,3])).imag\n",
    "    tau[10] = (m_01_12/np.sqrt(state[3,3]*m_00_11)).real\n",
    "    tau[11] = (m_01_12/np.sqrt(state[3,3]*m_00_11)).imag\n",
    "    tau[12] = (state[3,1]/np.sqrt(state[3,3])).real\n",
    "    tau[13] = (state[3,1]/np.sqrt(state[3,3])).imag\n",
    "    tau[14] = (state[3,0]/np.sqrt(state[3,3])).real\n",
    "    tau[15] = (state[3,0]/np.sqrt(state[3,3])).imag\n",
    "    \n",
    "    return tau\n",
    "find_tau(state).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4513f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation*}\\left(\\begin{array}{*{11}c}0.015 & (0.006+0.016j) & (0.042-0.113j) & (-0.005-0.004j)\\\\(0.006-0.016j) & 0.020 & (-0.105-0.089j) & (-0.006+0.004j)\\\\(0.042+0.113j) & (-0.105+0.089j) & 0.963 & (0.014-0.044j)\\\\(-0.005+0.004j) & (-0.006-0.004j) & (0.014+0.044j) & 0.002\\\\\\end{array}\\right)\\end{equation*}"
      ],
      "text/plain": [
       "Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\n",
       "Qobj data =\n",
       "[[ 0.01496033+0.j          0.00584247+0.01613467j  0.04160146-0.11259451j\n",
       "  -0.00457873-0.00358958j]\n",
       " [ 0.00584247-0.01613467j  0.01968285+0.j         -0.10518618-0.08883866j\n",
       "  -0.00565949+0.00353631j]\n",
       " [ 0.04160146+0.11259451j -0.10518618+0.08883866j  0.96309418+0.j\n",
       "   0.01428346-0.04444233j]\n",
       " [-0.00457873+0.00358958j -0.00565949-0.00353631j  0.01428346+0.04444233j\n",
       "   0.00226264+0.j        ]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation*}\\left(\\begin{array}{*{11}c}0.015 & (0.006+0.016j) & (0.042-0.113j) & (-0.005-0.004j)\\\\(0.006-0.016j) & 0.020 & (-0.105-0.089j) & (-0.006+0.004j)\\\\(0.042+0.113j) & (-0.105+0.089j) & 0.963 & (0.014-0.044j)\\\\(-0.005+0.004j) & (-0.006-0.004j) & (0.014+0.044j) & 0.002\\\\\\end{array}\\right)\\end{equation*}"
      ],
      "text/plain": [
       "Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\n",
       "Qobj data =\n",
       "[[ 0.01496031+0.j          0.00584246+0.01613468j  0.04160142-0.11259443j\n",
       "  -0.00457872-0.00358957j]\n",
       " [ 0.00584246-0.01613468j  0.01968292+0.j         -0.10518637-0.08883869j\n",
       "  -0.00565948+0.0035363j ]\n",
       " [ 0.04160142+0.11259443j -0.10518637+0.08883869j  0.96309414+0.j\n",
       "   0.01428343-0.04444223j]\n",
       " [-0.00457872+0.00358957j -0.00565948-0.0035363j   0.01428343+0.04444223j\n",
       "   0.00226263+0.j        ]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tau = find_tau(state)\n",
    "\n",
    "def est_rho(tau):\n",
    "    t_pred = np.matrix(np.array([[tau[0], 0, 0, 0],\n",
    "                                 [tau[4] + 1j*tau[5], tau[1], 0, 0],\n",
    "                                 [tau[10] + 1j*tau[11], tau[6] + 1j*tau[7], tau[2], 0],\n",
    "                                 [tau[14] + 1j*tau[15], tau[12] + 1j*tau[13], tau[8] + 1j*tau[9], tau[3]]]))\n",
    "    \n",
    "    rho_pred = np.matmul(t_pred.H, t_pred)/np.trace(np.matmul(t_pred.H, t_pred))\n",
    "    return rho_pred\n",
    "    \n",
    "display(qt.Qobj(state), qt.Qobj(est_rho(tau)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f9e89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c806fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets\n",
    "#200 DMs for pure and mixed each\n",
    "#perform 200 noisy meaurements on each with var=pi/6\n",
    "\n",
    "M_dataset = np.zeros([200, 200, 6, 6], dtype = 'complex') #state x meas x 6x6 = input\n",
    "P_dataset = np.zeros([200, 200, 6, 6], dtype = 'complex') #state x meas x 6x6 = input\n",
    "M_label = np.zeros([200, 200, 16], dtype = 'complex')\n",
    "P_label = np.zeros([200, 200, 16], dtype = 'complex')\n",
    "var = np.pi/6\n",
    "\n",
    "for jk in range(200):\n",
    "    state = mixed_state()#Instantiating State\n",
    "    for ij in range(200):\n",
    "        P_N = noisy_meas_proj(var) #36 meas projectors with var\n",
    "        M_label[jk, ij] = find_tau(state)\n",
    "        M_dataset[jk, ij] = meas(state, P_N) #6x6 matrix of trace of projectors and state\n",
    "        \n",
    "for jk in range(200):\n",
    "    state = pure_state()#Instantiating State\n",
    "    for ij in range(200):\n",
    "        P_N = noisy_meas_proj(var) #36 meas projectors with var\n",
    "        P_label[jk, ij] = find_tau(state)\n",
    "        P_dataset[jk, ij] = meas(state, P_N) #6x6 matrix of trace of projectors and state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c65ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('M_dataset', M_dataset)\n",
    "# np.savetxt('P_dataset', P_dataset)\n",
    "\n",
    "M_ds = M_dataset.reshape(40000, 1, 6, 6).real\n",
    "P_ds = P_dataset.reshape(40000, 1, 6, 6).real\n",
    "M_l = M_label.reshape(40000, 16).real\n",
    "P_l = P_label.reshape(40000, 16).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d068a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_len = 40000\n",
    "test_size = 0.025\n",
    "indices = list(range(ds_len))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(test_size * ds_len))\n",
    "train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "M_testset = M_ds[test_idx]\n",
    "M_testlabel = M_l[test_idx]\n",
    "M_trainset = M_ds[train_idx]\n",
    "M_trainlabel = M_l[train_idx]\n",
    "\n",
    "train_len = int(ds_len*(1-test_size))\n",
    "valid_size = 0.2\n",
    "indices = list(range(train_len))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * train_len))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "M_validset = M_trainset[valid_idx]\n",
    "M_validlabel = M_trainlabel[valid_idx]\n",
    "M_trainset = M_trainset[train_idx]\n",
    "M_trainlabel = M_trainlabel[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38a00342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "#mixed_train_set\n",
    "M_trainset = torch.Tensor(M_trainset) # transform to torch tensor\n",
    "M_trainlabel = torch.tensor(M_trainlabel)\n",
    "\n",
    "#mixed_valid set\n",
    "M_validset = torch.Tensor(M_validset) # transform to torch tensor\n",
    "M_validlabel = torch.tensor(M_validlabel)\n",
    "\n",
    "#mixed_test set\n",
    "M_testset = torch.Tensor(M_testset) # transform to torch tensor\n",
    "M_testlabel = torch.tensor(M_testlabel)\n",
    "\n",
    "#datasets\n",
    "train_data = TensorDataset(M_trainset, M_trainlabel)\n",
    "valid_data = TensorDataset(M_validset, M_validlabel)\n",
    "test_data = TensorDataset(M_testset, M_testlabel)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e2300ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer (sees 6x6x1 image tensor)\n",
    "        self.conv1 = nn.Conv2d(1, 25, 3, padding=1)\n",
    "        # convolutional layer (sees 3x3x25 tensor)\n",
    "        self.conv2 = nn.Conv2d(25, 80, 3, padding=1)\n",
    "\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # linear layer (80 * 3 * 3 -> 450)\n",
    "        self.fc1 = nn.Linear(2880, 450)\n",
    "        # linear layer (450 -> 16)\n",
    "        self.fc2 = nn.Linear(450, 16)\n",
    "        # dropout layer (p=0.5)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "#         x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        # flatten input\n",
    "        x = x.view(-1, 2880)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be525afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01de8aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(25, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2880, out_features=450, bias=True)\n",
      "  (fc2): Linear(in_features=450, out_features=16, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.021071 \tValidation Loss: 0.014437\n",
      "Validation loss decreased (inf --> 0.014437).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.014221 \tValidation Loss: 0.012548\n",
      "Validation loss decreased (0.014437 --> 0.012548).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.013070 \tValidation Loss: 0.011764\n",
      "Validation loss decreased (0.012548 --> 0.011764).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.012347 \tValidation Loss: 0.011104\n",
      "Validation loss decreased (0.011764 --> 0.011104).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.011740 \tValidation Loss: 0.010542\n",
      "Validation loss decreased (0.011104 --> 0.010542).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.011265 \tValidation Loss: 0.010133\n",
      "Validation loss decreased (0.010542 --> 0.010133).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.010912 \tValidation Loss: 0.009749\n",
      "Validation loss decreased (0.010133 --> 0.009749).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.010582 \tValidation Loss: 0.009446\n",
      "Validation loss decreased (0.009749 --> 0.009446).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.010294 \tValidation Loss: 0.009176\n",
      "Validation loss decreased (0.009446 --> 0.009176).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.010044 \tValidation Loss: 0.008890\n",
      "Validation loss decreased (0.009176 --> 0.008890).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.009807 \tValidation Loss: 0.008654\n",
      "Validation loss decreased (0.008890 --> 0.008654).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.009574 \tValidation Loss: 0.008414\n",
      "Validation loss decreased (0.008654 --> 0.008414).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.009361 \tValidation Loss: 0.008197\n",
      "Validation loss decreased (0.008414 --> 0.008197).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.009159 \tValidation Loss: 0.008004\n",
      "Validation loss decreased (0.008197 --> 0.008004).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.008965 \tValidation Loss: 0.007773\n",
      "Validation loss decreased (0.008004 --> 0.007773).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.008763 \tValidation Loss: 0.007612\n",
      "Validation loss decreased (0.007773 --> 0.007612).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.008598 \tValidation Loss: 0.007400\n",
      "Validation loss decreased (0.007612 --> 0.007400).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.008410 \tValidation Loss: 0.007207\n",
      "Validation loss decreased (0.007400 --> 0.007207).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.008254 \tValidation Loss: 0.007050\n",
      "Validation loss decreased (0.007207 --> 0.007050).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.008064 \tValidation Loss: 0.006874\n",
      "Validation loss decreased (0.007050 --> 0.006874).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.007903 \tValidation Loss: 0.006742\n",
      "Validation loss decreased (0.006874 --> 0.006742).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.007770 \tValidation Loss: 0.006563\n",
      "Validation loss decreased (0.006742 --> 0.006563).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.007646 \tValidation Loss: 0.006424\n",
      "Validation loss decreased (0.006563 --> 0.006424).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.007500 \tValidation Loss: 0.006285\n",
      "Validation loss decreased (0.006424 --> 0.006285).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.007369 \tValidation Loss: 0.006143\n",
      "Validation loss decreased (0.006285 --> 0.006143).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.007234 \tValidation Loss: 0.006052\n",
      "Validation loss decreased (0.006143 --> 0.006052).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.007110 \tValidation Loss: 0.005927\n",
      "Validation loss decreased (0.006052 --> 0.005927).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.006991 \tValidation Loss: 0.005797\n",
      "Validation loss decreased (0.005927 --> 0.005797).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.006878 \tValidation Loss: 0.005693\n",
      "Validation loss decreased (0.005797 --> 0.005693).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.006798 \tValidation Loss: 0.005640\n",
      "Validation loss decreased (0.005693 --> 0.005640).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.006680 \tValidation Loss: 0.005487\n",
      "Validation loss decreased (0.005640 --> 0.005487).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.006610 \tValidation Loss: 0.005403\n",
      "Validation loss decreased (0.005487 --> 0.005403).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.006502 \tValidation Loss: 0.005326\n",
      "Validation loss decreased (0.005403 --> 0.005326).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.006428 \tValidation Loss: 0.005216\n",
      "Validation loss decreased (0.005326 --> 0.005216).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.006339 \tValidation Loss: 0.005116\n",
      "Validation loss decreased (0.005216 --> 0.005116).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.006253 \tValidation Loss: 0.005052\n",
      "Validation loss decreased (0.005116 --> 0.005052).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.006161 \tValidation Loss: 0.004987\n",
      "Validation loss decreased (0.005052 --> 0.004987).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.006101 \tValidation Loss: 0.004928\n",
      "Validation loss decreased (0.004987 --> 0.004928).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.006013 \tValidation Loss: 0.004839\n",
      "Validation loss decreased (0.004928 --> 0.004839).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.005943 \tValidation Loss: 0.004790\n",
      "Validation loss decreased (0.004839 --> 0.004790).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.005881 \tValidation Loss: 0.004709\n",
      "Validation loss decreased (0.004790 --> 0.004709).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.005820 \tValidation Loss: 0.004642\n",
      "Validation loss decreased (0.004709 --> 0.004642).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.005767 \tValidation Loss: 0.004576\n",
      "Validation loss decreased (0.004642 --> 0.004576).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.005702 \tValidation Loss: 0.004526\n",
      "Validation loss decreased (0.004576 --> 0.004526).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 0.005640 \tValidation Loss: 0.004478\n",
      "Validation loss decreased (0.004526 --> 0.004478).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.005577 \tValidation Loss: 0.004412\n",
      "Validation loss decreased (0.004478 --> 0.004412).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.005531 \tValidation Loss: 0.004357\n",
      "Validation loss decreased (0.004412 --> 0.004357).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.005462 \tValidation Loss: 0.004333\n",
      "Validation loss decreased (0.004357 --> 0.004333).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.005418 \tValidation Loss: 0.004267\n",
      "Validation loss decreased (0.004333 --> 0.004267).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.005364 \tValidation Loss: 0.004244\n",
      "Validation loss decreased (0.004267 --> 0.004244).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 0.005333 \tValidation Loss: 0.004176\n",
      "Validation loss decreased (0.004244 --> 0.004176).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.005266 \tValidation Loss: 0.004159\n",
      "Validation loss decreased (0.004176 --> 0.004159).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.005218 \tValidation Loss: 0.004105\n",
      "Validation loss decreased (0.004159 --> 0.004105).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.005160 \tValidation Loss: 0.004054\n",
      "Validation loss decreased (0.004105 --> 0.004054).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.005125 \tValidation Loss: 0.004034\n",
      "Validation loss decreased (0.004054 --> 0.004034).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 0.005094 \tValidation Loss: 0.003984\n",
      "Validation loss decreased (0.004034 --> 0.003984).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.005078 \tValidation Loss: 0.003930\n",
      "Validation loss decreased (0.003984 --> 0.003930).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 0.005030 \tValidation Loss: 0.003911\n",
      "Validation loss decreased (0.003930 --> 0.003911).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.004981 \tValidation Loss: 0.003892\n",
      "Validation loss decreased (0.003911 --> 0.003892).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.004935 \tValidation Loss: 0.003876\n",
      "Validation loss decreased (0.003892 --> 0.003876).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 \tTraining Loss: 0.004909 \tValidation Loss: 0.003809\n",
      "Validation loss decreased (0.003876 --> 0.003809).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 0.004854 \tValidation Loss: 0.003780\n",
      "Validation loss decreased (0.003809 --> 0.003780).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.004837 \tValidation Loss: 0.003756\n",
      "Validation loss decreased (0.003780 --> 0.003756).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.004799 \tValidation Loss: 0.003703\n",
      "Validation loss decreased (0.003756 --> 0.003703).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.004770 \tValidation Loss: 0.003688\n",
      "Validation loss decreased (0.003703 --> 0.003688).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 0.004745 \tValidation Loss: 0.003676\n",
      "Validation loss decreased (0.003688 --> 0.003676).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 0.004702 \tValidation Loss: 0.003653\n",
      "Validation loss decreased (0.003676 --> 0.003653).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.004681 \tValidation Loss: 0.003653\n",
      "Validation loss decreased (0.003653 --> 0.003653).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.004634 \tValidation Loss: 0.003570\n",
      "Validation loss decreased (0.003653 --> 0.003570).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.004623 \tValidation Loss: 0.003564\n",
      "Validation loss decreased (0.003570 --> 0.003564).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 0.004587 \tValidation Loss: 0.003518\n",
      "Validation loss decreased (0.003564 --> 0.003518).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 0.004551 \tValidation Loss: 0.003537\n",
      "Epoch: 73 \tTraining Loss: 0.004523 \tValidation Loss: 0.003488\n",
      "Validation loss decreased (0.003518 --> 0.003488).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.004501 \tValidation Loss: 0.003460\n",
      "Validation loss decreased (0.003488 --> 0.003460).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 0.004477 \tValidation Loss: 0.003436\n",
      "Validation loss decreased (0.003460 --> 0.003436).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 0.004434 \tValidation Loss: 0.003391\n",
      "Validation loss decreased (0.003436 --> 0.003391).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 0.004422 \tValidation Loss: 0.003375\n",
      "Validation loss decreased (0.003391 --> 0.003375).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 0.004422 \tValidation Loss: 0.003391\n",
      "Epoch: 79 \tTraining Loss: 0.004372 \tValidation Loss: 0.003374\n",
      "Validation loss decreased (0.003375 --> 0.003374).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 0.004351 \tValidation Loss: 0.003327\n",
      "Validation loss decreased (0.003374 --> 0.003327).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 0.004343 \tValidation Loss: 0.003301\n",
      "Validation loss decreased (0.003327 --> 0.003301).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 0.004288 \tValidation Loss: 0.003282\n",
      "Validation loss decreased (0.003301 --> 0.003282).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 0.004296 \tValidation Loss: 0.003277\n",
      "Validation loss decreased (0.003282 --> 0.003277).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 0.004257 \tValidation Loss: 0.003251\n",
      "Validation loss decreased (0.003277 --> 0.003251).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 0.004253 \tValidation Loss: 0.003247\n",
      "Validation loss decreased (0.003251 --> 0.003247).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 0.004238 \tValidation Loss: 0.003223\n",
      "Validation loss decreased (0.003247 --> 0.003223).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 0.004202 \tValidation Loss: 0.003203\n",
      "Validation loss decreased (0.003223 --> 0.003203).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.004187 \tValidation Loss: 0.003199\n",
      "Validation loss decreased (0.003203 --> 0.003199).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 0.004159 \tValidation Loss: 0.003190\n",
      "Validation loss decreased (0.003199 --> 0.003190).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 0.004125 \tValidation Loss: 0.003159\n",
      "Validation loss decreased (0.003190 --> 0.003159).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 0.004107 \tValidation Loss: 0.003141\n",
      "Validation loss decreased (0.003159 --> 0.003141).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 0.004106 \tValidation Loss: 0.003106\n",
      "Validation loss decreased (0.003141 --> 0.003106).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 0.004086 \tValidation Loss: 0.003104\n",
      "Validation loss decreased (0.003106 --> 0.003104).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 0.004063 \tValidation Loss: 0.003121\n",
      "Epoch: 95 \tTraining Loss: 0.004038 \tValidation Loss: 0.003103\n",
      "Validation loss decreased (0.003104 --> 0.003103).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.004021 \tValidation Loss: 0.003086\n",
      "Validation loss decreased (0.003103 --> 0.003086).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 0.004015 \tValidation Loss: 0.003060\n",
      "Validation loss decreased (0.003086 --> 0.003060).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 0.003994 \tValidation Loss: 0.003035\n",
      "Validation loss decreased (0.003060 --> 0.003035).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 0.003983 \tValidation Loss: 0.003028\n",
      "Validation loss decreased (0.003035 --> 0.003028).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 0.003967 \tValidation Loss: 0.002992\n",
      "Validation loss decreased (0.003028 --> 0.002992).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# create a complete CNN\n",
    "\n",
    "print(model)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "# model = model.float()\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            data, target = data.float(), target.float()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    \n",
    "    # At completion of epoch\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), '2020_ML_Assisted_QSE.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb2fcdd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2863190ed90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log10(train_losses), label='Training loss')\n",
    "plt.plot(np.log10(valid_losses), label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03707bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('2020_ML_Assisted_QSE.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b284495f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.005255\n",
      "Loss per Batch: 0.005239\n",
      "Loss per Batch: 0.000948\n",
      "Loss per Batch: 0.004773\n",
      "Loss per Batch: 0.004947\n",
      "Loss per Batch: 0.005166\n",
      "Loss per Batch: 0.001286\n",
      "Loss per Batch: 0.001580\n",
      "Loss per Batch: 0.001873\n",
      "Loss per Batch: 0.002459\n",
      "Loss per Batch: 0.001932\n",
      "Loss per Batch: 0.001436\n",
      "Loss per Batch: 0.001102\n",
      "Loss per Batch: 0.003874\n",
      "Loss per Batch: 0.001818\n",
      "Loss per Batch: 0.001679\n",
      "Loss per Batch: 0.002756\n",
      "Loss per Batch: 0.001516\n",
      "Loss per Batch: 0.002500\n",
      "Loss per Batch: 0.001261\n",
      "Loss per Batch: 0.001747\n",
      "Loss per Batch: 0.006640\n",
      "Loss per Batch: 0.002285\n",
      "Loss per Batch: 0.005049\n",
      "Loss per Batch: 0.001418\n",
      "Loss per Batch: 0.001205\n",
      "Loss per Batch: 0.007133\n",
      "Loss per Batch: 0.001336\n",
      "Loss per Batch: 0.003008\n",
      "Loss per Batch: 0.005505\n",
      "Loss per Batch: 0.005572\n",
      "Loss per Batch: 0.007252\n",
      "Loss per Batch: 0.002340\n",
      "Loss per Batch: 0.002874\n",
      "Loss per Batch: 0.001772\n",
      "Loss per Batch: 0.002475\n",
      "Loss per Batch: 0.003479\n",
      "Loss per Batch: 0.000566\n",
      "Loss per Batch: 0.002592\n",
      "Loss per Batch: 0.004889\n",
      "Loss per Batch: 0.000963\n",
      "Loss per Batch: 0.001279\n",
      "Loss per Batch: 0.002568\n",
      "Loss per Batch: 0.005900\n",
      "Loss per Batch: 0.000720\n",
      "Loss per Batch: 0.001296\n",
      "Loss per Batch: 0.002305\n",
      "Loss per Batch: 0.003517\n",
      "Loss per Batch: 0.001779\n",
      "Loss per Batch: 0.001393\n",
      "Loss per Batch: 0.001936\n",
      "Loss per Batch: 0.002309\n",
      "Loss per Batch: 0.003956\n",
      "Loss per Batch: 0.001775\n",
      "Loss per Batch: 0.004424\n",
      "Loss per Batch: 0.002644\n",
      "Loss per Batch: 0.003053\n",
      "Loss per Batch: 0.004243\n",
      "Loss per Batch: 0.004898\n",
      "Loss per Batch: 0.003245\n",
      "Loss per Batch: 0.007593\n",
      "Loss per Batch: 0.002071\n",
      "Loss per Batch: 0.005327\n",
      "Loss per Batch: 0.005915\n",
      "Loss per Batch: 0.007503\n",
      "Loss per Batch: 0.004784\n",
      "Loss per Batch: 0.001554\n",
      "Loss per Batch: 0.001740\n",
      "Loss per Batch: 0.004321\n",
      "Loss per Batch: 0.000905\n",
      "Loss per Batch: 0.001654\n",
      "Loss per Batch: 0.000704\n",
      "Loss per Batch: 0.001768\n",
      "Loss per Batch: 0.002421\n",
      "Loss per Batch: 0.001709\n",
      "Loss per Batch: 0.001263\n",
      "Loss per Batch: 0.006622\n",
      "Loss per Batch: 0.003546\n",
      "Loss per Batch: 0.006699\n",
      "Loss per Batch: 0.001292\n",
      "Loss per Batch: 0.002972\n",
      "Loss per Batch: 0.003316\n",
      "Loss per Batch: 0.004172\n",
      "Loss per Batch: 0.003094\n",
      "Loss per Batch: 0.005349\n",
      "Loss per Batch: 0.001692\n",
      "Loss per Batch: 0.007375\n",
      "Loss per Batch: 0.002482\n",
      "Loss per Batch: 0.008745\n",
      "Loss per Batch: 0.005029\n",
      "Loss per Batch: 0.002436\n",
      "Loss per Batch: 0.006821\n",
      "Loss per Batch: 0.004738\n",
      "Loss per Batch: 0.003291\n",
      "Loss per Batch: 0.002028\n",
      "Loss per Batch: 0.002204\n",
      "Loss per Batch: 0.003552\n",
      "Loss per Batch: 0.002812\n",
      "Loss per Batch: 0.002316\n",
      "Loss per Batch: 0.001682\n",
      "Loss per Batch: 0.000569\n",
      "Loss per Batch: 0.001705\n",
      "Loss per Batch: 0.002136\n",
      "Loss per Batch: 0.000652\n",
      "Loss per Batch: 0.006666\n",
      "Loss per Batch: 0.001958\n",
      "Loss per Batch: 0.008302\n",
      "Loss per Batch: 0.001228\n",
      "Loss per Batch: 0.003461\n",
      "Loss per Batch: 0.001887\n",
      "Loss per Batch: 0.002097\n",
      "Loss per Batch: 0.001646\n",
      "Loss per Batch: 0.005463\n",
      "Loss per Batch: 0.001756\n",
      "Loss per Batch: 0.004066\n",
      "Loss per Batch: 0.001480\n",
      "Loss per Batch: 0.002010\n",
      "Loss per Batch: 0.004672\n",
      "Loss per Batch: 0.004549\n",
      "Loss per Batch: 0.001921\n",
      "Loss per Batch: 0.002120\n",
      "Loss per Batch: 0.000873\n",
      "Loss per Batch: 0.003800\n",
      "Loss per Batch: 0.004480\n",
      "Loss per Batch: 0.001987\n",
      "Loss per Batch: 0.003215\n",
      "Loss per Batch: 0.003361\n",
      "Loss per Batch: 0.000870\n",
      "Loss per Batch: 0.003548\n",
      "Loss per Batch: 0.003565\n",
      "Loss per Batch: 0.002989\n",
      "Loss per Batch: 0.004917\n",
      "Loss per Batch: 0.001648\n",
      "Loss per Batch: 0.001921\n",
      "Loss per Batch: 0.001875\n",
      "Loss per Batch: 0.003936\n",
      "Loss per Batch: 0.001695\n",
      "Loss per Batch: 0.008013\n",
      "Loss per Batch: 0.006271\n",
      "Loss per Batch: 0.005117\n",
      "Loss per Batch: 0.000560\n",
      "Loss per Batch: 0.003078\n",
      "Loss per Batch: 0.003080\n",
      "Loss per Batch: 0.001382\n",
      "Loss per Batch: 0.001082\n",
      "Loss per Batch: 0.009260\n",
      "Loss per Batch: 0.004340\n",
      "Loss per Batch: 0.000375\n",
      "Loss per Batch: 0.003510\n",
      "Loss per Batch: 0.003198\n",
      "Loss per Batch: 0.001438\n",
      "Loss per Batch: 0.003130\n",
      "Loss per Batch: 0.002095\n",
      "Loss per Batch: 0.004362\n",
      "Loss per Batch: 0.000985\n",
      "Loss per Batch: 0.003886\n",
      "Loss per Batch: 0.002198\n",
      "Loss per Batch: 0.002237\n",
      "Loss per Batch: 0.004215\n",
      "Loss per Batch: 0.012713\n",
      "Loss per Batch: 0.000822\n",
      "Loss per Batch: 0.001157\n",
      "Loss per Batch: 0.003674\n",
      "Loss per Batch: 0.001493\n",
      "Loss per Batch: 0.003265\n",
      "Loss per Batch: 0.001541\n",
      "Loss per Batch: 0.001133\n",
      "Loss per Batch: 0.002153\n",
      "Loss per Batch: 0.001446\n",
      "Loss per Batch: 0.002739\n",
      "Loss per Batch: 0.002971\n",
      "Loss per Batch: 0.000840\n",
      "Loss per Batch: 0.004271\n",
      "Loss per Batch: 0.003962\n",
      "Loss per Batch: 0.002094\n",
      "Loss per Batch: 0.001868\n",
      "Loss per Batch: 0.007226\n",
      "Loss per Batch: 0.002250\n",
      "Loss per Batch: 0.004943\n",
      "Loss per Batch: 0.003185\n",
      "Loss per Batch: 0.006873\n",
      "Loss per Batch: 0.002819\n",
      "Loss per Batch: 0.002665\n",
      "Loss per Batch: 0.005088\n",
      "Loss per Batch: 0.001201\n",
      "Loss per Batch: 0.002721\n",
      "Loss per Batch: 0.001493\n",
      "Loss per Batch: 0.006114\n",
      "Loss per Batch: 0.001434\n",
      "Loss per Batch: 0.001300\n",
      "Loss per Batch: 0.001077\n",
      "Loss per Batch: 0.000673\n",
      "Loss per Batch: 0.004629\n",
      "Loss per Batch: 0.000896\n",
      "Loss per Batch: 0.009273\n",
      "Loss per Batch: 0.003709\n",
      "Loss per Batch: 0.001068\n",
      "Loss per Batch: 0.003866\n",
      "Loss per Batch: 0.000563\n",
      "Loss per Batch: 0.004811\n",
      "Loss per Batch: 0.007443\n",
      "Loss per Batch: 0.002169\n",
      "Loss per Batch: 0.001652\n",
      "Loss per Batch: 0.004384\n",
      "Loss per Batch: 0.000591\n",
      "Loss per Batch: 0.001880\n",
      "Loss per Batch: 0.001069\n",
      "Loss per Batch: 0.001365\n",
      "Loss per Batch: 0.002438\n",
      "Loss per Batch: 0.004968\n",
      "Loss per Batch: 0.001086\n",
      "Loss per Batch: 0.006795\n",
      "Loss per Batch: 0.000889\n",
      "Loss per Batch: 0.001753\n",
      "Loss per Batch: 0.007881\n",
      "Loss per Batch: 0.002722\n",
      "Loss per Batch: 0.002103\n",
      "Loss per Batch: 0.001997\n",
      "Loss per Batch: 0.002112\n",
      "Loss per Batch: 0.003212\n",
      "Loss per Batch: 0.000956\n",
      "Loss per Batch: 0.006751\n",
      "Loss per Batch: 0.004137\n",
      "Loss per Batch: 0.000990\n",
      "Loss per Batch: 0.000863\n",
      "Loss per Batch: 0.003915\n",
      "Loss per Batch: 0.001158\n",
      "Loss per Batch: 0.001765\n",
      "Loss per Batch: 0.003296\n",
      "Loss per Batch: 0.002369\n",
      "Loss per Batch: 0.001864\n",
      "Loss per Batch: 0.003516\n",
      "Loss per Batch: 0.000880\n",
      "Loss per Batch: 0.003797\n",
      "Loss per Batch: 0.001649\n",
      "Loss per Batch: 0.001439\n",
      "Loss per Batch: 0.003702\n",
      "Loss per Batch: 0.001002\n",
      "Loss per Batch: 0.004163\n",
      "Loss per Batch: 0.002818\n",
      "Loss per Batch: 0.001309\n",
      "Loss per Batch: 0.004291\n",
      "Loss per Batch: 0.000850\n",
      "Loss per Batch: 0.003155\n",
      "Loss per Batch: 0.002650\n",
      "Loss per Batch: 0.001246\n",
      "Loss per Batch: 0.004803\n",
      "Loss per Batch: 0.003408\n",
      "Loss per Batch: 0.001870\n",
      "Loss per Batch: 0.001255\n",
      "Final Test Loss: 0.003054\n",
      "Average Fidelity: 0.985398\n"
     ]
    }
   ],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                fidelity += qt.fidelity(qt.Qobj(est_rho(aa)), qt.Qobj(est_rho(bb)))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e11a4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation*}\\left(\\begin{array}{*{11}c}0.334 & (0.199-0.003j) & (0.013+0.110j) & (0.024-0.058j)\\\\(0.199+0.003j) & 0.348 & (0.025+0.012j) & (-0.001+0.027j)\\\\(0.013-0.110j) & (0.025-0.012j) & 0.131 & (0.026-0.042j)\\\\(0.024+0.058j) & (-0.001-0.027j) & (0.026+0.042j) & 0.188\\\\\\end{array}\\right)\\end{equation*}"
      ],
      "text/plain": [
       "Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\n",
       "Qobj data =\n",
       "[[ 0.33364634+0.j          0.19898649-0.00276141j  0.01274197+0.10974701j\n",
       "   0.02396479-0.05813544j]\n",
       " [ 0.19898649+0.00276141j  0.34790772+0.j          0.02461791+0.01244288j\n",
       "  -0.0011488 +0.02711809j]\n",
       " [ 0.01274197-0.10974701j  0.02461791-0.01244288j  0.13060747+0.j\n",
       "   0.02552012-0.04197753j]\n",
       " [ 0.02396479+0.05813544j -0.0011488 -0.02711809j  0.02552012+0.04197753j\n",
       "   0.18783847+0.j        ]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation*}\\left(\\begin{array}{*{11}c}0.359 & (0.162+0.123j) & (-0.009+0.108j) & (0.015-0.072j)\\\\(0.162-0.123j) & 0.354 & (0.032+0.015j) & (0.006+0.040j)\\\\(-0.009-0.108j) & (0.032-0.015j) & 0.099 & (0.022+0.015j)\\\\(0.015+0.072j) & (0.006-0.040j) & (0.022-0.015j) & 0.188\\\\\\end{array}\\right)\\end{equation*}"
      ],
      "text/plain": [
       "Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\n",
       "Qobj data =\n",
       "[[ 0.3591056 +0.j          0.16228738+0.12275733j -0.00902119+0.1076445j\n",
       "   0.01536391-0.07225003j]\n",
       " [ 0.16228738-0.12275733j  0.35423568+0.j          0.03207403+0.01496818j\n",
       "   0.00581496+0.04012542j]\n",
       " [-0.00902119-0.1076445j   0.03207403-0.01496818j  0.09886707+0.j\n",
       "   0.02161318+0.01508737j]\n",
       " [ 0.01536391+0.07225003j  0.00581496-0.04012542j  0.02161318-0.01508737j\n",
       "   0.18779166+0.j        ]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity: 0.979673\n"
     ]
    }
   ],
   "source": [
    "a = np.array(output.cpu().numpy()[1])\n",
    "b = np.array(target.cpu().numpy()[1])\n",
    "display(qt.Qobj(est_rho(a)), qt.Qobj(est_rho(b)))\n",
    "print('Fidelity: {:.6f}'.format(qt.fidelity(qt.Qobj(est_rho(a)), qt.Qobj(est_rho(b)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
