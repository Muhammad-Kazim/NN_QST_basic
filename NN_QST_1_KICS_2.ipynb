{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cacf45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Quantum States\n",
    "\n",
    "import numpy as np\n",
    "import qutip as qt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pure_state():\n",
    "    eps = 1e-7\n",
    "    rand_ket = qt.rand_ket_haar(4)\n",
    "    rho_p = qt.ket2dm(qt.Qobj(rand_ket))\n",
    "    rho_p = (1-eps)*rho_p + (eps/4)*np.eye(4)\n",
    "    return np.array(rho_p)\n",
    "\n",
    "def mixed_state(d):\n",
    "    \n",
    "    \"\"\"d = dimension of matrix\n",
    "    \"\"\"\n",
    "    G = np.random.normal(0, 1, [d,d]) + 1j*np.random.normal(0, 1, [d,d])\n",
    "    G = np.matrix(G)\n",
    "    rho_m = (G*G.H)/np.trace(G*G.H)\n",
    "    return rho_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea72639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate Noise via Variance\n",
    "\n",
    "def eig_val_corr(state):\n",
    "    \n",
    "    \"\"\" state = nxn dimensional state\n",
    "        Corrects state by truncating negative eigenvalues and reconstructs normailized positive\n",
    "        eigenvalued eigenvectos' states\n",
    "    \"\"\"\n",
    "    if np.sum(np.linalg.eig(state)[0].real < 0) > 0 or np.sum(np.linalg.eig(state)[0].real > 1):\n",
    "        eig_val, eig_vec = np.linalg.eig(state)\n",
    "        eig_val = eig_val.real\n",
    "        eig_val[eig_val < 0] = 0\n",
    "        eig_val = eig_val/np.sum(eig_val)\n",
    "        \n",
    "        d = state.shape[0]\n",
    "        state = np.zeros([d,d], dtype = 'complex')\n",
    "        for ij in range(d):\n",
    "            state += eig_val[ij]*np.matmul(eig_vec[ij].reshape(d,1), np.matrix(eig_vec[ij].reshape(d,1)).H)\n",
    "            \n",
    "    return state\n",
    "            \n",
    "def noise_state(state, var):\n",
    "    \n",
    "    \"\"\" state = 4x4 quantum state\n",
    "        var = scalar value. High var = high noise in state to be measured.\n",
    "    \"\"\"\n",
    "    d = state.shape[0]\n",
    "    noisy_state_raw = state + np.random.normal(0, var, [d,d]) + 1j*np.random.normal(0, var, [d,d])\n",
    "    \n",
    "    if np.sum(np.linalg.eig(noisy_state_raw)[0].real < 0) > 0:\n",
    "        state = eig_val_corr(noisy_state_raw)\n",
    "    else:\n",
    "        state = noisy_state_raw\n",
    "            \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7bf204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f3b89da0d0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checking relation between var and infidelity\n",
    "\n",
    "fid = np.zeros([11, 1000])\n",
    "k=0\n",
    "for ij in range(11):\n",
    "    var = 0.01*ij\n",
    "    for jk in range(1000):\n",
    "        rho = mixed_state(d=8)\n",
    "        rho_noisy = noise_state(rho, var)\n",
    "\n",
    "        fid[ij, jk] = qt.fidelity(qt.Qobj(rho), qt.Qobj(rho_noisy))\n",
    "        \n",
    "plt.plot(0.01*np.arange(11), np.mean(fid, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dffd9d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pauli_basis():\n",
    "    \n",
    "    I = np.array([[1, 0],\n",
    "                  [0, 1]], dtype = 'complex')\n",
    "    X = np.array([[0, 1],\n",
    "                  [1, 0]], dtype = 'complex')\n",
    "    Y = np.array([[0, -1j],\n",
    "                  [1j, 0]], dtype = 'complex')\n",
    "    Z = np.array([[1, 0],\n",
    "                  [0, -1]], dtype = 'complex')\n",
    "    pauli = np.array([I, X, Y ,Z], dtype = 'complex')\n",
    "    \n",
    "    return pauli\n",
    "\n",
    "def pauli_words(optrs, qbts):\n",
    "    if qbts == 1:\n",
    "        return optrs\n",
    "    else:\n",
    "        d = optrs.shape[-1]\n",
    "        pauli = pauli_basis()\n",
    "        temp_optrs = np.zeros([(2*d)**2, d*2, d*2], dtype = 'complex')\n",
    "        optrs = optrs.reshape([d**2, d, d])\n",
    "        for i in range(4):\n",
    "            for j in range(d**2):\n",
    "                temp_optrs[(d**2)*i + j] = np.kron(pauli[i], optrs[j])\n",
    "        rep_4 = int(np.log10((2*d)**2)/np.log10(4))\n",
    "        temp_optrs = temp_optrs.reshape(np.append(np.repeat(4, rep_4), [d*2, d*2]))\n",
    "        return pauli_words(temp_optrs, qbts - 1)\n",
    "\n",
    "\n",
    "def counts_to_bloch(rho_noisy, optrs, shots, qbts):\n",
    "    \n",
    "    mn_rnd = np.zeros([4**qbts, 2**qbts])\n",
    "    mn_eval = np.zeros([4**qbts, 2**qbts])\n",
    "\n",
    "    for ii in range(4**qbts):\n",
    "        rr = np.zeros([1, 2**qbts])\n",
    "        for jj in range(2**qbts):\n",
    "            e_val, e_vec = np.linalg.eig(optrs.reshape(64, 8, 8)[ii])\n",
    "            e_vec = np.matrix(e_vec[:,jj]).transpose()\n",
    "            e_vec_mat = e_vec*e_vec.getH()\n",
    "            rr[0, jj] = np.real(np.trace(np.matmul(rho_noisy, e_vec_mat)))\n",
    "        rr = np.round(rr[0], 8)\n",
    "        rr[rr < 0] = 0\n",
    "        rr = rr/np.sum(rr)\n",
    "        rr[-1] = 1 - np.sum(rr[:-1])\n",
    "        if np.sum(rr < 0) > 0:\n",
    "            print(rr)\n",
    "        rr[rr < 0.0000000001] = 0\n",
    "        mn_rnd[ii, :] = np.random.multinomial(shots, rr)\n",
    "        mn_eval[ii, :] = e_val.real\n",
    "\n",
    "    return np.sum(mn_rnd*mn_eval, axis=1)/shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e27ff179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-c81be5ade337>:44: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  temp_labels[i, j, k] = np.trace(np.matmul(optrs[i,j,k], rho))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoint: 0\n",
      "Datapoint: 100\n",
      "Datapoint: 200\n",
      "Datapoint: 300\n",
      "Datapoint: 400\n",
      "Datapoint: 500\n",
      "Datapoint: 600\n",
      "Datapoint: 700\n",
      "Datapoint: 800\n",
      "Datapoint: 900\n",
      "Datapoint: 1000\n",
      "Datapoint: 1100\n",
      "Datapoint: 1200\n",
      "Datapoint: 1300\n",
      "Datapoint: 1400\n",
      "Datapoint: 1500\n",
      "Datapoint: 1600\n",
      "Datapoint: 1700\n",
      "Datapoint: 1800\n",
      "Datapoint: 1900\n",
      "Datapoint: 2000\n",
      "Datapoint: 2100\n",
      "Datapoint: 2200\n",
      "Datapoint: 2300\n",
      "Datapoint: 2400\n",
      "Datapoint: 2500\n",
      "Datapoint: 2600\n",
      "Datapoint: 2700\n",
      "Datapoint: 2800\n",
      "Datapoint: 2900\n",
      "Datapoint: 3000\n",
      "Datapoint: 3100\n",
      "Datapoint: 3200\n",
      "Datapoint: 3300\n",
      "Datapoint: 3400\n",
      "Datapoint: 3500\n",
      "Datapoint: 3600\n",
      "Datapoint: 3700\n",
      "Datapoint: 3800\n",
      "Datapoint: 3900\n",
      "Datapoint: 4000\n",
      "Datapoint: 4100\n",
      "Datapoint: 4200\n",
      "Datapoint: 4300\n",
      "Datapoint: 4400\n",
      "Datapoint: 4500\n",
      "Datapoint: 4600\n",
      "Datapoint: 4700\n",
      "Datapoint: 4800\n",
      "Datapoint: 4900\n",
      "Datapoint: 5000\n",
      "Datapoint: 5100\n",
      "Datapoint: 5200\n",
      "Datapoint: 5300\n",
      "Datapoint: 5400\n",
      "Datapoint: 5500\n",
      "Datapoint: 5600\n",
      "Datapoint: 5700\n",
      "[ 1.13393531e-01  1.19088996e-01  2.30316358e-01  1.58550668e-01\n",
      "  1.30238959e-01  1.33306032e-01  1.15105456e-01 -2.22044605e-16]\n",
      "Datapoint: 5800\n",
      "Datapoint: 5900\n",
      "Datapoint: 6000\n",
      "Datapoint: 6100\n",
      "Datapoint: 6200\n",
      "Datapoint: 6300\n",
      "Datapoint: 6400\n",
      "Datapoint: 6500\n",
      "Datapoint: 6600\n",
      "Datapoint: 6700\n",
      "Datapoint: 6800\n",
      "Datapoint: 6900\n",
      "Datapoint: 7000\n",
      "Datapoint: 7100\n",
      "Datapoint: 7200\n",
      "Datapoint: 7300\n",
      "Datapoint: 7400\n",
      "Datapoint: 7500\n",
      "Datapoint: 7600\n",
      "Datapoint: 7700\n",
      "Datapoint: 7800\n",
      "Datapoint: 7900\n",
      "Datapoint: 8000\n",
      "Datapoint: 8100\n",
      "Datapoint: 8200\n",
      "Datapoint: 8300\n",
      "Datapoint: 8400\n",
      "Datapoint: 8500\n",
      "[ 1.50321632e-01  1.59381545e-01  1.47033389e-01  1.60801424e-01\n",
      "  1.40870017e-01  1.11815242e-01  1.29776751e-01 -2.22044605e-16]\n",
      "[ 1.59381545e-01  1.50321632e-01  1.47033389e-01  1.60801424e-01\n",
      "  1.11815242e-01  1.40870017e-01  1.29776751e-01 -2.22044605e-16]\n",
      "Datapoint: 8600\n",
      "Datapoint: 8700\n",
      "Datapoint: 8800\n",
      "Datapoint: 8900\n",
      "Datapoint: 9000\n",
      "Datapoint: 9100\n",
      "Datapoint: 9200\n",
      "Datapoint: 9300\n",
      "Datapoint: 9400\n",
      "Datapoint: 9500\n",
      "Datapoint: 9600\n",
      "Datapoint: 9700\n",
      "Datapoint: 9800\n",
      "Datapoint: 9900\n"
     ]
    }
   ],
   "source": [
    "data_pts = 10000\n",
    "d = 8\n",
    "var_vals = 5\n",
    "k = np.zeros(64)\n",
    "qbts = 3\n",
    "shots = 1000\n",
    "\n",
    "optrs = pauli_words(pauli_basis(), qbts)\n",
    "\n",
    "var_0_L = np.zeros([data_pts, 64])\n",
    "var_0_I = np.zeros([data_pts, 64])\n",
    "v0 = 0\n",
    "\n",
    "var_1_L = np.zeros([data_pts, 64])\n",
    "var_1_I = np.zeros([data_pts, 64])\n",
    "v1 = 0\n",
    "\n",
    "var_2_L = np.zeros([data_pts, 64])\n",
    "var_2_I = np.zeros([data_pts, 64])\n",
    "v2 = 0\n",
    "\n",
    "var_3_L = np.zeros([data_pts, 64])\n",
    "var_3_I = np.zeros([data_pts, 64])\n",
    "v3 = 0\n",
    "\n",
    "var_4_L = np.zeros([data_pts, 64])\n",
    "var_4_I = np.zeros([data_pts, 64])\n",
    "v4 = 0\n",
    "\n",
    "for ij in range(data_pts):\n",
    "    \n",
    "    rho = mixed_state(d)\n",
    "    \n",
    "    for jk in range(var_vals):\n",
    "        var = 0.01*jk\n",
    "        rho_noisy = noise_state(rho, var)\n",
    "        \n",
    "        temp_labels = np.zeros([4,4,4])\n",
    "        temp_inputs = np.zeros([4,4,4])\n",
    "        \n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                for k in range(4):\n",
    "                    temp_labels[i, j, k] = np.trace(np.matmul(optrs[i,j,k], rho))        \n",
    "         \n",
    "        if jk == 0:\n",
    "            var_0_L[v0] = temp_labels.reshape(64)\n",
    "            var_0_I[v0] = counts_to_bloch(rho_noisy,optrs, shots, qbts)\n",
    "            v0 += 1\n",
    "        \n",
    "        if jk == 1:\n",
    "            var_1_L[v1] = temp_labels.reshape(64)\n",
    "            var_1_I[v1] = counts_to_bloch(rho_noisy,optrs, shots, qbts)\n",
    "            v1 += 1\n",
    "            \n",
    "        if jk == 2:\n",
    "            var_2_L[v2] = temp_labels.reshape(64)\n",
    "            var_2_I[v2] = counts_to_bloch(rho_noisy,optrs, shots, qbts)\n",
    "            v2 += 1\n",
    "        \n",
    "        if jk == 3:\n",
    "            var_3_L[v3] = temp_labels.reshape(64)\n",
    "            var_3_I[v3] = counts_to_bloch(rho_noisy,optrs, shots, qbts)\n",
    "            v3 += 1\n",
    "            \n",
    "        if jk == 4:\n",
    "            var_4_L[v4] = temp_labels.reshape(64)\n",
    "            var_4_I[v4] = counts_to_bloch(rho_noisy,optrs, shots, qbts)\n",
    "            v4 += 1\n",
    "    if ij%100 == 0:\n",
    "        print(\"Datapoint:\",ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686c9209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9912356387433503 0.805200699096597 0.7395621299442228 0.7147399115680038 0.7018975208374534\n",
      "0.0009700103463936643 0.025321809009177923 0.03520112095728505 0.03937028087760285 0.04150426725347966\n"
     ]
    }
   ],
   "source": [
    "def state_recon_bloch_vec(optrs, r):\n",
    "    \n",
    "    \"\"\"return state given pauli tensors of X, Y, Z and bloch vectors of size d**qbts\n",
    "    \"\"\"\n",
    "    var_0_L_a = r.reshape(4,4,4)\n",
    "    state_act = np.zeros([8,8], dtype = 'complex')\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(4):\n",
    "                state_act += var_0_L_a[i, j, k]*optrs[i,j,k]\n",
    "    \n",
    "    return state_act/8\n",
    "                \n",
    "state_act = state_recon_bloch_vec(optrs, var_4_L[2])\n",
    "state_est = state_recon_bloch_vec(optrs, var_4_I[2])\n",
    "            \n",
    "qt.fidelity(qt.Qobj(state_act), qt.Qobj(state_est))\n",
    "\n",
    "def fid_avg(r_noise, r_actual, l):\n",
    "    \n",
    "    fidelity = 0.0\n",
    "    for ij in range(l):\n",
    "        a = state_recon_bloch_vec(optrs, r_noise[ij])\n",
    "        b = state_recon_bloch_vec(optrs, r_actual[ij])\n",
    "        fidelity += qt.fidelity(qt.Qobj(a), qt.Qobj(b))\n",
    "    return fidelity/l\n",
    "\n",
    "def mse(x1, y1):\n",
    "    \n",
    "    MSE = np.square(np.subtract(x1,y1)).mean()\n",
    "    return MSE\n",
    "\n",
    "def mse_avg(X, Y):\n",
    "    \n",
    "    x_len = X.shape[0]\n",
    "    y_len = Y.shape[0]\n",
    "    \n",
    "    avg_mse = 0.0\n",
    "    \n",
    "    if x_len == y_len:\n",
    "        for ii in range(x_len):\n",
    "            avg_mse += mse(X[ii], Y[ii])\n",
    "    \n",
    "    return avg_mse/x_len\n",
    "\n",
    "a_f = fid_avg(var_0_I, var_0_L, data_pts)\n",
    "a_m = mse_avg(var_0_I, var_0_L)\n",
    "\n",
    "b_f = fid_avg(var_1_I, var_1_L, data_pts)\n",
    "b_m = mse_avg(var_1_I, var_1_L)\n",
    "\n",
    "c_f = fid_avg(var_2_I, var_2_L, data_pts)\n",
    "c_m = mse_avg(var_2_I, var_2_L)\n",
    "\n",
    "d_f = fid_avg(var_3_I, var_3_L, data_pts)\n",
    "d_m = mse_avg(var_3_I, var_3_L)\n",
    "\n",
    "e_f = fid_avg(var_4_I, var_4_L, data_pts)\n",
    "e_m = mse_avg(var_4_I, var_4_L)\n",
    "\n",
    "print(a_f, b_f, c_f, d_f, e_f)\n",
    "print(a_m, b_m, c_m, d_m, e_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084b8a4",
   "metadata": {},
   "source": [
    "#N = 10\n",
    "\n",
    "\n",
    "#N = 100  \n",
    "0.9658720252728576 0.8033813290499711 0.7483017731774276 0.7178030796983127 0.7117393802535091 -> 1000 copies   \n",
    "0.9659714884673475 0.795450178700517 0.7444010676587345 0.7205603263362355 0.7130375925974438 -> 10,000 copies\n",
    "\n",
    "#N = 1000   -> 10,000 copies    \n",
    "Infidelity: 0.9912356387433503 0.805200699096597 0.7395621299442228 0.7147399115680038 0.7018975208374534      \n",
    "MSE: 0.0009700103463936643 0.025321809009177923 0.03520112095728505 0.03937028087760285 0.04150426725347966\n",
    "\n",
    "#N = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31f9e89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c806fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets\n",
    "#200 DMs for pure and mixed each\n",
    "#perform 200 noisy meaurements on each with var=pi/6\n",
    "\n",
    "M_dataset = var_0_I #1000x64\n",
    "M_label = var_0_L #1000x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "38c65ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_ds = M_dataset.reshape(data_pts,1, 8, 8).real\n",
    "M_l = M_label.reshape(data_pts, 64).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d068a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_len = data_pts\n",
    "test_size = 0.1\n",
    "indices = list(range(ds_len))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(test_size * ds_len))\n",
    "train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "M_testset = M_ds[test_idx]\n",
    "M_testlabel = M_l[test_idx]\n",
    "M_trainset = M_ds[train_idx]\n",
    "M_trainlabel = M_l[train_idx]\n",
    "\n",
    "train_len = int(ds_len*(1-test_size))\n",
    "valid_size = 0.2\n",
    "indices = list(range(train_len))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * train_len))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "M_validset = M_trainset[valid_idx]\n",
    "M_validlabel = M_trainlabel[valid_idx]\n",
    "M_trainset = M_trainset[train_idx]\n",
    "M_trainlabel = M_trainlabel[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "38a00342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "#mixed_train_set\n",
    "M_trainset = torch.Tensor(M_trainset) # transform to torch tensor\n",
    "M_trainlabel = torch.tensor(M_trainlabel)\n",
    "\n",
    "#mixed_valid set\n",
    "M_validset = torch.Tensor(M_validset) # transform to torch tensor\n",
    "M_validlabel = torch.tensor(M_validlabel)\n",
    "\n",
    "#mixed_test set\n",
    "M_testset = torch.Tensor(M_testset) # transform to torch tensor\n",
    "M_testlabel = torch.tensor(M_testlabel)\n",
    "\n",
    "#datasets\n",
    "train_data = TensorDataset(M_trainset, M_trainlabel)\n",
    "valid_data = TensorDataset(M_validset, M_validlabel)\n",
    "test_data = TensorDataset(M_testset, M_testlabel)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e2300ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer (sees 8x8x1 image tensor)\n",
    "        self.conv1 = nn.Conv2d(1, 24, 3, padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(24)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         convolutional layer (sees 4x4x12 tensor)\n",
    "        self.conv2 = nn.Conv2d(24, 128, 3, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "#         # convolutional layer (sees 2x2x64 tensor)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "#         self.conv3_bn = nn.BatchNorm2d(128)\n",
    "\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # linear layer (128 * 2 * 2 -> 450)\n",
    "        self.fc1 = nn.Linear(2048, 256)\n",
    "        # linear layer (256 -> 64)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        # dropout layer (p=0.5)\n",
    "        self.dropout = nn.Dropout(0.10)\n",
    "        self.m = nn.ELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv1_bn(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv2_bn(x)\n",
    "#         x = self.pool(x)\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = self.conv3_bn(x)\n",
    "\n",
    "        # flatten input\n",
    "        x = x.view(-1, 2048)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = self.m(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = self.m(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be525afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01de8aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (conv2): Conv2d(24, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (m): ELU(alpha=1.0)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.067348 \tValidation Loss: 0.033790\n",
      "Validation loss decreased (inf --> 0.033790).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.052150 \tValidation Loss: 0.028719\n",
      "Validation loss decreased (0.033790 --> 0.028719).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.045881 \tValidation Loss: 0.025776\n",
      "Validation loss decreased (0.028719 --> 0.025776).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.041830 \tValidation Loss: 0.023879\n",
      "Validation loss decreased (0.025776 --> 0.023879).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.038934 \tValidation Loss: 0.022487\n",
      "Validation loss decreased (0.023879 --> 0.022487).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.036852 \tValidation Loss: 0.021386\n",
      "Validation loss decreased (0.022487 --> 0.021386).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.035367 \tValidation Loss: 0.020686\n",
      "Validation loss decreased (0.021386 --> 0.020686).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.034268 \tValidation Loss: 0.020063\n",
      "Validation loss decreased (0.020686 --> 0.020063).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.033227 \tValidation Loss: 0.019535\n",
      "Validation loss decreased (0.020063 --> 0.019535).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.032563 \tValidation Loss: 0.019142\n",
      "Validation loss decreased (0.019535 --> 0.019142).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.031822 \tValidation Loss: 0.018846\n",
      "Validation loss decreased (0.019142 --> 0.018846).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.031348 \tValidation Loss: 0.018552\n",
      "Validation loss decreased (0.018846 --> 0.018552).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.030780 \tValidation Loss: 0.018345\n",
      "Validation loss decreased (0.018552 --> 0.018345).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.030376 \tValidation Loss: 0.018109\n",
      "Validation loss decreased (0.018345 --> 0.018109).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.029988 \tValidation Loss: 0.017960\n",
      "Validation loss decreased (0.018109 --> 0.017960).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.029643 \tValidation Loss: 0.017773\n",
      "Validation loss decreased (0.017960 --> 0.017773).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.029267 \tValidation Loss: 0.017653\n",
      "Validation loss decreased (0.017773 --> 0.017653).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.028951 \tValidation Loss: 0.017504\n",
      "Validation loss decreased (0.017653 --> 0.017504).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.028659 \tValidation Loss: 0.017432\n",
      "Validation loss decreased (0.017504 --> 0.017432).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.028360 \tValidation Loss: 0.017343\n",
      "Validation loss decreased (0.017432 --> 0.017343).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.028074 \tValidation Loss: 0.017230\n",
      "Validation loss decreased (0.017343 --> 0.017230).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.027787 \tValidation Loss: 0.017124\n",
      "Validation loss decreased (0.017230 --> 0.017124).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.027613 \tValidation Loss: 0.017067\n",
      "Validation loss decreased (0.017124 --> 0.017067).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.027367 \tValidation Loss: 0.016966\n",
      "Validation loss decreased (0.017067 --> 0.016966).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.027073 \tValidation Loss: 0.016925\n",
      "Validation loss decreased (0.016966 --> 0.016925).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.026869 \tValidation Loss: 0.016872\n",
      "Validation loss decreased (0.016925 --> 0.016872).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.026658 \tValidation Loss: 0.016787\n",
      "Validation loss decreased (0.016872 --> 0.016787).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.026451 \tValidation Loss: 0.016766\n",
      "Validation loss decreased (0.016787 --> 0.016766).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.026347 \tValidation Loss: 0.016718\n",
      "Validation loss decreased (0.016766 --> 0.016718).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.026140 \tValidation Loss: 0.016655\n",
      "Validation loss decreased (0.016718 --> 0.016655).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.025951 \tValidation Loss: 0.016615\n",
      "Validation loss decreased (0.016655 --> 0.016615).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.025763 \tValidation Loss: 0.016565\n",
      "Validation loss decreased (0.016615 --> 0.016565).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.025495 \tValidation Loss: 0.016533\n",
      "Validation loss decreased (0.016565 --> 0.016533).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.025371 \tValidation Loss: 0.016513\n",
      "Validation loss decreased (0.016533 --> 0.016513).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.025271 \tValidation Loss: 0.016450\n",
      "Validation loss decreased (0.016513 --> 0.016450).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.025076 \tValidation Loss: 0.016430\n",
      "Validation loss decreased (0.016450 --> 0.016430).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.024936 \tValidation Loss: 0.016404\n",
      "Validation loss decreased (0.016430 --> 0.016404).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.024712 \tValidation Loss: 0.016364\n",
      "Validation loss decreased (0.016404 --> 0.016364).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.024530 \tValidation Loss: 0.016351\n",
      "Validation loss decreased (0.016364 --> 0.016351).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.024566 \tValidation Loss: 0.016288\n",
      "Validation loss decreased (0.016351 --> 0.016288).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.024303 \tValidation Loss: 0.016285\n",
      "Validation loss decreased (0.016288 --> 0.016285).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.024185 \tValidation Loss: 0.016249\n",
      "Validation loss decreased (0.016285 --> 0.016249).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.024096 \tValidation Loss: 0.016207\n",
      "Validation loss decreased (0.016249 --> 0.016207).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.023889 \tValidation Loss: 0.016186\n",
      "Validation loss decreased (0.016207 --> 0.016186).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 0.023795 \tValidation Loss: 0.016165\n",
      "Validation loss decreased (0.016186 --> 0.016165).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.023676 \tValidation Loss: 0.016137\n",
      "Validation loss decreased (0.016165 --> 0.016137).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.023553 \tValidation Loss: 0.016108\n",
      "Validation loss decreased (0.016137 --> 0.016108).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.023396 \tValidation Loss: 0.016083\n",
      "Validation loss decreased (0.016108 --> 0.016083).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.023311 \tValidation Loss: 0.016066\n",
      "Validation loss decreased (0.016083 --> 0.016066).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.023127 \tValidation Loss: 0.016038\n",
      "Validation loss decreased (0.016066 --> 0.016038).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 0.023019 \tValidation Loss: 0.016024\n",
      "Validation loss decreased (0.016038 --> 0.016024).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.022926 \tValidation Loss: 0.015985\n",
      "Validation loss decreased (0.016024 --> 0.015985).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.022821 \tValidation Loss: 0.015972\n",
      "Validation loss decreased (0.015985 --> 0.015972).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.022792 \tValidation Loss: 0.015950\n",
      "Validation loss decreased (0.015972 --> 0.015950).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.022628 \tValidation Loss: 0.015921\n",
      "Validation loss decreased (0.015950 --> 0.015921).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 0.022525 \tValidation Loss: 0.015905\n",
      "Validation loss decreased (0.015921 --> 0.015905).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.022426 \tValidation Loss: 0.015901\n",
      "Validation loss decreased (0.015905 --> 0.015901).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 0.022334 \tValidation Loss: 0.015870\n",
      "Validation loss decreased (0.015901 --> 0.015870).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 \tTraining Loss: 0.022192 \tValidation Loss: 0.015846\n",
      "Validation loss decreased (0.015870 --> 0.015846).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.022155 \tValidation Loss: 0.015835\n",
      "Validation loss decreased (0.015846 --> 0.015835).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 0.022105 \tValidation Loss: 0.015817\n",
      "Validation loss decreased (0.015835 --> 0.015817).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 0.022051 \tValidation Loss: 0.015799\n",
      "Validation loss decreased (0.015817 --> 0.015799).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.021922 \tValidation Loss: 0.015771\n",
      "Validation loss decreased (0.015799 --> 0.015771).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.021814 \tValidation Loss: 0.015758\n",
      "Validation loss decreased (0.015771 --> 0.015758).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.021743 \tValidation Loss: 0.015743\n",
      "Validation loss decreased (0.015758 --> 0.015743).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 0.021608 \tValidation Loss: 0.015740\n",
      "Validation loss decreased (0.015743 --> 0.015740).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 0.021557 \tValidation Loss: 0.015723\n",
      "Validation loss decreased (0.015740 --> 0.015723).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.021543 \tValidation Loss: 0.015714\n",
      "Validation loss decreased (0.015723 --> 0.015714).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.021351 \tValidation Loss: 0.015693\n",
      "Validation loss decreased (0.015714 --> 0.015693).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.021312 \tValidation Loss: 0.015671\n",
      "Validation loss decreased (0.015693 --> 0.015671).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 0.021278 \tValidation Loss: 0.015662\n",
      "Validation loss decreased (0.015671 --> 0.015662).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 0.021113 \tValidation Loss: 0.015644\n",
      "Validation loss decreased (0.015662 --> 0.015644).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.021128 \tValidation Loss: 0.015623\n",
      "Validation loss decreased (0.015644 --> 0.015623).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.021023 \tValidation Loss: 0.015627\n",
      "Epoch: 75 \tTraining Loss: 0.020955 \tValidation Loss: 0.015618\n",
      "Validation loss decreased (0.015623 --> 0.015618).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 0.020887 \tValidation Loss: 0.015585\n",
      "Validation loss decreased (0.015618 --> 0.015585).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 0.020850 \tValidation Loss: 0.015587\n",
      "Epoch: 78 \tTraining Loss: 0.020767 \tValidation Loss: 0.015574\n",
      "Validation loss decreased (0.015585 --> 0.015574).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 0.020664 \tValidation Loss: 0.015563\n",
      "Validation loss decreased (0.015574 --> 0.015563).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 0.020661 \tValidation Loss: 0.015555\n",
      "Validation loss decreased (0.015563 --> 0.015555).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 0.020541 \tValidation Loss: 0.015547\n",
      "Validation loss decreased (0.015555 --> 0.015547).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 0.020487 \tValidation Loss: 0.015511\n",
      "Validation loss decreased (0.015547 --> 0.015511).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 0.020411 \tValidation Loss: 0.015516\n",
      "Epoch: 84 \tTraining Loss: 0.020387 \tValidation Loss: 0.015519\n",
      "Epoch: 85 \tTraining Loss: 0.020311 \tValidation Loss: 0.015474\n",
      "Validation loss decreased (0.015511 --> 0.015474).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 0.020265 \tValidation Loss: 0.015486\n",
      "Epoch: 87 \tTraining Loss: 0.020144 \tValidation Loss: 0.015464\n",
      "Validation loss decreased (0.015474 --> 0.015464).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.020180 \tValidation Loss: 0.015473\n",
      "Epoch: 89 \tTraining Loss: 0.020059 \tValidation Loss: 0.015447\n",
      "Validation loss decreased (0.015464 --> 0.015447).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 0.020018 \tValidation Loss: 0.015444\n",
      "Validation loss decreased (0.015447 --> 0.015444).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 0.019995 \tValidation Loss: 0.015442\n",
      "Validation loss decreased (0.015444 --> 0.015442).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 0.019978 \tValidation Loss: 0.015417\n",
      "Validation loss decreased (0.015442 --> 0.015417).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 0.019822 \tValidation Loss: 0.015414\n",
      "Validation loss decreased (0.015417 --> 0.015414).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 0.019794 \tValidation Loss: 0.015402\n",
      "Validation loss decreased (0.015414 --> 0.015402).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 0.019770 \tValidation Loss: 0.015388\n",
      "Validation loss decreased (0.015402 --> 0.015388).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.019699 \tValidation Loss: 0.015383\n",
      "Validation loss decreased (0.015388 --> 0.015383).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 0.019641 \tValidation Loss: 0.015373\n",
      "Validation loss decreased (0.015383 --> 0.015373).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 0.019627 \tValidation Loss: 0.015365\n",
      "Validation loss decreased (0.015373 --> 0.015365).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 0.019572 \tValidation Loss: 0.015346\n",
      "Validation loss decreased (0.015365 --> 0.015346).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 0.019544 \tValidation Loss: 0.015355\n",
      "Epoch: 101 \tTraining Loss: 0.019459 \tValidation Loss: 0.015343\n",
      "Validation loss decreased (0.015346 --> 0.015343).  Saving model ...\n",
      "Epoch: 102 \tTraining Loss: 0.019419 \tValidation Loss: 0.015332\n",
      "Validation loss decreased (0.015343 --> 0.015332).  Saving model ...\n",
      "Epoch: 103 \tTraining Loss: 0.019377 \tValidation Loss: 0.015324\n",
      "Validation loss decreased (0.015332 --> 0.015324).  Saving model ...\n",
      "Epoch: 104 \tTraining Loss: 0.019321 \tValidation Loss: 0.015312\n",
      "Validation loss decreased (0.015324 --> 0.015312).  Saving model ...\n",
      "Epoch: 105 \tTraining Loss: 0.019329 \tValidation Loss: 0.015322\n",
      "Epoch: 106 \tTraining Loss: 0.019221 \tValidation Loss: 0.015319\n",
      "Epoch: 107 \tTraining Loss: 0.019252 \tValidation Loss: 0.015309\n",
      "Validation loss decreased (0.015312 --> 0.015309).  Saving model ...\n",
      "Epoch: 108 \tTraining Loss: 0.019163 \tValidation Loss: 0.015300\n",
      "Validation loss decreased (0.015309 --> 0.015300).  Saving model ...\n",
      "Epoch: 109 \tTraining Loss: 0.019110 \tValidation Loss: 0.015284\n",
      "Validation loss decreased (0.015300 --> 0.015284).  Saving model ...\n",
      "Epoch: 110 \tTraining Loss: 0.019118 \tValidation Loss: 0.015291\n",
      "Epoch: 111 \tTraining Loss: 0.019070 \tValidation Loss: 0.015278\n",
      "Validation loss decreased (0.015284 --> 0.015278).  Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 0.018977 \tValidation Loss: 0.015277\n",
      "Validation loss decreased (0.015278 --> 0.015277).  Saving model ...\n",
      "Epoch: 113 \tTraining Loss: 0.018997 \tValidation Loss: 0.015254\n",
      "Validation loss decreased (0.015277 --> 0.015254).  Saving model ...\n",
      "Epoch: 114 \tTraining Loss: 0.018941 \tValidation Loss: 0.015266\n",
      "Epoch: 115 \tTraining Loss: 0.018907 \tValidation Loss: 0.015261\n",
      "Epoch: 116 \tTraining Loss: 0.018845 \tValidation Loss: 0.015245\n",
      "Validation loss decreased (0.015254 --> 0.015245).  Saving model ...\n",
      "Epoch: 117 \tTraining Loss: 0.018837 \tValidation Loss: 0.015230\n",
      "Validation loss decreased (0.015245 --> 0.015230).  Saving model ...\n",
      "Epoch: 118 \tTraining Loss: 0.018841 \tValidation Loss: 0.015239\n",
      "Epoch: 119 \tTraining Loss: 0.018786 \tValidation Loss: 0.015242\n",
      "Epoch: 120 \tTraining Loss: 0.018708 \tValidation Loss: 0.015222\n",
      "Validation loss decreased (0.015230 --> 0.015222).  Saving model ...\n",
      "Epoch: 121 \tTraining Loss: 0.018713 \tValidation Loss: 0.015203\n",
      "Validation loss decreased (0.015222 --> 0.015203).  Saving model ...\n",
      "Epoch: 122 \tTraining Loss: 0.018694 \tValidation Loss: 0.015204\n",
      "Epoch: 123 \tTraining Loss: 0.018639 \tValidation Loss: 0.015205\n",
      "Epoch: 124 \tTraining Loss: 0.018622 \tValidation Loss: 0.015184\n",
      "Validation loss decreased (0.015203 --> 0.015184).  Saving model ...\n",
      "Epoch: 125 \tTraining Loss: 0.018557 \tValidation Loss: 0.015196\n",
      "Epoch: 126 \tTraining Loss: 0.018582 \tValidation Loss: 0.015204\n",
      "Epoch: 127 \tTraining Loss: 0.018489 \tValidation Loss: 0.015180\n",
      "Validation loss decreased (0.015184 --> 0.015180).  Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 0.018477 \tValidation Loss: 0.015176\n",
      "Validation loss decreased (0.015180 --> 0.015176).  Saving model ...\n",
      "Epoch: 129 \tTraining Loss: 0.018457 \tValidation Loss: 0.015166\n",
      "Validation loss decreased (0.015176 --> 0.015166).  Saving model ...\n",
      "Epoch: 130 \tTraining Loss: 0.018427 \tValidation Loss: 0.015166\n",
      "Validation loss decreased (0.015166 --> 0.015166).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 131 \tTraining Loss: 0.018363 \tValidation Loss: 0.015162\n",
      "Validation loss decreased (0.015166 --> 0.015162).  Saving model ...\n",
      "Epoch: 132 \tTraining Loss: 0.018359 \tValidation Loss: 0.015170\n",
      "Epoch: 133 \tTraining Loss: 0.018327 \tValidation Loss: 0.015155\n",
      "Validation loss decreased (0.015162 --> 0.015155).  Saving model ...\n",
      "Epoch: 134 \tTraining Loss: 0.018302 \tValidation Loss: 0.015146\n",
      "Validation loss decreased (0.015155 --> 0.015146).  Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 0.018290 \tValidation Loss: 0.015142\n",
      "Validation loss decreased (0.015146 --> 0.015142).  Saving model ...\n",
      "Epoch: 136 \tTraining Loss: 0.018290 \tValidation Loss: 0.015139\n",
      "Validation loss decreased (0.015142 --> 0.015139).  Saving model ...\n",
      "Epoch: 137 \tTraining Loss: 0.018248 \tValidation Loss: 0.015132\n",
      "Validation loss decreased (0.015139 --> 0.015132).  Saving model ...\n",
      "Epoch: 138 \tTraining Loss: 0.018214 \tValidation Loss: 0.015133\n",
      "Epoch: 139 \tTraining Loss: 0.018168 \tValidation Loss: 0.015122\n",
      "Validation loss decreased (0.015132 --> 0.015122).  Saving model ...\n",
      "Epoch: 140 \tTraining Loss: 0.018166 \tValidation Loss: 0.015119\n",
      "Validation loss decreased (0.015122 --> 0.015119).  Saving model ...\n",
      "Epoch: 141 \tTraining Loss: 0.018146 \tValidation Loss: 0.015123\n",
      "Epoch: 142 \tTraining Loss: 0.018103 \tValidation Loss: 0.015118\n",
      "Validation loss decreased (0.015119 --> 0.015118).  Saving model ...\n",
      "Epoch: 143 \tTraining Loss: 0.018077 \tValidation Loss: 0.015116\n",
      "Validation loss decreased (0.015118 --> 0.015116).  Saving model ...\n",
      "Epoch: 144 \tTraining Loss: 0.018076 \tValidation Loss: 0.015109\n",
      "Validation loss decreased (0.015116 --> 0.015109).  Saving model ...\n",
      "Epoch: 145 \tTraining Loss: 0.018021 \tValidation Loss: 0.015114\n",
      "Epoch: 146 \tTraining Loss: 0.018086 \tValidation Loss: 0.015110\n",
      "Epoch: 147 \tTraining Loss: 0.017991 \tValidation Loss: 0.015100\n",
      "Validation loss decreased (0.015109 --> 0.015100).  Saving model ...\n",
      "Epoch: 148 \tTraining Loss: 0.017972 \tValidation Loss: 0.015099\n",
      "Validation loss decreased (0.015100 --> 0.015099).  Saving model ...\n",
      "Epoch: 149 \tTraining Loss: 0.017899 \tValidation Loss: 0.015095\n",
      "Validation loss decreased (0.015099 --> 0.015095).  Saving model ...\n",
      "Epoch: 150 \tTraining Loss: 0.017900 \tValidation Loss: 0.015082\n",
      "Validation loss decreased (0.015095 --> 0.015082).  Saving model ...\n",
      "Epoch: 151 \tTraining Loss: 0.017896 \tValidation Loss: 0.015083\n",
      "Epoch: 152 \tTraining Loss: 0.017821 \tValidation Loss: 0.015084\n",
      "Epoch: 153 \tTraining Loss: 0.017840 \tValidation Loss: 0.015084\n",
      "Epoch: 154 \tTraining Loss: 0.017843 \tValidation Loss: 0.015079\n",
      "Validation loss decreased (0.015082 --> 0.015079).  Saving model ...\n",
      "Epoch: 155 \tTraining Loss: 0.017801 \tValidation Loss: 0.015063\n",
      "Validation loss decreased (0.015079 --> 0.015063).  Saving model ...\n",
      "Epoch: 156 \tTraining Loss: 0.017830 \tValidation Loss: 0.015062\n",
      "Validation loss decreased (0.015063 --> 0.015062).  Saving model ...\n",
      "Epoch: 157 \tTraining Loss: 0.017770 \tValidation Loss: 0.015062\n",
      "Epoch: 158 \tTraining Loss: 0.017781 \tValidation Loss: 0.015054\n",
      "Validation loss decreased (0.015062 --> 0.015054).  Saving model ...\n",
      "Epoch: 159 \tTraining Loss: 0.017711 \tValidation Loss: 0.015061\n",
      "Epoch: 160 \tTraining Loss: 0.017722 \tValidation Loss: 0.015054\n",
      "Validation loss decreased (0.015054 --> 0.015054).  Saving model ...\n",
      "Epoch: 161 \tTraining Loss: 0.017656 \tValidation Loss: 0.015057\n",
      "Epoch: 162 \tTraining Loss: 0.017673 \tValidation Loss: 0.015055\n",
      "Epoch: 163 \tTraining Loss: 0.017653 \tValidation Loss: 0.015043\n",
      "Validation loss decreased (0.015054 --> 0.015043).  Saving model ...\n",
      "Epoch: 164 \tTraining Loss: 0.017628 \tValidation Loss: 0.015045\n",
      "Epoch: 165 \tTraining Loss: 0.017655 \tValidation Loss: 0.015038\n",
      "Validation loss decreased (0.015043 --> 0.015038).  Saving model ...\n",
      "Epoch: 166 \tTraining Loss: 0.017595 \tValidation Loss: 0.015042\n",
      "Epoch: 167 \tTraining Loss: 0.017590 \tValidation Loss: 0.015029\n",
      "Validation loss decreased (0.015038 --> 0.015029).  Saving model ...\n",
      "Epoch: 168 \tTraining Loss: 0.017544 \tValidation Loss: 0.015029\n",
      "Epoch: 169 \tTraining Loss: 0.017552 \tValidation Loss: 0.015022\n",
      "Validation loss decreased (0.015029 --> 0.015022).  Saving model ...\n",
      "Epoch: 170 \tTraining Loss: 0.017534 \tValidation Loss: 0.015017\n",
      "Validation loss decreased (0.015022 --> 0.015017).  Saving model ...\n",
      "Epoch: 171 \tTraining Loss: 0.017525 \tValidation Loss: 0.015014\n",
      "Validation loss decreased (0.015017 --> 0.015014).  Saving model ...\n",
      "Epoch: 172 \tTraining Loss: 0.017535 \tValidation Loss: 0.015017\n",
      "Epoch: 173 \tTraining Loss: 0.017484 \tValidation Loss: 0.015020\n",
      "Epoch: 174 \tTraining Loss: 0.017482 \tValidation Loss: 0.015017\n",
      "Epoch: 175 \tTraining Loss: 0.017450 \tValidation Loss: 0.015011\n",
      "Validation loss decreased (0.015014 --> 0.015011).  Saving model ...\n",
      "Epoch: 176 \tTraining Loss: 0.017430 \tValidation Loss: 0.015009\n",
      "Validation loss decreased (0.015011 --> 0.015009).  Saving model ...\n",
      "Epoch: 177 \tTraining Loss: 0.017390 \tValidation Loss: 0.015012\n",
      "Epoch: 178 \tTraining Loss: 0.017353 \tValidation Loss: 0.014994\n",
      "Validation loss decreased (0.015009 --> 0.014994).  Saving model ...\n",
      "Epoch: 179 \tTraining Loss: 0.017368 \tValidation Loss: 0.015004\n",
      "Epoch: 180 \tTraining Loss: 0.017348 \tValidation Loss: 0.015003\n",
      "Epoch: 181 \tTraining Loss: 0.017355 \tValidation Loss: 0.014996\n",
      "Epoch: 182 \tTraining Loss: 0.017351 \tValidation Loss: 0.014999\n",
      "Epoch: 183 \tTraining Loss: 0.017330 \tValidation Loss: 0.015008\n",
      "Epoch: 184 \tTraining Loss: 0.017281 \tValidation Loss: 0.014997\n",
      "Epoch: 185 \tTraining Loss: 0.017299 \tValidation Loss: 0.014992\n",
      "Validation loss decreased (0.014994 --> 0.014992).  Saving model ...\n",
      "Epoch: 186 \tTraining Loss: 0.017272 \tValidation Loss: 0.014985\n",
      "Validation loss decreased (0.014992 --> 0.014985).  Saving model ...\n",
      "Epoch: 187 \tTraining Loss: 0.017262 \tValidation Loss: 0.014994\n",
      "Epoch: 188 \tTraining Loss: 0.017262 \tValidation Loss: 0.014982\n",
      "Validation loss decreased (0.014985 --> 0.014982).  Saving model ...\n",
      "Epoch: 189 \tTraining Loss: 0.017259 \tValidation Loss: 0.014972\n",
      "Validation loss decreased (0.014982 --> 0.014972).  Saving model ...\n",
      "Epoch: 190 \tTraining Loss: 0.017198 \tValidation Loss: 0.014978\n",
      "Epoch: 191 \tTraining Loss: 0.017197 \tValidation Loss: 0.014986\n",
      "Epoch: 192 \tTraining Loss: 0.017196 \tValidation Loss: 0.014968\n",
      "Validation loss decreased (0.014972 --> 0.014968).  Saving model ...\n",
      "Epoch: 193 \tTraining Loss: 0.017159 \tValidation Loss: 0.014972\n",
      "Epoch: 194 \tTraining Loss: 0.017196 \tValidation Loss: 0.014980\n",
      "Epoch: 195 \tTraining Loss: 0.017133 \tValidation Loss: 0.014972\n",
      "Epoch: 196 \tTraining Loss: 0.017128 \tValidation Loss: 0.014973\n",
      "Epoch: 197 \tTraining Loss: 0.017116 \tValidation Loss: 0.014965\n",
      "Validation loss decreased (0.014968 --> 0.014965).  Saving model ...\n",
      "Epoch: 198 \tTraining Loss: 0.017110 \tValidation Loss: 0.014961\n",
      "Validation loss decreased (0.014965 --> 0.014961).  Saving model ...\n",
      "Epoch: 199 \tTraining Loss: 0.017087 \tValidation Loss: 0.014960\n",
      "Validation loss decreased (0.014961 --> 0.014960).  Saving model ...\n",
      "Epoch: 200 \tTraining Loss: 0.017107 \tValidation Loss: 0.014957\n",
      "Validation loss decreased (0.014960 --> 0.014957).  Saving model ...\n",
      "Epoch: 201 \tTraining Loss: 0.017073 \tValidation Loss: 0.014962\n",
      "Epoch: 202 \tTraining Loss: 0.017052 \tValidation Loss: 0.014965\n",
      "Epoch: 203 \tTraining Loss: 0.017056 \tValidation Loss: 0.014949\n",
      "Validation loss decreased (0.014957 --> 0.014949).  Saving model ...\n",
      "Epoch: 204 \tTraining Loss: 0.017052 \tValidation Loss: 0.014948\n",
      "Validation loss decreased (0.014949 --> 0.014948).  Saving model ...\n",
      "Epoch: 205 \tTraining Loss: 0.017034 \tValidation Loss: 0.014959\n",
      "Epoch: 206 \tTraining Loss: 0.016994 \tValidation Loss: 0.014948\n",
      "Validation loss decreased (0.014948 --> 0.014948).  Saving model ...\n",
      "Epoch: 207 \tTraining Loss: 0.017016 \tValidation Loss: 0.014948\n",
      "Epoch: 208 \tTraining Loss: 0.017003 \tValidation Loss: 0.014938\n",
      "Validation loss decreased (0.014948 --> 0.014938).  Saving model ...\n",
      "Epoch: 209 \tTraining Loss: 0.016967 \tValidation Loss: 0.014940\n",
      "Epoch: 210 \tTraining Loss: 0.016974 \tValidation Loss: 0.014937\n",
      "Validation loss decreased (0.014938 --> 0.014937).  Saving model ...\n",
      "Epoch: 211 \tTraining Loss: 0.016993 \tValidation Loss: 0.014951\n",
      "Epoch: 212 \tTraining Loss: 0.016953 \tValidation Loss: 0.014940\n",
      "Epoch: 213 \tTraining Loss: 0.016944 \tValidation Loss: 0.014944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 214 \tTraining Loss: 0.016931 \tValidation Loss: 0.014934\n",
      "Validation loss decreased (0.014937 --> 0.014934).  Saving model ...\n",
      "Epoch: 215 \tTraining Loss: 0.016882 \tValidation Loss: 0.014933\n",
      "Validation loss decreased (0.014934 --> 0.014933).  Saving model ...\n",
      "Epoch: 216 \tTraining Loss: 0.016910 \tValidation Loss: 0.014943\n",
      "Epoch: 217 \tTraining Loss: 0.016896 \tValidation Loss: 0.014937\n",
      "Epoch: 218 \tTraining Loss: 0.016871 \tValidation Loss: 0.014932\n",
      "Validation loss decreased (0.014933 --> 0.014932).  Saving model ...\n",
      "Epoch: 219 \tTraining Loss: 0.016868 \tValidation Loss: 0.014925\n",
      "Validation loss decreased (0.014932 --> 0.014925).  Saving model ...\n",
      "Epoch: 220 \tTraining Loss: 0.016855 \tValidation Loss: 0.014933\n",
      "Epoch: 221 \tTraining Loss: 0.016837 \tValidation Loss: 0.014926\n",
      "Epoch: 222 \tTraining Loss: 0.016823 \tValidation Loss: 0.014928\n",
      "Epoch: 223 \tTraining Loss: 0.016822 \tValidation Loss: 0.014928\n",
      "Epoch: 224 \tTraining Loss: 0.016853 \tValidation Loss: 0.014925\n",
      "Validation loss decreased (0.014925 --> 0.014925).  Saving model ...\n",
      "Epoch: 225 \tTraining Loss: 0.016832 \tValidation Loss: 0.014924\n",
      "Validation loss decreased (0.014925 --> 0.014924).  Saving model ...\n",
      "Epoch: 226 \tTraining Loss: 0.016833 \tValidation Loss: 0.014915\n",
      "Validation loss decreased (0.014924 --> 0.014915).  Saving model ...\n",
      "Epoch: 227 \tTraining Loss: 0.016795 \tValidation Loss: 0.014907\n",
      "Validation loss decreased (0.014915 --> 0.014907).  Saving model ...\n",
      "Epoch: 228 \tTraining Loss: 0.016780 \tValidation Loss: 0.014917\n",
      "Epoch: 229 \tTraining Loss: 0.016742 \tValidation Loss: 0.014908\n",
      "Epoch: 230 \tTraining Loss: 0.016759 \tValidation Loss: 0.014923\n",
      "Epoch: 231 \tTraining Loss: 0.016781 \tValidation Loss: 0.014911\n",
      "Epoch: 232 \tTraining Loss: 0.016765 \tValidation Loss: 0.014910\n",
      "Epoch: 233 \tTraining Loss: 0.016704 \tValidation Loss: 0.014905\n",
      "Validation loss decreased (0.014907 --> 0.014905).  Saving model ...\n",
      "Epoch: 234 \tTraining Loss: 0.016709 \tValidation Loss: 0.014911\n",
      "Epoch: 235 \tTraining Loss: 0.016721 \tValidation Loss: 0.014898\n",
      "Validation loss decreased (0.014905 --> 0.014898).  Saving model ...\n",
      "Epoch: 236 \tTraining Loss: 0.016734 \tValidation Loss: 0.014902\n",
      "Epoch: 237 \tTraining Loss: 0.016706 \tValidation Loss: 0.014897\n",
      "Validation loss decreased (0.014898 --> 0.014897).  Saving model ...\n",
      "Epoch: 238 \tTraining Loss: 0.016699 \tValidation Loss: 0.014902\n",
      "Epoch: 239 \tTraining Loss: 0.016683 \tValidation Loss: 0.014898\n",
      "Epoch: 240 \tTraining Loss: 0.016689 \tValidation Loss: 0.014895\n",
      "Validation loss decreased (0.014897 --> 0.014895).  Saving model ...\n",
      "Epoch: 241 \tTraining Loss: 0.016714 \tValidation Loss: 0.014895\n",
      "Epoch: 242 \tTraining Loss: 0.016606 \tValidation Loss: 0.014900\n",
      "Epoch: 243 \tTraining Loss: 0.016635 \tValidation Loss: 0.014891\n",
      "Validation loss decreased (0.014895 --> 0.014891).  Saving model ...\n",
      "Epoch: 244 \tTraining Loss: 0.016662 \tValidation Loss: 0.014892\n",
      "Epoch: 245 \tTraining Loss: 0.016622 \tValidation Loss: 0.014897\n",
      "Epoch: 246 \tTraining Loss: 0.016613 \tValidation Loss: 0.014892\n",
      "Epoch: 247 \tTraining Loss: 0.016613 \tValidation Loss: 0.014884\n",
      "Validation loss decreased (0.014891 --> 0.014884).  Saving model ...\n",
      "Epoch: 248 \tTraining Loss: 0.016578 \tValidation Loss: 0.014895\n",
      "Epoch: 249 \tTraining Loss: 0.016651 \tValidation Loss: 0.014888\n",
      "Epoch: 250 \tTraining Loss: 0.016591 \tValidation Loss: 0.014886\n",
      "Epoch: 251 \tTraining Loss: 0.016585 \tValidation Loss: 0.014891\n",
      "Epoch: 252 \tTraining Loss: 0.016582 \tValidation Loss: 0.014887\n",
      "Epoch: 253 \tTraining Loss: 0.016569 \tValidation Loss: 0.014884\n",
      "Validation loss decreased (0.014884 --> 0.014884).  Saving model ...\n",
      "Epoch: 254 \tTraining Loss: 0.016632 \tValidation Loss: 0.014888\n",
      "Epoch: 255 \tTraining Loss: 0.016588 \tValidation Loss: 0.014879\n",
      "Validation loss decreased (0.014884 --> 0.014879).  Saving model ...\n",
      "Epoch: 256 \tTraining Loss: 0.016572 \tValidation Loss: 0.014884\n",
      "Epoch: 257 \tTraining Loss: 0.016535 \tValidation Loss: 0.014885\n",
      "Epoch: 258 \tTraining Loss: 0.016534 \tValidation Loss: 0.014880\n",
      "Epoch: 259 \tTraining Loss: 0.016509 \tValidation Loss: 0.014875\n",
      "Validation loss decreased (0.014879 --> 0.014875).  Saving model ...\n",
      "Epoch: 260 \tTraining Loss: 0.016532 \tValidation Loss: 0.014875\n",
      "Epoch: 261 \tTraining Loss: 0.016535 \tValidation Loss: 0.014876\n",
      "Epoch: 262 \tTraining Loss: 0.016513 \tValidation Loss: 0.014882\n",
      "Epoch: 263 \tTraining Loss: 0.016539 \tValidation Loss: 0.014873\n",
      "Validation loss decreased (0.014875 --> 0.014873).  Saving model ...\n",
      "Epoch: 264 \tTraining Loss: 0.016491 \tValidation Loss: 0.014865\n",
      "Validation loss decreased (0.014873 --> 0.014865).  Saving model ...\n",
      "Epoch: 265 \tTraining Loss: 0.016505 \tValidation Loss: 0.014876\n",
      "Epoch: 266 \tTraining Loss: 0.016469 \tValidation Loss: 0.014867\n",
      "Epoch: 267 \tTraining Loss: 0.016481 \tValidation Loss: 0.014868\n",
      "Epoch: 268 \tTraining Loss: 0.016438 \tValidation Loss: 0.014864\n",
      "Validation loss decreased (0.014865 --> 0.014864).  Saving model ...\n",
      "Epoch: 269 \tTraining Loss: 0.016480 \tValidation Loss: 0.014861\n",
      "Validation loss decreased (0.014864 --> 0.014861).  Saving model ...\n",
      "Epoch: 270 \tTraining Loss: 0.016429 \tValidation Loss: 0.014862\n",
      "Epoch: 271 \tTraining Loss: 0.016439 \tValidation Loss: 0.014872\n",
      "Epoch: 272 \tTraining Loss: 0.016427 \tValidation Loss: 0.014866\n",
      "Epoch: 273 \tTraining Loss: 0.016450 \tValidation Loss: 0.014863\n",
      "Epoch: 274 \tTraining Loss: 0.016478 \tValidation Loss: 0.014866\n",
      "Epoch: 275 \tTraining Loss: 0.016421 \tValidation Loss: 0.014864\n",
      "Epoch: 276 \tTraining Loss: 0.016410 \tValidation Loss: 0.014874\n",
      "Epoch: 277 \tTraining Loss: 0.016421 \tValidation Loss: 0.014860\n",
      "Validation loss decreased (0.014861 --> 0.014860).  Saving model ...\n",
      "Epoch: 278 \tTraining Loss: 0.016418 \tValidation Loss: 0.014859\n",
      "Validation loss decreased (0.014860 --> 0.014859).  Saving model ...\n",
      "Epoch: 279 \tTraining Loss: 0.016396 \tValidation Loss: 0.014855\n",
      "Validation loss decreased (0.014859 --> 0.014855).  Saving model ...\n",
      "Epoch: 280 \tTraining Loss: 0.016393 \tValidation Loss: 0.014865\n",
      "Epoch: 281 \tTraining Loss: 0.016405 \tValidation Loss: 0.014860\n",
      "Epoch: 282 \tTraining Loss: 0.016366 \tValidation Loss: 0.014855\n",
      "Validation loss decreased (0.014855 --> 0.014855).  Saving model ...\n",
      "Epoch: 283 \tTraining Loss: 0.016346 \tValidation Loss: 0.014850\n",
      "Validation loss decreased (0.014855 --> 0.014850).  Saving model ...\n",
      "Epoch: 284 \tTraining Loss: 0.016306 \tValidation Loss: 0.014854\n",
      "Epoch: 285 \tTraining Loss: 0.016327 \tValidation Loss: 0.014852\n",
      "Epoch: 286 \tTraining Loss: 0.016327 \tValidation Loss: 0.014855\n",
      "Epoch: 287 \tTraining Loss: 0.016349 \tValidation Loss: 0.014847\n",
      "Validation loss decreased (0.014850 --> 0.014847).  Saving model ...\n",
      "Epoch: 288 \tTraining Loss: 0.016334 \tValidation Loss: 0.014848\n",
      "Epoch: 289 \tTraining Loss: 0.016344 \tValidation Loss: 0.014860\n",
      "Epoch: 290 \tTraining Loss: 0.016348 \tValidation Loss: 0.014849\n",
      "Epoch: 291 \tTraining Loss: 0.016350 \tValidation Loss: 0.014851\n",
      "Epoch: 292 \tTraining Loss: 0.016274 \tValidation Loss: 0.014851\n",
      "Epoch: 293 \tTraining Loss: 0.016355 \tValidation Loss: 0.014846\n",
      "Validation loss decreased (0.014847 --> 0.014846).  Saving model ...\n",
      "Epoch: 294 \tTraining Loss: 0.016326 \tValidation Loss: 0.014847\n",
      "Epoch: 295 \tTraining Loss: 0.016335 \tValidation Loss: 0.014844\n",
      "Validation loss decreased (0.014846 --> 0.014844).  Saving model ...\n",
      "Epoch: 296 \tTraining Loss: 0.016295 \tValidation Loss: 0.014836\n",
      "Validation loss decreased (0.014844 --> 0.014836).  Saving model ...\n",
      "Epoch: 297 \tTraining Loss: 0.016314 \tValidation Loss: 0.014839\n",
      "Epoch: 298 \tTraining Loss: 0.016287 \tValidation Loss: 0.014841\n",
      "Epoch: 299 \tTraining Loss: 0.016282 \tValidation Loss: 0.014839\n",
      "Epoch: 300 \tTraining Loss: 0.016306 \tValidation Loss: 0.014846\n",
      "Epoch: 301 \tTraining Loss: 0.016302 \tValidation Loss: 0.014849\n",
      "Epoch: 302 \tTraining Loss: 0.016270 \tValidation Loss: 0.014835\n",
      "Validation loss decreased (0.014836 --> 0.014835).  Saving model ...\n",
      "Epoch: 303 \tTraining Loss: 0.016284 \tValidation Loss: 0.014841\n",
      "Epoch: 304 \tTraining Loss: 0.016252 \tValidation Loss: 0.014835\n",
      "Validation loss decreased (0.014835 --> 0.014835).  Saving model ...\n",
      "Epoch: 305 \tTraining Loss: 0.016253 \tValidation Loss: 0.014838\n",
      "Epoch: 306 \tTraining Loss: 0.016265 \tValidation Loss: 0.014839\n",
      "Epoch: 307 \tTraining Loss: 0.016246 \tValidation Loss: 0.014832\n",
      "Validation loss decreased (0.014835 --> 0.014832).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 308 \tTraining Loss: 0.016275 \tValidation Loss: 0.014830\n",
      "Validation loss decreased (0.014832 --> 0.014830).  Saving model ...\n",
      "Epoch: 309 \tTraining Loss: 0.016230 \tValidation Loss: 0.014834\n",
      "Epoch: 310 \tTraining Loss: 0.016238 \tValidation Loss: 0.014842\n",
      "Epoch: 311 \tTraining Loss: 0.016234 \tValidation Loss: 0.014843\n",
      "Epoch: 312 \tTraining Loss: 0.016203 \tValidation Loss: 0.014841\n",
      "Epoch: 313 \tTraining Loss: 0.016228 \tValidation Loss: 0.014837\n",
      "Epoch: 314 \tTraining Loss: 0.016206 \tValidation Loss: 0.014840\n",
      "Epoch: 315 \tTraining Loss: 0.016169 \tValidation Loss: 0.014833\n",
      "Epoch: 316 \tTraining Loss: 0.016195 \tValidation Loss: 0.014836\n",
      "Epoch: 317 \tTraining Loss: 0.016202 \tValidation Loss: 0.014835\n",
      "Epoch: 318 \tTraining Loss: 0.016206 \tValidation Loss: 0.014825\n",
      "Validation loss decreased (0.014830 --> 0.014825).  Saving model ...\n",
      "Epoch: 319 \tTraining Loss: 0.016213 \tValidation Loss: 0.014833\n",
      "Epoch: 320 \tTraining Loss: 0.016198 \tValidation Loss: 0.014836\n",
      "Epoch: 321 \tTraining Loss: 0.016186 \tValidation Loss: 0.014833\n",
      "Epoch: 322 \tTraining Loss: 0.016174 \tValidation Loss: 0.014827\n",
      "Epoch: 323 \tTraining Loss: 0.016141 \tValidation Loss: 0.014828\n",
      "Epoch: 324 \tTraining Loss: 0.016161 \tValidation Loss: 0.014820\n",
      "Validation loss decreased (0.014825 --> 0.014820).  Saving model ...\n",
      "Epoch: 325 \tTraining Loss: 0.016144 \tValidation Loss: 0.014821\n",
      "Epoch: 326 \tTraining Loss: 0.016149 \tValidation Loss: 0.014826\n",
      "Epoch: 327 \tTraining Loss: 0.016158 \tValidation Loss: 0.014823\n",
      "Epoch: 328 \tTraining Loss: 0.016145 \tValidation Loss: 0.014832\n",
      "Epoch: 329 \tTraining Loss: 0.016138 \tValidation Loss: 0.014821\n",
      "Epoch: 330 \tTraining Loss: 0.016124 \tValidation Loss: 0.014822\n",
      "Epoch: 331 \tTraining Loss: 0.016133 \tValidation Loss: 0.014823\n",
      "Epoch: 332 \tTraining Loss: 0.016146 \tValidation Loss: 0.014821\n",
      "Epoch: 333 \tTraining Loss: 0.016144 \tValidation Loss: 0.014819\n",
      "Validation loss decreased (0.014820 --> 0.014819).  Saving model ...\n",
      "Epoch: 334 \tTraining Loss: 0.016134 \tValidation Loss: 0.014822\n",
      "Epoch: 335 \tTraining Loss: 0.016140 \tValidation Loss: 0.014816\n",
      "Validation loss decreased (0.014819 --> 0.014816).  Saving model ...\n",
      "Epoch: 336 \tTraining Loss: 0.016103 \tValidation Loss: 0.014822\n",
      "Epoch: 337 \tTraining Loss: 0.016121 \tValidation Loss: 0.014824\n",
      "Epoch: 338 \tTraining Loss: 0.016090 \tValidation Loss: 0.014824\n",
      "Epoch: 339 \tTraining Loss: 0.016064 \tValidation Loss: 0.014817\n",
      "Epoch: 340 \tTraining Loss: 0.016091 \tValidation Loss: 0.014813\n",
      "Validation loss decreased (0.014816 --> 0.014813).  Saving model ...\n",
      "Epoch: 341 \tTraining Loss: 0.016119 \tValidation Loss: 0.014817\n",
      "Epoch: 342 \tTraining Loss: 0.016122 \tValidation Loss: 0.014820\n",
      "Epoch: 343 \tTraining Loss: 0.016072 \tValidation Loss: 0.014818\n",
      "Epoch: 344 \tTraining Loss: 0.016066 \tValidation Loss: 0.014818\n",
      "Epoch: 345 \tTraining Loss: 0.016074 \tValidation Loss: 0.014815\n",
      "Epoch: 346 \tTraining Loss: 0.016093 \tValidation Loss: 0.014813\n",
      "Validation loss decreased (0.014813 --> 0.014813).  Saving model ...\n",
      "Epoch: 347 \tTraining Loss: 0.016061 \tValidation Loss: 0.014823\n",
      "Epoch: 348 \tTraining Loss: 0.016067 \tValidation Loss: 0.014811\n",
      "Validation loss decreased (0.014813 --> 0.014811).  Saving model ...\n",
      "Epoch: 349 \tTraining Loss: 0.016051 \tValidation Loss: 0.014811\n",
      "Validation loss decreased (0.014811 --> 0.014811).  Saving model ...\n",
      "Epoch: 350 \tTraining Loss: 0.016059 \tValidation Loss: 0.014811\n",
      "Epoch: 351 \tTraining Loss: 0.016047 \tValidation Loss: 0.014813\n",
      "Epoch: 352 \tTraining Loss: 0.016043 \tValidation Loss: 0.014812\n",
      "Epoch: 353 \tTraining Loss: 0.016033 \tValidation Loss: 0.014814\n",
      "Epoch: 354 \tTraining Loss: 0.016042 \tValidation Loss: 0.014809\n",
      "Validation loss decreased (0.014811 --> 0.014809).  Saving model ...\n",
      "Epoch: 355 \tTraining Loss: 0.016041 \tValidation Loss: 0.014809\n",
      "Validation loss decreased (0.014809 --> 0.014809).  Saving model ...\n",
      "Epoch: 356 \tTraining Loss: 0.016045 \tValidation Loss: 0.014810\n",
      "Epoch: 357 \tTraining Loss: 0.016012 \tValidation Loss: 0.014799\n",
      "Validation loss decreased (0.014809 --> 0.014799).  Saving model ...\n",
      "Epoch: 358 \tTraining Loss: 0.016021 \tValidation Loss: 0.014809\n",
      "Epoch: 359 \tTraining Loss: 0.016011 \tValidation Loss: 0.014797\n",
      "Validation loss decreased (0.014799 --> 0.014797).  Saving model ...\n",
      "Epoch: 360 \tTraining Loss: 0.016018 \tValidation Loss: 0.014809\n",
      "Epoch: 361 \tTraining Loss: 0.016025 \tValidation Loss: 0.014811\n",
      "Epoch: 362 \tTraining Loss: 0.015995 \tValidation Loss: 0.014812\n",
      "Epoch: 363 \tTraining Loss: 0.016025 \tValidation Loss: 0.014803\n",
      "Epoch: 364 \tTraining Loss: 0.016025 \tValidation Loss: 0.014802\n",
      "Epoch: 365 \tTraining Loss: 0.015965 \tValidation Loss: 0.014807\n",
      "Epoch: 366 \tTraining Loss: 0.016003 \tValidation Loss: 0.014806\n",
      "Epoch: 367 \tTraining Loss: 0.016001 \tValidation Loss: 0.014807\n",
      "Epoch: 368 \tTraining Loss: 0.015975 \tValidation Loss: 0.014804\n",
      "Epoch: 369 \tTraining Loss: 0.016005 \tValidation Loss: 0.014805\n",
      "Epoch: 370 \tTraining Loss: 0.015989 \tValidation Loss: 0.014797\n",
      "Validation loss decreased (0.014797 --> 0.014797).  Saving model ...\n",
      "Epoch: 371 \tTraining Loss: 0.015961 \tValidation Loss: 0.014791\n",
      "Validation loss decreased (0.014797 --> 0.014791).  Saving model ...\n",
      "Epoch: 372 \tTraining Loss: 0.016005 \tValidation Loss: 0.014795\n",
      "Epoch: 373 \tTraining Loss: 0.015980 \tValidation Loss: 0.014803\n",
      "Epoch: 374 \tTraining Loss: 0.015971 \tValidation Loss: 0.014800\n",
      "Epoch: 375 \tTraining Loss: 0.015957 \tValidation Loss: 0.014804\n",
      "Epoch: 376 \tTraining Loss: 0.015959 \tValidation Loss: 0.014796\n",
      "Epoch: 377 \tTraining Loss: 0.015972 \tValidation Loss: 0.014799\n",
      "Epoch: 378 \tTraining Loss: 0.015959 \tValidation Loss: 0.014795\n",
      "Epoch: 379 \tTraining Loss: 0.015963 \tValidation Loss: 0.014797\n",
      "Epoch: 380 \tTraining Loss: 0.015951 \tValidation Loss: 0.014796\n",
      "Epoch: 381 \tTraining Loss: 0.015939 \tValidation Loss: 0.014790\n",
      "Validation loss decreased (0.014791 --> 0.014790).  Saving model ...\n",
      "Epoch: 382 \tTraining Loss: 0.015957 \tValidation Loss: 0.014797\n",
      "Epoch: 383 \tTraining Loss: 0.015935 \tValidation Loss: 0.014789\n",
      "Validation loss decreased (0.014790 --> 0.014789).  Saving model ...\n",
      "Epoch: 384 \tTraining Loss: 0.015942 \tValidation Loss: 0.014796\n",
      "Epoch: 385 \tTraining Loss: 0.015942 \tValidation Loss: 0.014789\n",
      "Epoch: 386 \tTraining Loss: 0.015934 \tValidation Loss: 0.014795\n",
      "Epoch: 387 \tTraining Loss: 0.015915 \tValidation Loss: 0.014791\n",
      "Epoch: 388 \tTraining Loss: 0.015910 \tValidation Loss: 0.014796\n",
      "Epoch: 389 \tTraining Loss: 0.015930 \tValidation Loss: 0.014792\n",
      "Epoch: 390 \tTraining Loss: 0.015923 \tValidation Loss: 0.014786\n",
      "Validation loss decreased (0.014789 --> 0.014786).  Saving model ...\n",
      "Epoch: 391 \tTraining Loss: 0.015895 \tValidation Loss: 0.014792\n",
      "Epoch: 392 \tTraining Loss: 0.015910 \tValidation Loss: 0.014796\n",
      "Epoch: 393 \tTraining Loss: 0.015886 \tValidation Loss: 0.014792\n",
      "Epoch: 394 \tTraining Loss: 0.015911 \tValidation Loss: 0.014788\n",
      "Epoch: 395 \tTraining Loss: 0.015905 \tValidation Loss: 0.014787\n",
      "Epoch: 396 \tTraining Loss: 0.015886 \tValidation Loss: 0.014789\n",
      "Epoch: 397 \tTraining Loss: 0.015894 \tValidation Loss: 0.014784\n",
      "Validation loss decreased (0.014786 --> 0.014784).  Saving model ...\n",
      "Epoch: 398 \tTraining Loss: 0.015922 \tValidation Loss: 0.014789\n",
      "Epoch: 399 \tTraining Loss: 0.015876 \tValidation Loss: 0.014789\n",
      "Epoch: 400 \tTraining Loss: 0.015888 \tValidation Loss: 0.014788\n",
      "Epoch: 401 \tTraining Loss: 0.015882 \tValidation Loss: 0.014784\n",
      "Validation loss decreased (0.014784 --> 0.014784).  Saving model ...\n",
      "Epoch: 402 \tTraining Loss: 0.015873 \tValidation Loss: 0.014789\n",
      "Epoch: 403 \tTraining Loss: 0.015889 \tValidation Loss: 0.014797\n",
      "Epoch: 404 \tTraining Loss: 0.015871 \tValidation Loss: 0.014782\n",
      "Validation loss decreased (0.014784 --> 0.014782).  Saving model ...\n",
      "Epoch: 405 \tTraining Loss: 0.015857 \tValidation Loss: 0.014781\n",
      "Validation loss decreased (0.014782 --> 0.014781).  Saving model ...\n",
      "Epoch: 406 \tTraining Loss: 0.015861 \tValidation Loss: 0.014786\n",
      "Epoch: 407 \tTraining Loss: 0.015858 \tValidation Loss: 0.014789\n",
      "Epoch: 408 \tTraining Loss: 0.015846 \tValidation Loss: 0.014787\n",
      "Epoch: 409 \tTraining Loss: 0.015864 \tValidation Loss: 0.014782\n",
      "Epoch: 410 \tTraining Loss: 0.015849 \tValidation Loss: 0.014783\n",
      "Epoch: 411 \tTraining Loss: 0.015856 \tValidation Loss: 0.014782\n",
      "Epoch: 412 \tTraining Loss: 0.015853 \tValidation Loss: 0.014779\n",
      "Validation loss decreased (0.014781 --> 0.014779).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 413 \tTraining Loss: 0.015843 \tValidation Loss: 0.014780\n",
      "Epoch: 414 \tTraining Loss: 0.015849 \tValidation Loss: 0.014778\n",
      "Validation loss decreased (0.014779 --> 0.014778).  Saving model ...\n",
      "Epoch: 415 \tTraining Loss: 0.015834 \tValidation Loss: 0.014786\n",
      "Epoch: 416 \tTraining Loss: 0.015836 \tValidation Loss: 0.014777\n",
      "Validation loss decreased (0.014778 --> 0.014777).  Saving model ...\n",
      "Epoch: 417 \tTraining Loss: 0.015849 \tValidation Loss: 0.014780\n",
      "Epoch: 418 \tTraining Loss: 0.015842 \tValidation Loss: 0.014775\n",
      "Validation loss decreased (0.014777 --> 0.014775).  Saving model ...\n",
      "Epoch: 419 \tTraining Loss: 0.015820 \tValidation Loss: 0.014774\n",
      "Validation loss decreased (0.014775 --> 0.014774).  Saving model ...\n",
      "Epoch: 420 \tTraining Loss: 0.015843 \tValidation Loss: 0.014786\n",
      "Epoch: 421 \tTraining Loss: 0.015817 \tValidation Loss: 0.014780\n",
      "Epoch: 422 \tTraining Loss: 0.015810 \tValidation Loss: 0.014776\n",
      "Epoch: 423 \tTraining Loss: 0.015795 \tValidation Loss: 0.014780\n",
      "Epoch: 424 \tTraining Loss: 0.015842 \tValidation Loss: 0.014773\n",
      "Validation loss decreased (0.014774 --> 0.014773).  Saving model ...\n",
      "Epoch: 425 \tTraining Loss: 0.015801 \tValidation Loss: 0.014777\n",
      "Epoch: 426 \tTraining Loss: 0.015820 \tValidation Loss: 0.014777\n",
      "Epoch: 427 \tTraining Loss: 0.015821 \tValidation Loss: 0.014785\n",
      "Epoch: 428 \tTraining Loss: 0.015820 \tValidation Loss: 0.014776\n",
      "Epoch: 429 \tTraining Loss: 0.015808 \tValidation Loss: 0.014775\n",
      "Epoch: 430 \tTraining Loss: 0.015814 \tValidation Loss: 0.014774\n",
      "Epoch: 431 \tTraining Loss: 0.015786 \tValidation Loss: 0.014779\n",
      "Epoch: 432 \tTraining Loss: 0.015801 \tValidation Loss: 0.014774\n",
      "Epoch: 433 \tTraining Loss: 0.015783 \tValidation Loss: 0.014775\n",
      "Epoch: 434 \tTraining Loss: 0.015791 \tValidation Loss: 0.014776\n",
      "Epoch: 435 \tTraining Loss: 0.015775 \tValidation Loss: 0.014773\n",
      "Validation loss decreased (0.014773 --> 0.014773).  Saving model ...\n",
      "Epoch: 436 \tTraining Loss: 0.015794 \tValidation Loss: 0.014772\n",
      "Validation loss decreased (0.014773 --> 0.014772).  Saving model ...\n",
      "Epoch: 437 \tTraining Loss: 0.015791 \tValidation Loss: 0.014776\n",
      "Epoch: 438 \tTraining Loss: 0.015780 \tValidation Loss: 0.014773\n",
      "Epoch: 439 \tTraining Loss: 0.015766 \tValidation Loss: 0.014771\n",
      "Validation loss decreased (0.014772 --> 0.014771).  Saving model ...\n",
      "Epoch: 440 \tTraining Loss: 0.015781 \tValidation Loss: 0.014767\n",
      "Validation loss decreased (0.014771 --> 0.014767).  Saving model ...\n",
      "Epoch: 441 \tTraining Loss: 0.015794 \tValidation Loss: 0.014763\n",
      "Validation loss decreased (0.014767 --> 0.014763).  Saving model ...\n",
      "Epoch: 442 \tTraining Loss: 0.015798 \tValidation Loss: 0.014769\n",
      "Epoch: 443 \tTraining Loss: 0.015790 \tValidation Loss: 0.014770\n",
      "Epoch: 444 \tTraining Loss: 0.015808 \tValidation Loss: 0.014771\n",
      "Epoch: 445 \tTraining Loss: 0.015758 \tValidation Loss: 0.014769\n",
      "Epoch: 446 \tTraining Loss: 0.015751 \tValidation Loss: 0.014763\n",
      "Validation loss decreased (0.014763 --> 0.014763).  Saving model ...\n",
      "Epoch: 447 \tTraining Loss: 0.015755 \tValidation Loss: 0.014767\n",
      "Epoch: 448 \tTraining Loss: 0.015768 \tValidation Loss: 0.014764\n",
      "Epoch: 449 \tTraining Loss: 0.015749 \tValidation Loss: 0.014770\n",
      "Epoch: 450 \tTraining Loss: 0.015764 \tValidation Loss: 0.014770\n",
      "Epoch: 451 \tTraining Loss: 0.015738 \tValidation Loss: 0.014765\n",
      "Epoch: 452 \tTraining Loss: 0.015756 \tValidation Loss: 0.014772\n",
      "Epoch: 453 \tTraining Loss: 0.015764 \tValidation Loss: 0.014769\n",
      "Epoch: 454 \tTraining Loss: 0.015754 \tValidation Loss: 0.014770\n",
      "Epoch: 455 \tTraining Loss: 0.015722 \tValidation Loss: 0.014765\n",
      "Epoch: 456 \tTraining Loss: 0.015770 \tValidation Loss: 0.014769\n",
      "Epoch: 457 \tTraining Loss: 0.015750 \tValidation Loss: 0.014763\n",
      "Epoch: 458 \tTraining Loss: 0.015726 \tValidation Loss: 0.014766\n",
      "Epoch: 459 \tTraining Loss: 0.015739 \tValidation Loss: 0.014768\n",
      "Epoch: 460 \tTraining Loss: 0.015715 \tValidation Loss: 0.014765\n",
      "Epoch: 461 \tTraining Loss: 0.015737 \tValidation Loss: 0.014762\n",
      "Validation loss decreased (0.014763 --> 0.014762).  Saving model ...\n",
      "Epoch: 462 \tTraining Loss: 0.015724 \tValidation Loss: 0.014761\n",
      "Validation loss decreased (0.014762 --> 0.014761).  Saving model ...\n",
      "Epoch: 463 \tTraining Loss: 0.015719 \tValidation Loss: 0.014764\n",
      "Epoch: 464 \tTraining Loss: 0.015735 \tValidation Loss: 0.014765\n",
      "Epoch: 465 \tTraining Loss: 0.015726 \tValidation Loss: 0.014766\n",
      "Epoch: 466 \tTraining Loss: 0.015744 \tValidation Loss: 0.014757\n",
      "Validation loss decreased (0.014761 --> 0.014757).  Saving model ...\n",
      "Epoch: 467 \tTraining Loss: 0.015697 \tValidation Loss: 0.014757\n",
      "Validation loss decreased (0.014757 --> 0.014757).  Saving model ...\n",
      "Epoch: 468 \tTraining Loss: 0.015731 \tValidation Loss: 0.014763\n",
      "Epoch: 469 \tTraining Loss: 0.015703 \tValidation Loss: 0.014760\n",
      "Epoch: 470 \tTraining Loss: 0.015697 \tValidation Loss: 0.014760\n",
      "Epoch: 471 \tTraining Loss: 0.015690 \tValidation Loss: 0.014757\n",
      "Epoch: 472 \tTraining Loss: 0.015715 \tValidation Loss: 0.014762\n",
      "Epoch: 473 \tTraining Loss: 0.015697 \tValidation Loss: 0.014764\n",
      "Epoch: 474 \tTraining Loss: 0.015702 \tValidation Loss: 0.014759\n",
      "Epoch: 475 \tTraining Loss: 0.015702 \tValidation Loss: 0.014762\n",
      "Epoch: 476 \tTraining Loss: 0.015714 \tValidation Loss: 0.014761\n",
      "Epoch: 477 \tTraining Loss: 0.015701 \tValidation Loss: 0.014758\n",
      "Epoch: 478 \tTraining Loss: 0.015707 \tValidation Loss: 0.014760\n",
      "Epoch: 479 \tTraining Loss: 0.015683 \tValidation Loss: 0.014756\n",
      "Validation loss decreased (0.014757 --> 0.014756).  Saving model ...\n",
      "Epoch: 480 \tTraining Loss: 0.015690 \tValidation Loss: 0.014760\n",
      "Epoch: 481 \tTraining Loss: 0.015679 \tValidation Loss: 0.014757\n",
      "Epoch: 482 \tTraining Loss: 0.015668 \tValidation Loss: 0.014760\n",
      "Epoch: 483 \tTraining Loss: 0.015669 \tValidation Loss: 0.014749\n",
      "Validation loss decreased (0.014756 --> 0.014749).  Saving model ...\n",
      "Epoch: 484 \tTraining Loss: 0.015663 \tValidation Loss: 0.014755\n",
      "Epoch: 485 \tTraining Loss: 0.015670 \tValidation Loss: 0.014754\n",
      "Epoch: 486 \tTraining Loss: 0.015675 \tValidation Loss: 0.014754\n",
      "Epoch: 487 \tTraining Loss: 0.015661 \tValidation Loss: 0.014762\n",
      "Epoch: 488 \tTraining Loss: 0.015645 \tValidation Loss: 0.014752\n",
      "Epoch: 489 \tTraining Loss: 0.015676 \tValidation Loss: 0.014752\n",
      "Epoch: 490 \tTraining Loss: 0.015664 \tValidation Loss: 0.014750\n",
      "Epoch: 491 \tTraining Loss: 0.015660 \tValidation Loss: 0.014752\n",
      "Epoch: 492 \tTraining Loss: 0.015655 \tValidation Loss: 0.014748\n",
      "Validation loss decreased (0.014749 --> 0.014748).  Saving model ...\n",
      "Epoch: 493 \tTraining Loss: 0.015668 \tValidation Loss: 0.014755\n",
      "Epoch: 494 \tTraining Loss: 0.015639 \tValidation Loss: 0.014754\n",
      "Epoch: 495 \tTraining Loss: 0.015641 \tValidation Loss: 0.014759\n",
      "Epoch: 496 \tTraining Loss: 0.015674 \tValidation Loss: 0.014758\n",
      "Epoch: 497 \tTraining Loss: 0.015663 \tValidation Loss: 0.014750\n",
      "Epoch: 498 \tTraining Loss: 0.015634 \tValidation Loss: 0.014756\n",
      "Epoch: 499 \tTraining Loss: 0.015656 \tValidation Loss: 0.014753\n",
      "Epoch: 500 \tTraining Loss: 0.015663 \tValidation Loss: 0.014749\n",
      "Epoch: 501 \tTraining Loss: 0.015636 \tValidation Loss: 0.014751\n",
      "Epoch: 502 \tTraining Loss: 0.015641 \tValidation Loss: 0.014758\n",
      "Epoch: 503 \tTraining Loss: 0.015631 \tValidation Loss: 0.014750\n",
      "Epoch: 504 \tTraining Loss: 0.015640 \tValidation Loss: 0.014751\n",
      "Epoch: 505 \tTraining Loss: 0.015622 \tValidation Loss: 0.014752\n",
      "Epoch: 506 \tTraining Loss: 0.015618 \tValidation Loss: 0.014747\n",
      "Validation loss decreased (0.014748 --> 0.014747).  Saving model ...\n",
      "Epoch: 507 \tTraining Loss: 0.015619 \tValidation Loss: 0.014745\n",
      "Validation loss decreased (0.014747 --> 0.014745).  Saving model ...\n",
      "Epoch: 508 \tTraining Loss: 0.015606 \tValidation Loss: 0.014747\n",
      "Epoch: 509 \tTraining Loss: 0.015616 \tValidation Loss: 0.014750\n",
      "Epoch: 510 \tTraining Loss: 0.015603 \tValidation Loss: 0.014752\n",
      "Epoch: 511 \tTraining Loss: 0.015640 \tValidation Loss: 0.014743\n",
      "Validation loss decreased (0.014745 --> 0.014743).  Saving model ...\n",
      "Epoch: 512 \tTraining Loss: 0.015623 \tValidation Loss: 0.014751\n",
      "Epoch: 513 \tTraining Loss: 0.015620 \tValidation Loss: 0.014747\n",
      "Epoch: 514 \tTraining Loss: 0.015602 \tValidation Loss: 0.014749\n",
      "Epoch: 515 \tTraining Loss: 0.015624 \tValidation Loss: 0.014750\n",
      "Epoch: 516 \tTraining Loss: 0.015612 \tValidation Loss: 0.014748\n",
      "Epoch: 517 \tTraining Loss: 0.015622 \tValidation Loss: 0.014745\n",
      "Epoch: 518 \tTraining Loss: 0.015586 \tValidation Loss: 0.014746\n",
      "Epoch: 519 \tTraining Loss: 0.015607 \tValidation Loss: 0.014743\n",
      "Epoch: 520 \tTraining Loss: 0.015618 \tValidation Loss: 0.014743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 521 \tTraining Loss: 0.015622 \tValidation Loss: 0.014748\n",
      "Epoch: 522 \tTraining Loss: 0.015610 \tValidation Loss: 0.014744\n",
      "Epoch: 523 \tTraining Loss: 0.015619 \tValidation Loss: 0.014739\n",
      "Validation loss decreased (0.014743 --> 0.014739).  Saving model ...\n",
      "Epoch: 524 \tTraining Loss: 0.015606 \tValidation Loss: 0.014747\n",
      "Epoch: 525 \tTraining Loss: 0.015565 \tValidation Loss: 0.014741\n",
      "Epoch: 526 \tTraining Loss: 0.015590 \tValidation Loss: 0.014738\n",
      "Validation loss decreased (0.014739 --> 0.014738).  Saving model ...\n",
      "Epoch: 527 \tTraining Loss: 0.015607 \tValidation Loss: 0.014742\n",
      "Epoch: 528 \tTraining Loss: 0.015607 \tValidation Loss: 0.014736\n",
      "Validation loss decreased (0.014738 --> 0.014736).  Saving model ...\n",
      "Epoch: 529 \tTraining Loss: 0.015578 \tValidation Loss: 0.014743\n",
      "Epoch: 530 \tTraining Loss: 0.015619 \tValidation Loss: 0.014742\n",
      "Epoch: 531 \tTraining Loss: 0.015588 \tValidation Loss: 0.014745\n",
      "Epoch: 532 \tTraining Loss: 0.015595 \tValidation Loss: 0.014745\n",
      "Epoch: 533 \tTraining Loss: 0.015592 \tValidation Loss: 0.014737\n",
      "Epoch: 534 \tTraining Loss: 0.015545 \tValidation Loss: 0.014750\n",
      "Epoch: 535 \tTraining Loss: 0.015571 \tValidation Loss: 0.014744\n",
      "Epoch: 536 \tTraining Loss: 0.015587 \tValidation Loss: 0.014732\n",
      "Validation loss decreased (0.014736 --> 0.014732).  Saving model ...\n",
      "Epoch: 537 \tTraining Loss: 0.015577 \tValidation Loss: 0.014740\n",
      "Epoch: 538 \tTraining Loss: 0.015576 \tValidation Loss: 0.014738\n",
      "Epoch: 539 \tTraining Loss: 0.015581 \tValidation Loss: 0.014743\n",
      "Epoch: 540 \tTraining Loss: 0.015584 \tValidation Loss: 0.014744\n",
      "Epoch: 541 \tTraining Loss: 0.015568 \tValidation Loss: 0.014751\n",
      "Epoch: 542 \tTraining Loss: 0.015573 \tValidation Loss: 0.014739\n",
      "Epoch: 543 \tTraining Loss: 0.015559 \tValidation Loss: 0.014740\n",
      "Epoch: 544 \tTraining Loss: 0.015553 \tValidation Loss: 0.014741\n",
      "Epoch: 545 \tTraining Loss: 0.015552 \tValidation Loss: 0.014737\n",
      "Epoch: 546 \tTraining Loss: 0.015550 \tValidation Loss: 0.014740\n",
      "Epoch: 547 \tTraining Loss: 0.015555 \tValidation Loss: 0.014746\n",
      "Epoch: 548 \tTraining Loss: 0.015558 \tValidation Loss: 0.014737\n",
      "Epoch: 549 \tTraining Loss: 0.015552 \tValidation Loss: 0.014737\n",
      "Epoch: 550 \tTraining Loss: 0.015553 \tValidation Loss: 0.014733\n",
      "Epoch: 551 \tTraining Loss: 0.015575 \tValidation Loss: 0.014733\n",
      "Epoch: 552 \tTraining Loss: 0.015573 \tValidation Loss: 0.014732\n",
      "Validation loss decreased (0.014732 --> 0.014732).  Saving model ...\n",
      "Epoch: 553 \tTraining Loss: 0.015548 \tValidation Loss: 0.014735\n",
      "Epoch: 554 \tTraining Loss: 0.015554 \tValidation Loss: 0.014741\n",
      "Epoch: 555 \tTraining Loss: 0.015552 \tValidation Loss: 0.014733\n",
      "Epoch: 556 \tTraining Loss: 0.015542 \tValidation Loss: 0.014738\n",
      "Epoch: 557 \tTraining Loss: 0.015545 \tValidation Loss: 0.014737\n",
      "Epoch: 558 \tTraining Loss: 0.015531 \tValidation Loss: 0.014736\n",
      "Epoch: 559 \tTraining Loss: 0.015518 \tValidation Loss: 0.014734\n",
      "Epoch: 560 \tTraining Loss: 0.015536 \tValidation Loss: 0.014734\n",
      "Epoch: 561 \tTraining Loss: 0.015545 \tValidation Loss: 0.014732\n",
      "Epoch: 562 \tTraining Loss: 0.015560 \tValidation Loss: 0.014727\n",
      "Validation loss decreased (0.014732 --> 0.014727).  Saving model ...\n",
      "Epoch: 563 \tTraining Loss: 0.015550 \tValidation Loss: 0.014736\n",
      "Epoch: 564 \tTraining Loss: 0.015516 \tValidation Loss: 0.014741\n",
      "Epoch: 565 \tTraining Loss: 0.015535 \tValidation Loss: 0.014735\n",
      "Epoch: 566 \tTraining Loss: 0.015519 \tValidation Loss: 0.014732\n",
      "Epoch: 567 \tTraining Loss: 0.015545 \tValidation Loss: 0.014737\n",
      "Epoch: 568 \tTraining Loss: 0.015535 \tValidation Loss: 0.014728\n",
      "Epoch: 569 \tTraining Loss: 0.015534 \tValidation Loss: 0.014736\n",
      "Epoch: 570 \tTraining Loss: 0.015514 \tValidation Loss: 0.014736\n",
      "Epoch: 571 \tTraining Loss: 0.015508 \tValidation Loss: 0.014724\n",
      "Validation loss decreased (0.014727 --> 0.014724).  Saving model ...\n",
      "Epoch: 572 \tTraining Loss: 0.015511 \tValidation Loss: 0.014730\n",
      "Epoch: 573 \tTraining Loss: 0.015523 \tValidation Loss: 0.014728\n",
      "Epoch: 574 \tTraining Loss: 0.015519 \tValidation Loss: 0.014731\n",
      "Epoch: 575 \tTraining Loss: 0.015523 \tValidation Loss: 0.014736\n",
      "Epoch: 576 \tTraining Loss: 0.015513 \tValidation Loss: 0.014728\n",
      "Epoch: 577 \tTraining Loss: 0.015549 \tValidation Loss: 0.014736\n",
      "Epoch: 578 \tTraining Loss: 0.015505 \tValidation Loss: 0.014727\n",
      "Epoch: 579 \tTraining Loss: 0.015497 \tValidation Loss: 0.014741\n",
      "Epoch: 580 \tTraining Loss: 0.015517 \tValidation Loss: 0.014729\n",
      "Epoch: 581 \tTraining Loss: 0.015492 \tValidation Loss: 0.014732\n",
      "Epoch: 582 \tTraining Loss: 0.015485 \tValidation Loss: 0.014726\n",
      "Epoch: 583 \tTraining Loss: 0.015524 \tValidation Loss: 0.014729\n",
      "Epoch: 584 \tTraining Loss: 0.015526 \tValidation Loss: 0.014728\n",
      "Epoch: 585 \tTraining Loss: 0.015512 \tValidation Loss: 0.014726\n",
      "Epoch: 586 \tTraining Loss: 0.015499 \tValidation Loss: 0.014727\n",
      "Epoch: 587 \tTraining Loss: 0.015492 \tValidation Loss: 0.014723\n",
      "Validation loss decreased (0.014724 --> 0.014723).  Saving model ...\n",
      "Epoch: 588 \tTraining Loss: 0.015513 \tValidation Loss: 0.014730\n",
      "Epoch: 589 \tTraining Loss: 0.015508 \tValidation Loss: 0.014721\n",
      "Validation loss decreased (0.014723 --> 0.014721).  Saving model ...\n",
      "Epoch: 590 \tTraining Loss: 0.015497 \tValidation Loss: 0.014730\n",
      "Epoch: 591 \tTraining Loss: 0.015478 \tValidation Loss: 0.014728\n",
      "Epoch: 592 \tTraining Loss: 0.015473 \tValidation Loss: 0.014723\n",
      "Epoch: 593 \tTraining Loss: 0.015477 \tValidation Loss: 0.014736\n",
      "Epoch: 594 \tTraining Loss: 0.015485 \tValidation Loss: 0.014723\n",
      "Epoch: 595 \tTraining Loss: 0.015477 \tValidation Loss: 0.014724\n",
      "Epoch: 596 \tTraining Loss: 0.015445 \tValidation Loss: 0.014733\n",
      "Epoch: 597 \tTraining Loss: 0.015482 \tValidation Loss: 0.014719\n",
      "Validation loss decreased (0.014721 --> 0.014719).  Saving model ...\n",
      "Epoch: 598 \tTraining Loss: 0.015474 \tValidation Loss: 0.014732\n",
      "Epoch: 599 \tTraining Loss: 0.015480 \tValidation Loss: 0.014725\n",
      "Epoch: 600 \tTraining Loss: 0.015484 \tValidation Loss: 0.014725\n",
      "Epoch: 601 \tTraining Loss: 0.015469 \tValidation Loss: 0.014731\n",
      "Epoch: 602 \tTraining Loss: 0.015479 \tValidation Loss: 0.014725\n",
      "Epoch: 603 \tTraining Loss: 0.015501 \tValidation Loss: 0.014726\n",
      "Epoch: 604 \tTraining Loss: 0.015457 \tValidation Loss: 0.014719\n",
      "Epoch: 605 \tTraining Loss: 0.015492 \tValidation Loss: 0.014723\n",
      "Epoch: 606 \tTraining Loss: 0.015469 \tValidation Loss: 0.014723\n",
      "Epoch: 607 \tTraining Loss: 0.015456 \tValidation Loss: 0.014724\n",
      "Epoch: 608 \tTraining Loss: 0.015466 \tValidation Loss: 0.014728\n",
      "Epoch: 609 \tTraining Loss: 0.015433 \tValidation Loss: 0.014720\n",
      "Epoch: 610 \tTraining Loss: 0.015466 \tValidation Loss: 0.014722\n",
      "Epoch: 611 \tTraining Loss: 0.015477 \tValidation Loss: 0.014718\n",
      "Validation loss decreased (0.014719 --> 0.014718).  Saving model ...\n",
      "Epoch: 612 \tTraining Loss: 0.015470 \tValidation Loss: 0.014724\n",
      "Epoch: 613 \tTraining Loss: 0.015455 \tValidation Loss: 0.014726\n",
      "Epoch: 614 \tTraining Loss: 0.015473 \tValidation Loss: 0.014728\n",
      "Epoch: 615 \tTraining Loss: 0.015450 \tValidation Loss: 0.014722\n",
      "Epoch: 616 \tTraining Loss: 0.015470 \tValidation Loss: 0.014713\n",
      "Validation loss decreased (0.014718 --> 0.014713).  Saving model ...\n",
      "Epoch: 617 \tTraining Loss: 0.015454 \tValidation Loss: 0.014727\n",
      "Epoch: 618 \tTraining Loss: 0.015438 \tValidation Loss: 0.014718\n",
      "Epoch: 619 \tTraining Loss: 0.015436 \tValidation Loss: 0.014721\n",
      "Epoch: 620 \tTraining Loss: 0.015426 \tValidation Loss: 0.014720\n",
      "Epoch: 621 \tTraining Loss: 0.015432 \tValidation Loss: 0.014715\n",
      "Epoch: 622 \tTraining Loss: 0.015458 \tValidation Loss: 0.014718\n",
      "Epoch: 623 \tTraining Loss: 0.015444 \tValidation Loss: 0.014722\n",
      "Epoch: 624 \tTraining Loss: 0.015435 \tValidation Loss: 0.014716\n",
      "Epoch: 625 \tTraining Loss: 0.015431 \tValidation Loss: 0.014715\n",
      "Epoch: 626 \tTraining Loss: 0.015456 \tValidation Loss: 0.014723\n",
      "Epoch: 627 \tTraining Loss: 0.015443 \tValidation Loss: 0.014722\n",
      "Epoch: 628 \tTraining Loss: 0.015453 \tValidation Loss: 0.014720\n",
      "Epoch: 629 \tTraining Loss: 0.015439 \tValidation Loss: 0.014710\n",
      "Validation loss decreased (0.014713 --> 0.014710).  Saving model ...\n",
      "Epoch: 630 \tTraining Loss: 0.015440 \tValidation Loss: 0.014715\n",
      "Epoch: 631 \tTraining Loss: 0.015416 \tValidation Loss: 0.014725\n",
      "Epoch: 632 \tTraining Loss: 0.015429 \tValidation Loss: 0.014713\n",
      "Epoch: 633 \tTraining Loss: 0.015410 \tValidation Loss: 0.014719\n",
      "Epoch: 634 \tTraining Loss: 0.015431 \tValidation Loss: 0.014724\n",
      "Epoch: 635 \tTraining Loss: 0.015430 \tValidation Loss: 0.014713\n",
      "Epoch: 636 \tTraining Loss: 0.015431 \tValidation Loss: 0.014717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 637 \tTraining Loss: 0.015406 \tValidation Loss: 0.014715\n",
      "Epoch: 638 \tTraining Loss: 0.015424 \tValidation Loss: 0.014716\n",
      "Epoch: 639 \tTraining Loss: 0.015425 \tValidation Loss: 0.014720\n",
      "Epoch: 640 \tTraining Loss: 0.015409 \tValidation Loss: 0.014706\n",
      "Validation loss decreased (0.014710 --> 0.014706).  Saving model ...\n",
      "Epoch: 641 \tTraining Loss: 0.015404 \tValidation Loss: 0.014718\n",
      "Epoch: 642 \tTraining Loss: 0.015419 \tValidation Loss: 0.014713\n",
      "Epoch: 643 \tTraining Loss: 0.015402 \tValidation Loss: 0.014712\n",
      "Epoch: 644 \tTraining Loss: 0.015411 \tValidation Loss: 0.014716\n",
      "Epoch: 645 \tTraining Loss: 0.015422 \tValidation Loss: 0.014719\n",
      "Epoch: 646 \tTraining Loss: 0.015424 \tValidation Loss: 0.014708\n",
      "Epoch: 647 \tTraining Loss: 0.015389 \tValidation Loss: 0.014715\n",
      "Epoch: 648 \tTraining Loss: 0.015402 \tValidation Loss: 0.014718\n",
      "Epoch: 649 \tTraining Loss: 0.015411 \tValidation Loss: 0.014710\n",
      "Epoch: 650 \tTraining Loss: 0.015419 \tValidation Loss: 0.014714\n",
      "Epoch: 651 \tTraining Loss: 0.015412 \tValidation Loss: 0.014708\n",
      "Epoch: 652 \tTraining Loss: 0.015398 \tValidation Loss: 0.014717\n",
      "Epoch: 653 \tTraining Loss: 0.015406 \tValidation Loss: 0.014714\n",
      "Epoch: 654 \tTraining Loss: 0.015391 \tValidation Loss: 0.014719\n",
      "Epoch: 655 \tTraining Loss: 0.015405 \tValidation Loss: 0.014708\n",
      "Epoch: 656 \tTraining Loss: 0.015413 \tValidation Loss: 0.014714\n",
      "Epoch: 657 \tTraining Loss: 0.015367 \tValidation Loss: 0.014708\n",
      "Epoch: 658 \tTraining Loss: 0.015393 \tValidation Loss: 0.014713\n",
      "Epoch: 659 \tTraining Loss: 0.015383 \tValidation Loss: 0.014707\n",
      "Epoch: 660 \tTraining Loss: 0.015401 \tValidation Loss: 0.014715\n",
      "Epoch: 661 \tTraining Loss: 0.015398 \tValidation Loss: 0.014710\n",
      "Epoch: 662 \tTraining Loss: 0.015424 \tValidation Loss: 0.014707\n",
      "Epoch: 663 \tTraining Loss: 0.015401 \tValidation Loss: 0.014705\n",
      "Validation loss decreased (0.014706 --> 0.014705).  Saving model ...\n",
      "Epoch: 664 \tTraining Loss: 0.015423 \tValidation Loss: 0.014704\n",
      "Validation loss decreased (0.014705 --> 0.014704).  Saving model ...\n",
      "Epoch: 665 \tTraining Loss: 0.015423 \tValidation Loss: 0.014718\n",
      "Epoch: 666 \tTraining Loss: 0.015382 \tValidation Loss: 0.014711\n",
      "Epoch: 667 \tTraining Loss: 0.015385 \tValidation Loss: 0.014706\n",
      "Epoch: 668 \tTraining Loss: 0.015394 \tValidation Loss: 0.014711\n",
      "Epoch: 669 \tTraining Loss: 0.015387 \tValidation Loss: 0.014711\n",
      "Epoch: 670 \tTraining Loss: 0.015392 \tValidation Loss: 0.014710\n",
      "Epoch: 671 \tTraining Loss: 0.015373 \tValidation Loss: 0.014705\n",
      "Epoch: 672 \tTraining Loss: 0.015373 \tValidation Loss: 0.014712\n",
      "Epoch: 673 \tTraining Loss: 0.015363 \tValidation Loss: 0.014708\n",
      "Epoch: 674 \tTraining Loss: 0.015384 \tValidation Loss: 0.014708\n",
      "Epoch: 675 \tTraining Loss: 0.015369 \tValidation Loss: 0.014711\n",
      "Epoch: 676 \tTraining Loss: 0.015363 \tValidation Loss: 0.014704\n",
      "Validation loss decreased (0.014704 --> 0.014704).  Saving model ...\n",
      "Epoch: 677 \tTraining Loss: 0.015365 \tValidation Loss: 0.014713\n",
      "Epoch: 678 \tTraining Loss: 0.015354 \tValidation Loss: 0.014713\n",
      "Epoch: 679 \tTraining Loss: 0.015361 \tValidation Loss: 0.014705\n",
      "Epoch: 680 \tTraining Loss: 0.015355 \tValidation Loss: 0.014714\n",
      "Epoch: 681 \tTraining Loss: 0.015368 \tValidation Loss: 0.014703\n",
      "Validation loss decreased (0.014704 --> 0.014703).  Saving model ...\n",
      "Epoch: 682 \tTraining Loss: 0.015372 \tValidation Loss: 0.014704\n",
      "Epoch: 683 \tTraining Loss: 0.015370 \tValidation Loss: 0.014713\n",
      "Epoch: 684 \tTraining Loss: 0.015373 \tValidation Loss: 0.014707\n",
      "Epoch: 685 \tTraining Loss: 0.015350 \tValidation Loss: 0.014703\n",
      "Epoch: 686 \tTraining Loss: 0.015376 \tValidation Loss: 0.014710\n",
      "Epoch: 687 \tTraining Loss: 0.015350 \tValidation Loss: 0.014706\n",
      "Epoch: 688 \tTraining Loss: 0.015362 \tValidation Loss: 0.014710\n",
      "Epoch: 689 \tTraining Loss: 0.015351 \tValidation Loss: 0.014704\n",
      "Epoch: 690 \tTraining Loss: 0.015380 \tValidation Loss: 0.014698\n",
      "Validation loss decreased (0.014703 --> 0.014698).  Saving model ...\n",
      "Epoch: 691 \tTraining Loss: 0.015365 \tValidation Loss: 0.014697\n",
      "Validation loss decreased (0.014698 --> 0.014697).  Saving model ...\n",
      "Epoch: 692 \tTraining Loss: 0.015345 \tValidation Loss: 0.014712\n",
      "Epoch: 693 \tTraining Loss: 0.015368 \tValidation Loss: 0.014701\n",
      "Epoch: 694 \tTraining Loss: 0.015350 \tValidation Loss: 0.014703\n",
      "Epoch: 695 \tTraining Loss: 0.015337 \tValidation Loss: 0.014698\n",
      "Epoch: 696 \tTraining Loss: 0.015344 \tValidation Loss: 0.014706\n",
      "Epoch: 697 \tTraining Loss: 0.015329 \tValidation Loss: 0.014700\n",
      "Epoch: 698 \tTraining Loss: 0.015343 \tValidation Loss: 0.014704\n",
      "Epoch: 699 \tTraining Loss: 0.015327 \tValidation Loss: 0.014699\n",
      "Epoch: 700 \tTraining Loss: 0.015336 \tValidation Loss: 0.014700\n",
      "Epoch: 701 \tTraining Loss: 0.015354 \tValidation Loss: 0.014701\n",
      "Epoch: 702 \tTraining Loss: 0.015334 \tValidation Loss: 0.014705\n",
      "Epoch: 703 \tTraining Loss: 0.015334 \tValidation Loss: 0.014701\n",
      "Epoch: 704 \tTraining Loss: 0.015340 \tValidation Loss: 0.014703\n",
      "Epoch: 705 \tTraining Loss: 0.015342 \tValidation Loss: 0.014701\n",
      "Epoch: 706 \tTraining Loss: 0.015329 \tValidation Loss: 0.014699\n",
      "Epoch: 707 \tTraining Loss: 0.015345 \tValidation Loss: 0.014701\n",
      "Epoch: 708 \tTraining Loss: 0.015332 \tValidation Loss: 0.014702\n",
      "Epoch: 709 \tTraining Loss: 0.015327 \tValidation Loss: 0.014698\n",
      "Epoch: 710 \tTraining Loss: 0.015322 \tValidation Loss: 0.014695\n",
      "Validation loss decreased (0.014697 --> 0.014695).  Saving model ...\n",
      "Epoch: 711 \tTraining Loss: 0.015336 \tValidation Loss: 0.014703\n",
      "Epoch: 712 \tTraining Loss: 0.015343 \tValidation Loss: 0.014705\n",
      "Epoch: 713 \tTraining Loss: 0.015344 \tValidation Loss: 0.014704\n",
      "Epoch: 714 \tTraining Loss: 0.015334 \tValidation Loss: 0.014698\n",
      "Epoch: 715 \tTraining Loss: 0.015328 \tValidation Loss: 0.014696\n",
      "Epoch: 716 \tTraining Loss: 0.015311 \tValidation Loss: 0.014702\n",
      "Epoch: 717 \tTraining Loss: 0.015337 \tValidation Loss: 0.014691\n",
      "Validation loss decreased (0.014695 --> 0.014691).  Saving model ...\n",
      "Epoch: 718 \tTraining Loss: 0.015304 \tValidation Loss: 0.014701\n",
      "Epoch: 719 \tTraining Loss: 0.015300 \tValidation Loss: 0.014705\n",
      "Epoch: 720 \tTraining Loss: 0.015329 \tValidation Loss: 0.014697\n",
      "Epoch: 721 \tTraining Loss: 0.015318 \tValidation Loss: 0.014703\n",
      "Epoch: 722 \tTraining Loss: 0.015340 \tValidation Loss: 0.014699\n",
      "Epoch: 723 \tTraining Loss: 0.015342 \tValidation Loss: 0.014698\n",
      "Epoch: 724 \tTraining Loss: 0.015332 \tValidation Loss: 0.014689\n",
      "Validation loss decreased (0.014691 --> 0.014689).  Saving model ...\n",
      "Epoch: 725 \tTraining Loss: 0.015316 \tValidation Loss: 0.014697\n",
      "Epoch: 726 \tTraining Loss: 0.015318 \tValidation Loss: 0.014701\n",
      "Epoch: 727 \tTraining Loss: 0.015338 \tValidation Loss: 0.014702\n",
      "Epoch: 728 \tTraining Loss: 0.015342 \tValidation Loss: 0.014701\n",
      "Epoch: 729 \tTraining Loss: 0.015329 \tValidation Loss: 0.014700\n",
      "Epoch: 730 \tTraining Loss: 0.015319 \tValidation Loss: 0.014697\n",
      "Epoch: 731 \tTraining Loss: 0.015317 \tValidation Loss: 0.014699\n",
      "Epoch: 732 \tTraining Loss: 0.015334 \tValidation Loss: 0.014695\n",
      "Epoch: 733 \tTraining Loss: 0.015307 \tValidation Loss: 0.014693\n",
      "Epoch: 734 \tTraining Loss: 0.015312 \tValidation Loss: 0.014693\n",
      "Epoch: 735 \tTraining Loss: 0.015278 \tValidation Loss: 0.014702\n",
      "Epoch: 736 \tTraining Loss: 0.015302 \tValidation Loss: 0.014693\n",
      "Epoch: 737 \tTraining Loss: 0.015300 \tValidation Loss: 0.014701\n",
      "Epoch: 738 \tTraining Loss: 0.015309 \tValidation Loss: 0.014699\n",
      "Epoch: 739 \tTraining Loss: 0.015294 \tValidation Loss: 0.014697\n",
      "Epoch: 740 \tTraining Loss: 0.015317 \tValidation Loss: 0.014693\n",
      "Epoch: 741 \tTraining Loss: 0.015316 \tValidation Loss: 0.014696\n",
      "Epoch: 742 \tTraining Loss: 0.015305 \tValidation Loss: 0.014688\n",
      "Validation loss decreased (0.014689 --> 0.014688).  Saving model ...\n",
      "Epoch: 743 \tTraining Loss: 0.015290 \tValidation Loss: 0.014695\n",
      "Epoch: 744 \tTraining Loss: 0.015332 \tValidation Loss: 0.014691\n",
      "Epoch: 745 \tTraining Loss: 0.015288 \tValidation Loss: 0.014693\n",
      "Epoch: 746 \tTraining Loss: 0.015287 \tValidation Loss: 0.014703\n",
      "Epoch: 747 \tTraining Loss: 0.015292 \tValidation Loss: 0.014698\n",
      "Epoch: 748 \tTraining Loss: 0.015293 \tValidation Loss: 0.014686\n",
      "Validation loss decreased (0.014688 --> 0.014686).  Saving model ...\n",
      "Epoch: 749 \tTraining Loss: 0.015290 \tValidation Loss: 0.014698\n",
      "Epoch: 750 \tTraining Loss: 0.015306 \tValidation Loss: 0.014694\n",
      "Epoch: 751 \tTraining Loss: 0.015294 \tValidation Loss: 0.014699\n",
      "Epoch: 752 \tTraining Loss: 0.015312 \tValidation Loss: 0.014694\n",
      "Epoch: 753 \tTraining Loss: 0.015280 \tValidation Loss: 0.014689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 754 \tTraining Loss: 0.015293 \tValidation Loss: 0.014695\n",
      "Epoch: 755 \tTraining Loss: 0.015280 \tValidation Loss: 0.014695\n",
      "Epoch: 756 \tTraining Loss: 0.015308 \tValidation Loss: 0.014691\n",
      "Epoch: 757 \tTraining Loss: 0.015267 \tValidation Loss: 0.014691\n",
      "Epoch: 758 \tTraining Loss: 0.015293 \tValidation Loss: 0.014696\n",
      "Epoch: 759 \tTraining Loss: 0.015275 \tValidation Loss: 0.014694\n",
      "Epoch: 760 \tTraining Loss: 0.015285 \tValidation Loss: 0.014688\n",
      "Epoch: 761 \tTraining Loss: 0.015286 \tValidation Loss: 0.014695\n",
      "Epoch: 762 \tTraining Loss: 0.015285 \tValidation Loss: 0.014692\n",
      "Epoch: 763 \tTraining Loss: 0.015274 \tValidation Loss: 0.014688\n",
      "Epoch: 764 \tTraining Loss: 0.015287 \tValidation Loss: 0.014693\n",
      "Epoch: 765 \tTraining Loss: 0.015271 \tValidation Loss: 0.014697\n",
      "Epoch: 766 \tTraining Loss: 0.015281 \tValidation Loss: 0.014694\n",
      "Epoch: 767 \tTraining Loss: 0.015267 \tValidation Loss: 0.014694\n",
      "Epoch: 768 \tTraining Loss: 0.015290 \tValidation Loss: 0.014695\n",
      "Epoch: 769 \tTraining Loss: 0.015287 \tValidation Loss: 0.014692\n",
      "Epoch: 770 \tTraining Loss: 0.015256 \tValidation Loss: 0.014685\n",
      "Validation loss decreased (0.014686 --> 0.014685).  Saving model ...\n",
      "Epoch: 771 \tTraining Loss: 0.015262 \tValidation Loss: 0.014690\n",
      "Epoch: 772 \tTraining Loss: 0.015285 \tValidation Loss: 0.014689\n",
      "Epoch: 773 \tTraining Loss: 0.015268 \tValidation Loss: 0.014696\n",
      "Epoch: 774 \tTraining Loss: 0.015280 \tValidation Loss: 0.014689\n",
      "Epoch: 775 \tTraining Loss: 0.015271 \tValidation Loss: 0.014691\n",
      "Epoch: 776 \tTraining Loss: 0.015266 \tValidation Loss: 0.014687\n",
      "Epoch: 777 \tTraining Loss: 0.015261 \tValidation Loss: 0.014690\n",
      "Epoch: 778 \tTraining Loss: 0.015261 \tValidation Loss: 0.014691\n",
      "Epoch: 779 \tTraining Loss: 0.015274 \tValidation Loss: 0.014686\n",
      "Epoch: 780 \tTraining Loss: 0.015254 \tValidation Loss: 0.014680\n",
      "Validation loss decreased (0.014685 --> 0.014680).  Saving model ...\n",
      "Epoch: 781 \tTraining Loss: 0.015265 \tValidation Loss: 0.014690\n",
      "Epoch: 782 \tTraining Loss: 0.015267 \tValidation Loss: 0.014693\n",
      "Epoch: 783 \tTraining Loss: 0.015255 \tValidation Loss: 0.014691\n",
      "Epoch: 784 \tTraining Loss: 0.015280 \tValidation Loss: 0.014682\n",
      "Epoch: 785 \tTraining Loss: 0.015246 \tValidation Loss: 0.014684\n",
      "Epoch: 786 \tTraining Loss: 0.015251 \tValidation Loss: 0.014685\n",
      "Epoch: 787 \tTraining Loss: 0.015273 \tValidation Loss: 0.014690\n",
      "Epoch: 788 \tTraining Loss: 0.015251 \tValidation Loss: 0.014691\n",
      "Epoch: 789 \tTraining Loss: 0.015267 \tValidation Loss: 0.014690\n",
      "Epoch: 790 \tTraining Loss: 0.015258 \tValidation Loss: 0.014694\n",
      "Epoch: 791 \tTraining Loss: 0.015241 \tValidation Loss: 0.014682\n",
      "Epoch: 792 \tTraining Loss: 0.015255 \tValidation Loss: 0.014685\n",
      "Epoch: 793 \tTraining Loss: 0.015225 \tValidation Loss: 0.014682\n",
      "Epoch: 794 \tTraining Loss: 0.015257 \tValidation Loss: 0.014687\n",
      "Epoch: 795 \tTraining Loss: 0.015257 \tValidation Loss: 0.014687\n",
      "Epoch: 796 \tTraining Loss: 0.015238 \tValidation Loss: 0.014682\n",
      "Epoch: 797 \tTraining Loss: 0.015252 \tValidation Loss: 0.014689\n",
      "Epoch: 798 \tTraining Loss: 0.015238 \tValidation Loss: 0.014693\n",
      "Epoch: 799 \tTraining Loss: 0.015250 \tValidation Loss: 0.014685\n",
      "Epoch: 800 \tTraining Loss: 0.015249 \tValidation Loss: 0.014689\n",
      "Epoch: 801 \tTraining Loss: 0.015235 \tValidation Loss: 0.014688\n",
      "Epoch: 802 \tTraining Loss: 0.015240 \tValidation Loss: 0.014687\n",
      "Epoch: 803 \tTraining Loss: 0.015226 \tValidation Loss: 0.014686\n",
      "Epoch: 804 \tTraining Loss: 0.015229 \tValidation Loss: 0.014682\n",
      "Epoch: 805 \tTraining Loss: 0.015239 \tValidation Loss: 0.014684\n",
      "Epoch: 806 \tTraining Loss: 0.015240 \tValidation Loss: 0.014678\n",
      "Validation loss decreased (0.014680 --> 0.014678).  Saving model ...\n",
      "Epoch: 807 \tTraining Loss: 0.015226 \tValidation Loss: 0.014688\n",
      "Epoch: 808 \tTraining Loss: 0.015234 \tValidation Loss: 0.014684\n",
      "Epoch: 809 \tTraining Loss: 0.015246 \tValidation Loss: 0.014686\n",
      "Epoch: 810 \tTraining Loss: 0.015241 \tValidation Loss: 0.014683\n",
      "Epoch: 811 \tTraining Loss: 0.015228 \tValidation Loss: 0.014683\n",
      "Epoch: 812 \tTraining Loss: 0.015223 \tValidation Loss: 0.014683\n",
      "Epoch: 813 \tTraining Loss: 0.015227 \tValidation Loss: 0.014678\n",
      "Validation loss decreased (0.014678 --> 0.014678).  Saving model ...\n",
      "Epoch: 814 \tTraining Loss: 0.015239 \tValidation Loss: 0.014683\n",
      "Epoch: 815 \tTraining Loss: 0.015205 \tValidation Loss: 0.014679\n",
      "Epoch: 816 \tTraining Loss: 0.015212 \tValidation Loss: 0.014680\n",
      "Epoch: 817 \tTraining Loss: 0.015238 \tValidation Loss: 0.014683\n",
      "Epoch: 818 \tTraining Loss: 0.015218 \tValidation Loss: 0.014685\n",
      "Epoch: 819 \tTraining Loss: 0.015234 \tValidation Loss: 0.014678\n",
      "Epoch: 820 \tTraining Loss: 0.015222 \tValidation Loss: 0.014680\n",
      "Epoch: 821 \tTraining Loss: 0.015229 \tValidation Loss: 0.014677\n",
      "Validation loss decreased (0.014678 --> 0.014677).  Saving model ...\n",
      "Epoch: 822 \tTraining Loss: 0.015253 \tValidation Loss: 0.014685\n",
      "Epoch: 823 \tTraining Loss: 0.015233 \tValidation Loss: 0.014685\n",
      "Epoch: 824 \tTraining Loss: 0.015217 \tValidation Loss: 0.014677\n",
      "Epoch: 825 \tTraining Loss: 0.015220 \tValidation Loss: 0.014678\n",
      "Epoch: 826 \tTraining Loss: 0.015226 \tValidation Loss: 0.014687\n",
      "Epoch: 827 \tTraining Loss: 0.015209 \tValidation Loss: 0.014678\n",
      "Epoch: 828 \tTraining Loss: 0.015215 \tValidation Loss: 0.014679\n",
      "Epoch: 829 \tTraining Loss: 0.015224 \tValidation Loss: 0.014677\n",
      "Epoch: 830 \tTraining Loss: 0.015215 \tValidation Loss: 0.014680\n",
      "Epoch: 831 \tTraining Loss: 0.015213 \tValidation Loss: 0.014684\n",
      "Epoch: 832 \tTraining Loss: 0.015229 \tValidation Loss: 0.014675\n",
      "Validation loss decreased (0.014677 --> 0.014675).  Saving model ...\n",
      "Epoch: 833 \tTraining Loss: 0.015221 \tValidation Loss: 0.014679\n",
      "Epoch: 834 \tTraining Loss: 0.015224 \tValidation Loss: 0.014677\n",
      "Epoch: 835 \tTraining Loss: 0.015223 \tValidation Loss: 0.014677\n",
      "Epoch: 836 \tTraining Loss: 0.015195 \tValidation Loss: 0.014680\n",
      "Epoch: 837 \tTraining Loss: 0.015215 \tValidation Loss: 0.014683\n",
      "Epoch: 838 \tTraining Loss: 0.015203 \tValidation Loss: 0.014675\n",
      "Validation loss decreased (0.014675 --> 0.014675).  Saving model ...\n",
      "Epoch: 839 \tTraining Loss: 0.015208 \tValidation Loss: 0.014680\n",
      "Epoch: 840 \tTraining Loss: 0.015226 \tValidation Loss: 0.014674\n",
      "Validation loss decreased (0.014675 --> 0.014674).  Saving model ...\n",
      "Epoch: 841 \tTraining Loss: 0.015208 \tValidation Loss: 0.014674\n",
      "Epoch: 842 \tTraining Loss: 0.015209 \tValidation Loss: 0.014673\n",
      "Validation loss decreased (0.014674 --> 0.014673).  Saving model ...\n",
      "Epoch: 843 \tTraining Loss: 0.015219 \tValidation Loss: 0.014670\n",
      "Validation loss decreased (0.014673 --> 0.014670).  Saving model ...\n",
      "Epoch: 844 \tTraining Loss: 0.015230 \tValidation Loss: 0.014677\n",
      "Epoch: 845 \tTraining Loss: 0.015204 \tValidation Loss: 0.014673\n",
      "Epoch: 846 \tTraining Loss: 0.015234 \tValidation Loss: 0.014675\n",
      "Epoch: 847 \tTraining Loss: 0.015194 \tValidation Loss: 0.014673\n",
      "Epoch: 848 \tTraining Loss: 0.015213 \tValidation Loss: 0.014678\n",
      "Epoch: 849 \tTraining Loss: 0.015213 \tValidation Loss: 0.014673\n",
      "Epoch: 850 \tTraining Loss: 0.015199 \tValidation Loss: 0.014675\n",
      "Epoch: 851 \tTraining Loss: 0.015208 \tValidation Loss: 0.014677\n",
      "Epoch: 852 \tTraining Loss: 0.015185 \tValidation Loss: 0.014685\n",
      "Epoch: 853 \tTraining Loss: 0.015198 \tValidation Loss: 0.014667\n",
      "Validation loss decreased (0.014670 --> 0.014667).  Saving model ...\n",
      "Epoch: 854 \tTraining Loss: 0.015198 \tValidation Loss: 0.014674\n",
      "Epoch: 855 \tTraining Loss: 0.015198 \tValidation Loss: 0.014669\n",
      "Epoch: 856 \tTraining Loss: 0.015192 \tValidation Loss: 0.014676\n",
      "Epoch: 857 \tTraining Loss: 0.015185 \tValidation Loss: 0.014672\n",
      "Epoch: 858 \tTraining Loss: 0.015195 \tValidation Loss: 0.014676\n",
      "Epoch: 859 \tTraining Loss: 0.015180 \tValidation Loss: 0.014678\n",
      "Epoch: 860 \tTraining Loss: 0.015182 \tValidation Loss: 0.014673\n",
      "Epoch: 861 \tTraining Loss: 0.015203 \tValidation Loss: 0.014675\n",
      "Epoch: 862 \tTraining Loss: 0.015210 \tValidation Loss: 0.014670\n",
      "Epoch: 863 \tTraining Loss: 0.015183 \tValidation Loss: 0.014673\n",
      "Epoch: 864 \tTraining Loss: 0.015175 \tValidation Loss: 0.014674\n",
      "Epoch: 865 \tTraining Loss: 0.015187 \tValidation Loss: 0.014673\n",
      "Epoch: 866 \tTraining Loss: 0.015178 \tValidation Loss: 0.014674\n",
      "Epoch: 867 \tTraining Loss: 0.015195 \tValidation Loss: 0.014670\n",
      "Epoch: 868 \tTraining Loss: 0.015190 \tValidation Loss: 0.014672\n",
      "Epoch: 869 \tTraining Loss: 0.015187 \tValidation Loss: 0.014677\n",
      "Epoch: 870 \tTraining Loss: 0.015197 \tValidation Loss: 0.014669\n",
      "Epoch: 871 \tTraining Loss: 0.015178 \tValidation Loss: 0.014677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 872 \tTraining Loss: 0.015189 \tValidation Loss: 0.014674\n",
      "Epoch: 873 \tTraining Loss: 0.015183 \tValidation Loss: 0.014680\n",
      "Epoch: 874 \tTraining Loss: 0.015182 \tValidation Loss: 0.014671\n",
      "Epoch: 875 \tTraining Loss: 0.015176 \tValidation Loss: 0.014674\n",
      "Epoch: 876 \tTraining Loss: 0.015190 \tValidation Loss: 0.014663\n",
      "Validation loss decreased (0.014667 --> 0.014663).  Saving model ...\n",
      "Epoch: 877 \tTraining Loss: 0.015163 \tValidation Loss: 0.014665\n",
      "Epoch: 878 \tTraining Loss: 0.015163 \tValidation Loss: 0.014674\n",
      "Epoch: 879 \tTraining Loss: 0.015190 \tValidation Loss: 0.014673\n",
      "Epoch: 880 \tTraining Loss: 0.015176 \tValidation Loss: 0.014673\n",
      "Epoch: 881 \tTraining Loss: 0.015158 \tValidation Loss: 0.014670\n",
      "Epoch: 882 \tTraining Loss: 0.015159 \tValidation Loss: 0.014674\n",
      "Epoch: 883 \tTraining Loss: 0.015168 \tValidation Loss: 0.014671\n",
      "Epoch: 884 \tTraining Loss: 0.015160 \tValidation Loss: 0.014672\n",
      "Epoch: 885 \tTraining Loss: 0.015181 \tValidation Loss: 0.014678\n",
      "Epoch: 886 \tTraining Loss: 0.015183 \tValidation Loss: 0.014669\n",
      "Epoch: 887 \tTraining Loss: 0.015162 \tValidation Loss: 0.014671\n",
      "Epoch: 888 \tTraining Loss: 0.015153 \tValidation Loss: 0.014663\n",
      "Epoch: 889 \tTraining Loss: 0.015166 \tValidation Loss: 0.014667\n",
      "Epoch: 890 \tTraining Loss: 0.015181 \tValidation Loss: 0.014669\n",
      "Epoch: 891 \tTraining Loss: 0.015162 \tValidation Loss: 0.014664\n",
      "Epoch: 892 \tTraining Loss: 0.015171 \tValidation Loss: 0.014669\n",
      "Epoch: 893 \tTraining Loss: 0.015177 \tValidation Loss: 0.014674\n",
      "Epoch: 894 \tTraining Loss: 0.015169 \tValidation Loss: 0.014670\n",
      "Epoch: 895 \tTraining Loss: 0.015159 \tValidation Loss: 0.014676\n",
      "Epoch: 896 \tTraining Loss: 0.015182 \tValidation Loss: 0.014673\n",
      "Epoch: 897 \tTraining Loss: 0.015167 \tValidation Loss: 0.014670\n",
      "Epoch: 898 \tTraining Loss: 0.015159 \tValidation Loss: 0.014668\n",
      "Epoch: 899 \tTraining Loss: 0.015176 \tValidation Loss: 0.014673\n",
      "Epoch: 900 \tTraining Loss: 0.015168 \tValidation Loss: 0.014669\n",
      "Epoch: 901 \tTraining Loss: 0.015168 \tValidation Loss: 0.014667\n",
      "Epoch: 902 \tTraining Loss: 0.015151 \tValidation Loss: 0.014663\n",
      "Epoch: 903 \tTraining Loss: 0.015156 \tValidation Loss: 0.014664\n",
      "Epoch: 904 \tTraining Loss: 0.015169 \tValidation Loss: 0.014670\n",
      "Epoch: 905 \tTraining Loss: 0.015138 \tValidation Loss: 0.014662\n",
      "Validation loss decreased (0.014663 --> 0.014662).  Saving model ...\n",
      "Epoch: 906 \tTraining Loss: 0.015144 \tValidation Loss: 0.014676\n",
      "Epoch: 907 \tTraining Loss: 0.015171 \tValidation Loss: 0.014673\n",
      "Epoch: 908 \tTraining Loss: 0.015151 \tValidation Loss: 0.014665\n",
      "Epoch: 909 \tTraining Loss: 0.015145 \tValidation Loss: 0.014664\n",
      "Epoch: 910 \tTraining Loss: 0.015146 \tValidation Loss: 0.014672\n",
      "Epoch: 911 \tTraining Loss: 0.015165 \tValidation Loss: 0.014665\n",
      "Epoch: 912 \tTraining Loss: 0.015154 \tValidation Loss: 0.014667\n",
      "Epoch: 913 \tTraining Loss: 0.015151 \tValidation Loss: 0.014665\n",
      "Epoch: 914 \tTraining Loss: 0.015141 \tValidation Loss: 0.014667\n",
      "Epoch: 915 \tTraining Loss: 0.015140 \tValidation Loss: 0.014663\n",
      "Epoch: 916 \tTraining Loss: 0.015169 \tValidation Loss: 0.014664\n",
      "Epoch: 917 \tTraining Loss: 0.015139 \tValidation Loss: 0.014663\n",
      "Epoch: 918 \tTraining Loss: 0.015150 \tValidation Loss: 0.014666\n",
      "Epoch: 919 \tTraining Loss: 0.015142 \tValidation Loss: 0.014671\n",
      "Epoch: 920 \tTraining Loss: 0.015131 \tValidation Loss: 0.014667\n",
      "Epoch: 921 \tTraining Loss: 0.015148 \tValidation Loss: 0.014662\n",
      "Validation loss decreased (0.014662 --> 0.014662).  Saving model ...\n",
      "Epoch: 922 \tTraining Loss: 0.015142 \tValidation Loss: 0.014669\n",
      "Epoch: 923 \tTraining Loss: 0.015141 \tValidation Loss: 0.014667\n",
      "Epoch: 924 \tTraining Loss: 0.015141 \tValidation Loss: 0.014659\n",
      "Validation loss decreased (0.014662 --> 0.014659).  Saving model ...\n",
      "Epoch: 925 \tTraining Loss: 0.015142 \tValidation Loss: 0.014669\n",
      "Epoch: 926 \tTraining Loss: 0.015156 \tValidation Loss: 0.014665\n",
      "Epoch: 927 \tTraining Loss: 0.015152 \tValidation Loss: 0.014666\n",
      "Epoch: 928 \tTraining Loss: 0.015140 \tValidation Loss: 0.014669\n",
      "Epoch: 929 \tTraining Loss: 0.015121 \tValidation Loss: 0.014668\n",
      "Epoch: 930 \tTraining Loss: 0.015134 \tValidation Loss: 0.014664\n",
      "Epoch: 931 \tTraining Loss: 0.015135 \tValidation Loss: 0.014661\n",
      "Epoch: 932 \tTraining Loss: 0.015141 \tValidation Loss: 0.014665\n",
      "Epoch: 933 \tTraining Loss: 0.015135 \tValidation Loss: 0.014664\n",
      "Epoch: 934 \tTraining Loss: 0.015134 \tValidation Loss: 0.014671\n",
      "Epoch: 935 \tTraining Loss: 0.015138 \tValidation Loss: 0.014662\n",
      "Epoch: 936 \tTraining Loss: 0.015131 \tValidation Loss: 0.014660\n",
      "Epoch: 937 \tTraining Loss: 0.015125 \tValidation Loss: 0.014665\n",
      "Epoch: 938 \tTraining Loss: 0.015125 \tValidation Loss: 0.014657\n",
      "Validation loss decreased (0.014659 --> 0.014657).  Saving model ...\n",
      "Epoch: 939 \tTraining Loss: 0.015125 \tValidation Loss: 0.014661\n",
      "Epoch: 940 \tTraining Loss: 0.015148 \tValidation Loss: 0.014666\n",
      "Epoch: 941 \tTraining Loss: 0.015114 \tValidation Loss: 0.014660\n",
      "Epoch: 942 \tTraining Loss: 0.015105 \tValidation Loss: 0.014664\n",
      "Epoch: 943 \tTraining Loss: 0.015132 \tValidation Loss: 0.014666\n",
      "Epoch: 944 \tTraining Loss: 0.015122 \tValidation Loss: 0.014663\n",
      "Epoch: 945 \tTraining Loss: 0.015107 \tValidation Loss: 0.014654\n",
      "Validation loss decreased (0.014657 --> 0.014654).  Saving model ...\n",
      "Epoch: 946 \tTraining Loss: 0.015134 \tValidation Loss: 0.014658\n",
      "Epoch: 947 \tTraining Loss: 0.015130 \tValidation Loss: 0.014661\n",
      "Epoch: 948 \tTraining Loss: 0.015130 \tValidation Loss: 0.014658\n",
      "Epoch: 949 \tTraining Loss: 0.015130 \tValidation Loss: 0.014658\n",
      "Epoch: 950 \tTraining Loss: 0.015089 \tValidation Loss: 0.014660\n",
      "Epoch: 951 \tTraining Loss: 0.015131 \tValidation Loss: 0.014654\n",
      "Validation loss decreased (0.014654 --> 0.014654).  Saving model ...\n",
      "Epoch: 952 \tTraining Loss: 0.015115 \tValidation Loss: 0.014661\n",
      "Epoch: 953 \tTraining Loss: 0.015116 \tValidation Loss: 0.014657\n",
      "Epoch: 954 \tTraining Loss: 0.015144 \tValidation Loss: 0.014658\n",
      "Epoch: 955 \tTraining Loss: 0.015112 \tValidation Loss: 0.014655\n",
      "Epoch: 956 \tTraining Loss: 0.015132 \tValidation Loss: 0.014657\n",
      "Epoch: 957 \tTraining Loss: 0.015116 \tValidation Loss: 0.014655\n",
      "Epoch: 958 \tTraining Loss: 0.015129 \tValidation Loss: 0.014654\n",
      "Validation loss decreased (0.014654 --> 0.014654).  Saving model ...\n",
      "Epoch: 959 \tTraining Loss: 0.015109 \tValidation Loss: 0.014659\n",
      "Epoch: 960 \tTraining Loss: 0.015128 \tValidation Loss: 0.014659\n",
      "Epoch: 961 \tTraining Loss: 0.015105 \tValidation Loss: 0.014658\n",
      "Epoch: 962 \tTraining Loss: 0.015110 \tValidation Loss: 0.014664\n",
      "Epoch: 963 \tTraining Loss: 0.015099 \tValidation Loss: 0.014656\n",
      "Epoch: 964 \tTraining Loss: 0.015125 \tValidation Loss: 0.014656\n",
      "Epoch: 965 \tTraining Loss: 0.015122 \tValidation Loss: 0.014661\n",
      "Epoch: 966 \tTraining Loss: 0.015111 \tValidation Loss: 0.014661\n",
      "Epoch: 967 \tTraining Loss: 0.015108 \tValidation Loss: 0.014655\n",
      "Epoch: 968 \tTraining Loss: 0.015103 \tValidation Loss: 0.014659\n",
      "Epoch: 969 \tTraining Loss: 0.015105 \tValidation Loss: 0.014654\n",
      "Validation loss decreased (0.014654 --> 0.014654).  Saving model ...\n",
      "Epoch: 970 \tTraining Loss: 0.015116 \tValidation Loss: 0.014656\n",
      "Epoch: 971 \tTraining Loss: 0.015111 \tValidation Loss: 0.014661\n",
      "Epoch: 972 \tTraining Loss: 0.015099 \tValidation Loss: 0.014654\n",
      "Epoch: 973 \tTraining Loss: 0.015089 \tValidation Loss: 0.014655\n",
      "Epoch: 974 \tTraining Loss: 0.015101 \tValidation Loss: 0.014657\n",
      "Epoch: 975 \tTraining Loss: 0.015105 \tValidation Loss: 0.014657\n",
      "Epoch: 976 \tTraining Loss: 0.015100 \tValidation Loss: 0.014652\n",
      "Validation loss decreased (0.014654 --> 0.014652).  Saving model ...\n",
      "Epoch: 977 \tTraining Loss: 0.015097 \tValidation Loss: 0.014654\n",
      "Epoch: 978 \tTraining Loss: 0.015101 \tValidation Loss: 0.014660\n",
      "Epoch: 979 \tTraining Loss: 0.015111 \tValidation Loss: 0.014659\n",
      "Epoch: 980 \tTraining Loss: 0.015103 \tValidation Loss: 0.014654\n",
      "Epoch: 981 \tTraining Loss: 0.015096 \tValidation Loss: 0.014659\n",
      "Epoch: 982 \tTraining Loss: 0.015103 \tValidation Loss: 0.014662\n",
      "Epoch: 983 \tTraining Loss: 0.015121 \tValidation Loss: 0.014655\n",
      "Epoch: 984 \tTraining Loss: 0.015075 \tValidation Loss: 0.014653\n",
      "Epoch: 985 \tTraining Loss: 0.015116 \tValidation Loss: 0.014655\n",
      "Epoch: 986 \tTraining Loss: 0.015102 \tValidation Loss: 0.014654\n",
      "Epoch: 987 \tTraining Loss: 0.015075 \tValidation Loss: 0.014654\n",
      "Epoch: 988 \tTraining Loss: 0.015090 \tValidation Loss: 0.014651\n",
      "Validation loss decreased (0.014652 --> 0.014651).  Saving model ...\n",
      "Epoch: 989 \tTraining Loss: 0.015094 \tValidation Loss: 0.014657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 990 \tTraining Loss: 0.015094 \tValidation Loss: 0.014650\n",
      "Validation loss decreased (0.014651 --> 0.014650).  Saving model ...\n",
      "Epoch: 991 \tTraining Loss: 0.015087 \tValidation Loss: 0.014650\n",
      "Epoch: 992 \tTraining Loss: 0.015082 \tValidation Loss: 0.014660\n",
      "Epoch: 993 \tTraining Loss: 0.015092 \tValidation Loss: 0.014657\n",
      "Epoch: 994 \tTraining Loss: 0.015096 \tValidation Loss: 0.014654\n",
      "Epoch: 995 \tTraining Loss: 0.015090 \tValidation Loss: 0.014649\n",
      "Validation loss decreased (0.014650 --> 0.014649).  Saving model ...\n",
      "Epoch: 996 \tTraining Loss: 0.015094 \tValidation Loss: 0.014652\n",
      "Epoch: 997 \tTraining Loss: 0.015077 \tValidation Loss: 0.014653\n",
      "Epoch: 998 \tTraining Loss: 0.015079 \tValidation Loss: 0.014650\n",
      "Epoch: 999 \tTraining Loss: 0.015092 \tValidation Loss: 0.014656\n",
      "Epoch: 1000 \tTraining Loss: 0.015077 \tValidation Loss: 0.014650\n"
     ]
    }
   ],
   "source": [
    "# create a complete CNN\n",
    "# trained pn v1\n",
    "\n",
    "print(model)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 1000\n",
    "# model = model.float()\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            data, target = data.float(), target.float()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    \n",
    "    # At completion of epoch\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'NN_QST_1_KICS_21.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb2fcdd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'V2')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log10(train_losses[10:2000]), label='Training loss')\n",
    "plt.plot(np.log10(valid_losses[10:2000]), label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.title('V2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03707bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('NN_QST_2_KICS_20.pt'))\n",
    "# model.load_state_dict(torch.load('NN_QST_2_KICS_21.pt'))\n",
    "model.load_state_dict(torch.load('NN_QST_1_KICS_21.pt'))\n",
    "# model.load_state_dict(torch.load('NN_QST_2_KICS_23.pt'))\n",
    "# model.load_state_dict(torch.load('NN_QST_2_KICS_24.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b284495f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.014336\n",
      "Loss per Batch: 0.014388\n",
      "Loss per Batch: 0.014670\n",
      "Loss per Batch: 0.015192\n",
      "Loss per Batch: 0.014934\n",
      "Loss per Batch: 0.015408\n",
      "Loss per Batch: 0.014638\n",
      "Loss per Batch: 0.015271\n",
      "Loss per Batch: 0.014865\n",
      "Loss per Batch: 0.014048\n",
      "Loss per Batch: 0.014345\n",
      "Loss per Batch: 0.014817\n",
      "Loss per Batch: 0.014197\n",
      "Loss per Batch: 0.013753\n",
      "Loss per Batch: 0.015373\n",
      "Loss per Batch: 0.015268\n",
      "Loss per Batch: 0.014914\n",
      "Loss per Batch: 0.015038\n",
      "Loss per Batch: 0.014488\n",
      "Loss per Batch: 0.014196\n",
      "Final Test Loss: 0.014707\n",
      "Average Fidelity: 0.868269\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v1\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(optrs, aa)\n",
    "                rho2 = state_recon_bloch_vec(optrs, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3fc413d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.015117\n",
      "Loss per Batch: 0.014351\n",
      "Loss per Batch: 0.015014\n",
      "Loss per Batch: 0.015634\n",
      "Loss per Batch: 0.015485\n",
      "Loss per Batch: 0.016034\n",
      "Loss per Batch: 0.015016\n",
      "Loss per Batch: 0.015377\n",
      "Loss per Batch: 0.015323\n",
      "Loss per Batch: 0.014743\n",
      "Loss per Batch: 0.014935\n",
      "Loss per Batch: 0.014778\n",
      "Loss per Batch: 0.014924\n",
      "Loss per Batch: 0.014491\n",
      "Loss per Batch: 0.014563\n",
      "Loss per Batch: 0.015800\n",
      "Loss per Batch: 0.014809\n",
      "Loss per Batch: 0.015442\n",
      "Loss per Batch: 0.015333\n",
      "Loss per Batch: 0.015264\n",
      "Final Test Loss: 0.015122\n",
      "Average Fidelity: 0.864662\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v2\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(optrs, aa)\n",
    "                rho2 = state_recon_bloch_vec(optrs, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "044b2e1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.014837\n",
      "Loss per Batch: 0.015367\n",
      "Loss per Batch: 0.015565\n",
      "Loss per Batch: 0.015430\n",
      "Loss per Batch: 0.015593\n",
      "Loss per Batch: 0.015365\n",
      "Loss per Batch: 0.016025\n",
      "Loss per Batch: 0.015432\n",
      "Loss per Batch: 0.015292\n",
      "Loss per Batch: 0.015745\n",
      "Loss per Batch: 0.016366\n",
      "Loss per Batch: 0.014718\n",
      "Loss per Batch: 0.015851\n",
      "Loss per Batch: 0.015823\n",
      "Loss per Batch: 0.015281\n",
      "Loss per Batch: 0.015164\n",
      "Loss per Batch: 0.015368\n",
      "Loss per Batch: 0.015113\n",
      "Loss per Batch: 0.015778\n",
      "Loss per Batch: 0.015061\n",
      "Final Test Loss: 0.015459\n",
      "Average Fidelity: 0.862606\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v3\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(optrs, aa)\n",
    "                rho2 = state_recon_bloch_vec(optrs, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "87228f00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.015842\n",
      "Loss per Batch: 0.015793\n",
      "Loss per Batch: 0.016248\n",
      "Loss per Batch: 0.016135\n",
      "Loss per Batch: 0.015770\n",
      "Loss per Batch: 0.016330\n",
      "Loss per Batch: 0.016097\n",
      "Loss per Batch: 0.016206\n",
      "Loss per Batch: 0.015159\n",
      "Loss per Batch: 0.016307\n",
      "Loss per Batch: 0.016223\n",
      "Loss per Batch: 0.016229\n",
      "Loss per Batch: 0.016039\n",
      "Loss per Batch: 0.015696\n",
      "Loss per Batch: 0.015759\n",
      "Loss per Batch: 0.016497\n",
      "Loss per Batch: 0.016477\n",
      "Loss per Batch: 0.015381\n",
      "Loss per Batch: 0.016224\n",
      "Loss per Batch: 0.016186\n",
      "Final Test Loss: 0.016030\n",
      "Average Fidelity: 0.859249\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v4\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(optrs, aa)\n",
    "                rho2 = state_recon_bloch_vec(optrs, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "528a6fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.012466\n",
      "Loss per Batch: 0.013152\n",
      "Loss per Batch: 0.012871\n",
      "Loss per Batch: 0.012787\n",
      "Loss per Batch: 0.012997\n",
      "Loss per Batch: 0.013003\n",
      "Loss per Batch: 0.012563\n",
      "Loss per Batch: 0.012168\n",
      "Loss per Batch: 0.012906\n",
      "Loss per Batch: 0.012516\n",
      "Loss per Batch: 0.012617\n",
      "Loss per Batch: 0.012455\n",
      "Loss per Batch: 0.012839\n",
      "Loss per Batch: 0.012505\n",
      "Loss per Batch: 0.012823\n",
      "Loss per Batch: 0.012527\n",
      "Loss per Batch: 0.012101\n",
      "Loss per Batch: 0.012841\n",
      "Loss per Batch: 0.012532\n",
      "Loss per Batch: 0.012687\n",
      "Final Test Loss: 0.012668\n",
      "Average Fidelity: 0.884815\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v0\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(optrs, aa)\n",
    "                rho2 = state_recon_bloch_vec(optrs, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae66c9c",
   "metadata": {},
   "source": [
    "Trained on v1 for 1000 epochs with 10,000 copies\n",
    "\n",
    "#v0     \n",
    "Final Test Loss: 0.012668     \n",
    "Average Fidelity: 0.884815\n",
    "\n",
    "#v1   \n",
    "Final Test Loss: 0.014707    \n",
    "Average Fidelity: 0.868269\n",
    "    \n",
    "#v2   \n",
    "Final Test Loss: 0.015122    \n",
    "Average Fidelity: 0.864662\n",
    "    \n",
    "#v3   \n",
    "Final Test Loss: 0.015459    \n",
    "Average Fidelity: 0.862606\n",
    "    \n",
    "#v4   \n",
    "Final Test Loss: 0.016030     \n",
    "Average Fidelity: 0.859249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1bed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
