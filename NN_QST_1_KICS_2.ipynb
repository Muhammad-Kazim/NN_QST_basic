{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cacf45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Quantum States\n",
    "\n",
    "import numpy as np\n",
    "import qutip as qt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pure_state():\n",
    "    eps = 1e-7\n",
    "    rand_ket = qt.rand_ket_haar(4)\n",
    "    rho_p = qt.ket2dm(qt.Qobj(rand_ket))\n",
    "    rho_p = (1-eps)*rho_p + (eps/4)*np.eye(4)\n",
    "    return np.array(rho_p)\n",
    "\n",
    "def mixed_state(d):\n",
    "    \n",
    "    \"\"\"d = dimension of matrix\n",
    "    \"\"\"\n",
    "    G = np.random.normal(0, 1, [d,d]) + 1j*np.random.normal(0, 1, [d,d])\n",
    "    G = np.matrix(G)\n",
    "    rho_m = (G*G.H)/np.trace(G*G.H)\n",
    "    return rho_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea72639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate Noise via Variance\n",
    "\n",
    "def eig_val_corr(state):\n",
    "    \n",
    "    \"\"\" state = nxn dimensional state\n",
    "        Corrects state by truncating negative eigenvalues and reconstructs normailized positive\n",
    "        eigenvalued eigenvectos' states\n",
    "    \"\"\"\n",
    "    if np.sum(np.linalg.eig(state)[0].real < 0) > 0 or np.sum(np.linalg.eig(state)[0].real > 1):\n",
    "        eig_val, eig_vec = np.linalg.eig(state)\n",
    "        eig_val = eig_val.real\n",
    "        eig_val[eig_val < 0] = 0\n",
    "        eig_val = eig_val/np.sum(eig_val)\n",
    "        \n",
    "        d = state.shape[0]\n",
    "        state = np.zeros([d,d], dtype = 'complex')\n",
    "        for ij in range(d):\n",
    "            state += eig_val[ij]*np.matmul(eig_vec[ij].reshape(d,1), np.matrix(eig_vec[ij].reshape(d,1)).H)\n",
    "            \n",
    "    return state\n",
    "            \n",
    "def noise_state(state, var):\n",
    "    \n",
    "    \"\"\" state = 4x4 quantum state\n",
    "        var = scalar value. High var = high noise in state to be measured.\n",
    "    \"\"\"\n",
    "    d = state.shape[0]\n",
    "    noisy_state_raw = state + np.random.normal(0, var, [d,d]) + 1j*np.random.normal(0, var, [d,d])\n",
    "    \n",
    "    if np.sum(np.linalg.eig(noisy_state_raw)[0].real < 0) > 0:\n",
    "        state = eig_val_corr(noisy_state_raw)\n",
    "    else:\n",
    "        state = noisy_state_raw\n",
    "            \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7bf204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1412618cf10>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAjU0lEQVR4nO3de3RU53nv8e+j+10I0Ii7ABsQ+EZAwdcEI1wHJ8f1iU9OYudi16dZHFbtHsc9ceum67Rps9p6ndzbOHU5btI4bWK7SZw4tWM7wTaOHbARNjZgLhYYgQAjAUICobue88cMeCwkNNKMtEczv89as2Zm73ej50Xw23ve2e/e5u6IiEjqygi6ABERGV0KehGRFKegFxFJcQp6EZEUp6AXEUlxWUEXMJDJkyf77Nmzgy5DRGTc2Lx581F3Lx9oXVIG/ezZs6mtrQ26DBGRccPM6gdbp6EbEZEUp6AXEUlxCnoRkRSnoBcRSXEKehGRFDdk0JvZ98ys0cy2DbLezOwfzKzOzN40syVR61aZ2a7IuvsSWbiIiMQmliP6fwVWnWf9DcC8yGM18E8AZpYJPBBZvwi41cwWxVOsiIgM35BB7+4vAsfP0+Qm4GEP2whMMLOpwDKgzt33unsX8Eik7ajo6O5l7Yt7eOnto6P1I0RExqVEjNFPBw5EvW+ILBts+YDMbLWZ1ZpZbVNT07CLyMnM4J/X7+XR2gNDNxYRSSOJCHobYJmfZ/mA3H2tu1e7e3V5+YCzeM8rI8NYURVi/a5Genr7hr29iEiqSkTQNwAzo97PAA6dZ/moWVkVorWjh831zaP5Y0RExpVEBP0TwG2Rs2+uAFrc/TCwCZhnZnPMLAe4JdJ21FwzbzLZmcZzOxtH88eIiIwrsZxe+WNgA7DAzBrM7A/NbI2ZrYk0eQrYC9QB/w/4IwB37wHuAp4BdgCPufv2UejDWcV52Vw+ZxLrFPQiImcNefVKd791iPUO3DnIuqcI7wjGTE1ViL/5z7eoP9ZG5aTCsfzRIiJJKeVmxtZUhQA0fCMiEpFyQT97ciFzywsV9CIiESkX9BA++2bj3mOc6uwJuhQRkcClZNDXVFXQ3euaJSsiQooGffXsMorzsnhu55GgSxERCVxKBn12ZgbL55fz3M4m+voGnYwrIpIWUjLoAVYuDHH0VCdbD7YEXYqISKBSNuiXzw+RYWjylIikvZQN+omFOXxgVpnG6UUk7aVs0EN48tS2g60cae0IuhQRkcCkdNCvXBieJfu8hm9EJI2ldNAvqChm+oR8jdOLSFpL6aA3M2qqQrz09lE6unuDLkdEJBApHfQANQtDtHf3snHvsaBLEREJRMoH/ZVzJ5GfnamLnIlI2kr5oM/LzuTqCyezbkcj4Uvni4ikl5QPegifZnnwRDu7j5wKuhQRkTGXNkEPuhmJiKSntAj6KaV5XDStRLNkRSQtpUXQQ/hmJJvrm2lu6wq6FBGRMRVT0JvZKjPbZWZ1ZnbfAOvLzOxxM3vTzF41s4uj1u0zs61mtsXMahNZ/HDULKygz2H97qagShARCcSQQW9mmcADwA3AIuBWM1vUr9mXgC3ufilwG/DtfutXuPtid69OQM0jcun0UiYX5WiWrIiknViO6JcBde6+1927gEeAm/q1WQSsA3D3ncBsM6tIaKVxysgwViwIsX5XI929fUGXIyIyZmIJ+unAgaj3DZFl0d4AbgYws2VAJTAjss6BZ81ss5mtHuyHmNlqM6s1s9qmptEZXqmpCtHa0cPm+uZR+fNFRJJRLEFvAyzrP/PofqDMzLYAfwy8DvRE1l3t7ksID/3caWYfHuiHuPtad6929+ry8vKYih+ua+ZNJjvTdDVLEUkrsQR9AzAz6v0M4FB0A3dvdfc73H0x4TH6cuCdyLpDkedG4HHCQ0GBKM7L5vI5kzROLyJpJZag3wTMM7M5ZpYD3AI8Ed3AzCZE1gF8HnjR3VvNrNDMiiNtCoHrgW2JK3/4aqpC1DWeov5YW5BliIiMmSGD3t17gLuAZ4AdwGPuvt3M1pjZmkizhcB2M9tJeIjm7sjyCuAlM3sDeBV40t2fTnQnhuPMzUg0S1ZE0kVWLI3c/SngqX7LHox6vQGYN8B2e4HL4qwxoSonFXJBeSHP7WzkjqvnBF2OiMioS5uZsdFWLqxg495jnOrsGbqxiMg4l5ZBX1MVorvXeeltzZIVkdSXlkG/tLKM4rwsjdOLSFpIy6DPzsxg+fxyntvZRF+fbkYiIqktLYMewmffHD3VydaDLUGXIiIyqtI26JfPD5FhaPKUiKS8tA36iYU5LJlVppuRiEjKS9ugB6hZGGLbwVbebekIuhQRkVGT1kG/sip8JeXnd2n4RkRSV1oH/fyKIqZPyNdpliKS0tI66M2MmqoQL719lI7u3qDLEREZFWkd9BAep2/v7mXj3mNBlyIiMirSPuivnDuJ/OxMDd+ISMpK+6DPy87k6gsns25HI+6aJSsiqSftgx7Cs2QPnmhn95FTQZciIpJwCnpgxYLwzUjWafKUiKQgBT0wpTSPi6eX8NwOjdOLSOpR0EfUVFXw2v5mmtu6gi5FRCShFPQRNVUh+hzW79bNSEQktcQU9Ga2ysx2mVmdmd03wPoyM3vczN40s1fN7OJYt00Wl04vZXJRjq5mKSIpZ8igN7NM4AHgBmARcKuZLerX7EvAFne/FLgN+PYwtk0KGRnGigUh1u9qpLu3L+hyREQSJpYj+mVAnbvvdfcu4BHgpn5tFgHrANx9JzDbzCpi3DZprFwYorWjh831zUGXIiKSMLEE/XTgQNT7hsiyaG8ANwOY2TKgEpgR47ZJ45p55WRnmmbJikhKiSXobYBl/aeQ3g+UmdkW4I+B14GeGLcN/xCz1WZWa2a1TU3BfCFalJvFFXMnsW6HzqcXkdQRS9A3ADOj3s8ADkU3cPdWd7/D3RcTHqMvB96JZduoP2Otu1e7e3V5eXnsPUiwmqoQe5raqD/WFlgNIiKJFEvQbwLmmdkcM8sBbgGeiG5gZhMi6wA+D7zo7q2xbJtsaqrCs2Q1fCMiqWLIoHf3HuAu4BlgB/CYu283szVmtibSbCGw3cx2Ej7D5u7zbZv4biRO5aRCLigvVNCLSMrIiqWRuz8FPNVv2YNRrzcA82LdNtmtXFjB919+h1OdPRTlxvRXJCKStDQzdgA1VSG6e52X3tYsWREZ/xT0A1haWUZJXhbrdJEzEUkBCvoBZGdmsHxBiOd3NdLXp5uRiMj4pqAfxMqqEEdPdbH1YEvQpYiIxEVBP4jl88vJMHSRMxEZ9xT0gygrzGHJrDKe012nRGScU9CfR83CENsOtvJuS0fQpYiIjJiC/jxWVlUA8PwuDd+IyPiloD+P+RVFTJ+Qr9MsRWRcU9Cfh5mxcmGIl+uO0tHdG3Q5IiIjoqAfQk1ViPbuXjbuPRZ0KSIiI6KgH8IVcyeRn52pi5yJyLiloB9CXnYm18ybzLodjbhrlqyIjD8K+hjUVIU4eKKd3UdOBV2KiMiwKehjsGJB+GYk6zR5SkTGIQV9DKaU5nHx9BKe02mWIjIOKehjVFNVwWv7m2lu6wq6FBGRYVHQx2hlVYg+h/W7dTMSERlfFPQxumR6KZOLcnU1SxEZdxT0McrIMGqqylm/q5Hu3r6gyxERiVlMQW9mq8xsl5nVmdl9A6wvNbNfmtkbZrbdzO6IWrfPzLaa2RYzq01k8WOtpqqC1o4eNtc3B12KiEjMhgx6M8sEHgBuABYBt5rZon7N7gTecvfLgGuBr5tZTtT6Fe6+2N2rE1N2MK6ZN5nsTNMsWREZV2I5ol8G1Ln7XnfvAh4BburXxoFiMzOgCDgO9CS00iRQlJvFFXMnsW6HzqcXkfEjlqCfDhyIet8QWRbtO8BC4BCwFbjb3c8MZDvwrJltNrPVg/0QM1ttZrVmVtvUlLxnttRUhdjT1Eb9sbagSxERiUksQW8DLOt/0ZePAFuAacBi4DtmVhJZd7W7LyE89HOnmX14oB/i7mvdvdrdq8vLy2OpPRA1VeFZshq+EZHxIpagbwBmRr2fQfjIPdodwM88rA54B6gCcPdDkedG4HHCQ0HjVuWkQi4MFSnoRWTciCXoNwHzzGxO5AvWW4An+rXZD6wEMLMKYAGw18wKzaw4srwQuB7Ylqjig7KyKsTGvcc41ZlyX0OISAoaMujdvQe4C3gG2AE85u7bzWyNma2JNPsKcJWZbQXWAX/m7keBCuAlM3sDeBV40t2fHo2OjKWaqhDdvc5LbyfvdwkiImdkxdLI3Z8Cnuq37MGo14cIH633324vcFmcNSadpZVllORlsW5HI6sunhp0OSIi56WZsSOQlZnB8gUhnt/VSF+fbkYiIslNQT9CK6tCHD3VxZsHW4IuRUTkvBT0I7R8fjkZBs9p8pSIJDkF/QiVFeawtLKM53bpNEsRSW4K+jjUVFWw7WAr77Z0BF2KiMigFPRxWLkwPEv2eR3Vi0gSU9DHYV6oiBll+azTvWRFJIkp6ONgZtRUhXi57igd3b1BlyMiMiAFfZxqqkK0d/eyYe+xoEsRERmQgj5OV8ydRH52Js9p+EZEkpSCPk552ZlcM28yv9lxhK4e3UtWRJKPgj4BPr1sFodbOvjmb3YHXYqIyDkU9AmwoirEp6pn8uD6Pbz6zvGgyxEReR8FfYL8nxsXMbOsgHse3cLJju6gyxEROUtBnyBFuVl881OLOdzSzpefeCvockREzlLQJ9DSyjLuXHEhP32tgae2Hg66HBERQEGfcP9r5TwunVHKlx7fypFWXQNHRIKnoE+w7MwMvvmpxXR09/LF/3hDNyYRkcAp6EfBBeVF/MVHF/Lbt4/y8IZ9QZcjImlOQT9KPntFJdcuKOfvf7WTt4+cDLocEUljMQW9ma0ys11mVmdm9w2wvtTMfmlmb5jZdjO7I9ZtU5WZ8X//26UU5GTyhUe3aNasiARmyKA3s0zgAeAGYBFwq5kt6tfsTuAtd78MuBb4upnlxLhtygqV5PH3N1/K9kOtfEuzZkUkILEc0S8D6tx9r7t3AY8AN/Vr40CxmRlQBBwHemLcNqWtungKn6yewYPr97Bpn2bNisjYiyXopwMHot43RJZF+w6wEDgEbAXudve+GLcFwMxWm1mtmdU2NTXFWP748Jc3XsQMzZoVkYDEEvQ2wLL+5wx+BNgCTAMWA98xs5IYtw0vdF/r7tXuXl1eXh5DWeNHeNbsZRw60c5f/1KzZkVkbMUS9A3AzKj3MwgfuUe7A/iZh9UB7wBVMW6bFpZWTuSPrr2Qn2xu4OltmjUrImMnlqDfBMwzszlmlgPcAjzRr81+YCWAmVUAC4C9MW6bNu6+bh6XTC/lz3+2lUbNmhWRMTJk0Lt7D3AX8AywA3jM3beb2RozWxNp9hXgKjPbCqwD/szdjw627Wh0ZDw4M2u2vbuXe3/yJu6aNSsio8+SMWyqq6u9trY26DJGzcMb9vGXv9jO39x0EbddOTvockQkBZjZZnevHmidZsYG4HNXVLJ8fjl/++QO6hpPBV2OiKQ4BX0AzIyvfiI8a/YezZoVkVGmoA9IeNbsJWw92MI/rHs76HJEJIUp6AO06uKpfGLpDL77Qh2b6zVrVkRGh4I+YH914yKmTcjnnkff4FRnT9DliEgKUtAHrDgvm29+ajENzaf5m1+m7ZmnIjKKFPRJ4IOzJ7Jm+QU8VtvAM9vfDbocEUkxCvok8YXr5nPx9JLwrNmTmjUrIomjoE8SOVkZfOtTi2nr7OFPNWtWRBJIQZ9ELgwV8+c3VPHCrib+7ZX9QZcjIilCQZ9kbrtyNh+aN5m/ffIt9jRp1qyIxE9Bn2QyMoyv/ffLyMsOz5rt7tWsWRGJj4I+CVWU5PF3H7+ENxs0a1ZE4qegT1IfvWQqNy+ZzgPPa9asiMRHQZ/Evvz7FzG1VLNmRSQ+CvokVhKZNXug+TRf0b1mRWSEFPRJbtmc8KzZR2sPaNasiIyIgn4cuOe6+Vw0TbNmRWRkFPTjQPSs2T/TrFkRGSYF/Tgxr6KY+26o4vldTfy7Zs2KyDDEFPRmtsrMdplZnZndN8D6e81sS+Sxzcx6zWxiZN0+M9saWZe6d/weA7efnTW7g72aNSsiMRoy6M0sE3gAuAFYBNxqZoui27j7V919sbsvBv4cWO/u0Sd/r4isH/AO5RKbjAzjq5+4jJysDM2aFZGYxXJEvwyoc/e97t4FPALcdJ72twI/TkRxcq4ppeFZs280tPCPz9UFXY6IjAOxBP104EDU+4bIsnOYWQGwCvhp1GIHnjWzzWa2erAfYmarzazWzGqbmppiKCt9fezSqdz8gfCs2df2NwddjogkuViC3gZYNthpHzcCL/cbtrna3ZcQHvq508w+PNCG7r7W3avdvbq8vDyGstLbl2+6iCkledzz6BZOdnQHXY6IJLFYgr4BmBn1fgZwaJC2t9Bv2MbdD0WeG4HHCQ8FSZxK8rL5xicv48Dx09R8fT0/+N0+Ont6gy5LRJJQLEG/CZhnZnPMLIdwmD/Rv5GZlQLLgV9ELSs0s+Izr4HrgW2JKFzg8rmT+I81VzJnciF/9cR2ar62nsc2HaBHX9KKSJQhg97de4C7gGeAHcBj7r7dzNaY2Zqoph8HnnX3tqhlFcBLZvYG8CrwpLs/nbjyZWnlRB5dfQUP/49lTC7K4U9/+ibXf/NFnnjjEH19mlglImDJOMuyurraa2t1yv1wuTvPvnWEbzy7m11HTlI1pZgvXr+AlQtDmA30VYuIpAoz2zzYKeyaGZtCzIyPXDSFp+7+EN++ZTEd3b18/uFaPv7d3/Fy3dGgyxORgCjoU1BmhnHT4un8+k+Wc//Nl3CktYPPPPQKt67dyOZ6nY4pkm40dJMGOrp7+dEr+/nuC3UcPdVFTVWI/339fC6aVhp0aSKSIOcbulHQp5G2zh7+9Xf7+Of1e2jt6OFjl07lnuvmc2GoKOjSRCROCnp5n5b2bh767V7+5aV36Oju5eYlM7h75TxmTiwIujQRGSEFvQzo2KlO/umFPTy8sR5355YPzuKumgupKMkLujQRGSYFvZzX4ZZ2/vG5Oh7bdIDMDOP2q2azZvkFTCzMCbo0EYmRgl5iUn+sjW//5m0e33KQwpws/vCaOXz+Q3MozssOujQRGYKCXoZl95GTfOPZ3Ty9/V0mFGSzZvkF3H7lbPJzMoMuTUQGoaCXEdna0MLXnt3F+t1NlBfncteKC7ll2UxysxT4IslGQS9x2bTvOF99ZhevvnOc6RPyuXvlPG5eMp2sTM23E0kWugSCxOWDs9+7cNqkqAunPf56A109ulKmSLLTEb0Mi7vzzPYjfOPXu9h95BSTi3L59OWz+Mzls3RapkiANHQjCdfX57z4dhMPb6jn+V2NZJrxkYuncPuVs/ng7DJdLVNkjJ0v6LPGuhhJDRkZxrULQly7IET9sTb+bWM9j246wJNvHqZqSjG3XzWbmxZPoyBH/8REgqYjekmY9q5efr7lID/43T52vnuSkrwsPlk9k89dWUnlpMKgyxNJaRq6kTHl7mza18wPNuzjmW3v0uvOtfPLuf2q2Xx4XjkZGRrWEUk0Dd3ImDIzls2ZyLI5E3m3pYMfvbqfH72ynz/4/iZmTyrgc1fO5hNLZ1Carxm3ImNBR/QyJrp6+vjVtsM8vKGezfXN5Gdn8vEl07ntykqqppQEXZ7IuBf30I2ZrQK+DWQCD7n7/f3W3wt8JvI2C1gIlLv78aG2HYiCPrVtO9jCwxv28Ysth+js6ePyORO5/arZ/N6iCrI1CUtkROIKejPLBHYDvwc0AJuAW939rUHa3wjc4+41w932DAV9emhu6+Kx2gP8cGM9Dc3tTCnJ4zOXz+KWZbMoL84NujyRcSXembHLgDp33+vuXcAjwE3naX8r8OMRbitppKwwh/+5/ALW37uCh26rZl5FEV//9W6uun8dX3jkdV7b30wyDi2KjDexfBk7HTgQ9b4BuHyghmZWAKwC7hrBtquB1QCzZs2KoSxJFZkZxnWLKrhuUQV7mk7xww31/GRzAz/fcohLppdy25WV3HjZNPKydTE1kZGI5Yh+oHPhBjvMuhF42d2PD3dbd1/r7tXuXl1eXh5DWZKKLigv4su/fxEbv7SSr/zXi+no7uXen7zJlX+/jvt/tZOG5tNBlygy7sRyRN8AzIx6PwM4NEjbW3hv2Ga424qcVZSbxeeuqOSzl89iw95jPPy7eta+uIe1L+5hxYIQVVOLCRXnESrOJVSSS3lRHqGSXB31iwwglqDfBMwzsznAQcJh/un+jcysFFgOfHa424oMxsy46oLJXHXBZA6eaOdHr9Tz89cP8cLuJnr7zv1wWJyXRag4l/Li3PftCELFeZFl4dcl+Vm6Ho+kjSGD3t17zOwu4BnCp0h+z923m9mayPoHI00/Djzr7m1DbZvoTkh6mD4hn3s/UsW9H6mit89pPt1FY2snjSc7aDzZSVPk0Xiyg8bWTrYcOEHjyQ46us+9lHJOVgblRWd2Av12BCXv7SQmFubouvsy7mnClKQ0d+dUZw+NJztpbO2k6VQnja0dkR1CeKdw5vWJ093nbJ9hMLHwvR3AnMmFLK0sY8msMqZNyA+gRyID0yUQJG2ZGcV52RTnZXNBedF523b29EZ9Kgg/mlrf+7Rw5GQHG/ce4/sv7wNgamkeSyKhv7SyjEVTS8jJ0tG/JB8FvUhEblYmM8oKmFFWMGib7t4+dh4+yeb642zef4LX6pt58s3Dke0zuHRG6dnwXzKrTBO/JClo6EYkTu+2dPDa/mZeq29m8/5mth1sobs3/P+qclJBOPQry1gyawJVU0rI1NU7ZRToMsUiY6iju5fth1rYXN/Ma/Un2Ly/maaTnQAU5mSyeNaE98J/ZhmlBbqKp8RPY/QiYygvO5OllRNZWjkRCH8h3NDczmv7m8Phv7+Z776w5+zpoReGilgaGedfUjmBuZOLdM1+SSgd0YsEoK2zhzcaTvD6/hNnw//MWT+l+dksiRz1L60s47KZEyjM1TGZnJ+O6EWSTGFu1tmJYBA+6t97tI3N9c28Hjnyf35XExA+xXN+RTGVkwqYWVbAzIkFzCjLP/us+/LKUPQvRCQJmBkXlBdxQXkRn6wOXzWkpb2bLQfCR/zbDrawp6mN9bubzpkANqkwhxkTC5hZls+MsgJmTsxnZll4JzC9LJ/cLF0WIt0p6EWSVGl+Nsvnl7N8/nsX+XN3jp7q4kDzaRqa2zlw/DQNkdfbDrbwzPZ3z57xA2AGoeLc938SKCtgRmRnMLU0TzN/04CCXmQcMTPKI9fyWTKr7Jz1vX1O48kODhw/sxNo50DzaQ4cP82r7xznF1vaib5EUGaGMbU0770dwJlPBJGdQkVxnr4YTgEKepEUEg7ufKaW5rNszsRz1nf39nH4RAcNzacjO4D2s58O1u9uojFyGugZOZkZVJTmMqUkj4rIY0pJHhWlkeeSXCpK8nTV0CSnoBdJI9mZGcyaVMCsSQPP/u3o7uXgifCngQPN7TQcP83hlg6OtHaw7WALv9lxZMCLxE0oyKai+MwOILxjCEV2ClNKw5eQnlyYq08HAVHQi8hZedmZZ78UHoi709rRw5HWDt5t6eDd1g4aW8PP77aELxK383ArR0910v8q0lkZRqg4l4rSPCqKwzuAisinguhPCTqVNPH0NyoiMTMzSvOzKc3PZn5F8aDtenr7OHqqK7IDCH8iOBLZIRxp7aCu6RQv1x3lZGfPOdsW5WaFw780j6ml+UybkM+00jymRj0XaWcwLPrbEpGEy8rMYEpp+Kj9ffeY66ets+ds+B+JfCo48/pwSwe/fTv8vUH/eZ0leVlMm5DP1NK88I4g8npqaT7TJ+RTUZqr00qjKOhFJDCFuVnnHSqC8BfIR1o7OHSig8Mt7e97PnSinS0HTtA8wL0EJhflMm1CHtNK85ka/Twhn2ml+ZQX56bNBeYU9CKS1LIzM4a8fHR7V+974d/SzuHIzuDgiXbqmk7x27ebaOvqfd82WRlGRUke0yaEPwmc3RmUhu82NrEwh4mFORTljv/bTiroRWTcy8/JZG55EXOH+BL50In2930qOHyig4ORTwVPb+ugq3eA205mZlBWmE1ZQQ6TinLCz4U5TCzMZWJhNhMLcykrzGZS5LmsIIfsJJuEpqAXkZQX/SXywqklA7bp63OOtXVx6EQ7x9u6ONbWRXP/59NdbD/UyvG2Llrazx0uOqMkL4tJRbmUFWS/b4dw5nlSYQ5lhTlnnwtzMkf1U4OCXkQEyMh4b9ZxLLp7+zhxupvjbV3vPU53cfxUF8fbOjl+upvjbZ0cPNHO1oMnaG7rHvATA4RvVj+xIIdZEwt4bM2ViewWEGPQm9kq4NtAJvCQu98/QJtrgW8B2cBRd18eWb4POAn0Aj2DXUZTRGQ8yc7MGNaO4cyN6pvbujnW1knz6S6OnQp/SjjzqWG0vhweMujNLBN4APg9oAHYZGZPuPtbUW0mAN8FVrn7fjML9ftjVrj70cSVLSIyvkTfqH6wmcmjJZZvDJYBde6+1927gEeAm/q1+TTwM3ffD+DujYktU0RERiqWoJ8OHIh63xBZFm0+UGZmL5jZZjO7LWqdA89Glq8e7IeY2WozqzWz2qampljrFxGRIcQyRj/QoFH/+w9mAUuBlUA+sMHMNrr7buBqdz8UGc75tZntdPcXz/kD3dcCayF8K8HhdEJERAYXyxF9A++fxDwDODRAm6fdvS0yFv8icBmAux+KPDcCjxMeChIRkTESS9BvAuaZ2RwzywFuAZ7o1+YXwIfMLMvMCoDLgR1mVmhmxQBmVghcD2xLXPkiIjKUIYdu3L3HzO4CniF8euX33H27ma2JrH/Q3XeY2dPAm0Af4VMwt5nZXODxyESALOBH7v70aHVGRETOZd7/snBJoLq62mtra4MuQ0Rk3DCzzYPNU0quCzKIiEjCJeURvZk1AfUj3HwykG6Ts9Tn1Jdu/QX1ebgq3b18oBVJGfTxMLPadLvMgvqc+tKtv6A+J5KGbkREUpyCXkQkxaVi0K8NuoAAqM+pL936C+pzwqTcGL2IiLxfKh7Ri4hIFAW9iEiKGzdBb2arzGyXmdWZ2X0DrDcz+4fI+jfNbEms2yarkfbZzGaa2fNmtsPMtpvZ3WNf/cjE83uOrM80s9fN7D/Hrur4xPlve4KZ/cTMdkZ+34m/D90oiLPP90T+XW8zsx+bWd7YVj8yMfS5ysw2mFmnmX1xONsOyd2T/kH4Gjt7gLlADvAGsKhfm48CvyJ8WeUrgFdi3TYZH3H2eSqwJPK6GNid6n2OWv8nwI+A/wy6P2PRZ+AHwOcjr3OACUH3aTT7TPheGO8A+ZH3jwF/EHSfEtTnEPBB4G+BLw5n26Ee4+WIPpa7XN0EPOxhG4EJZjY1xm2T0Yj77O6H3f01AHc/Cezg3JvFJKN4fs+Y2QzgY8BDY1l0nEbcZzMrAT4M/AuAu3e5+4kxrH2k4vo9E75AYr6ZZQEFnHvZ9GQ0ZJ/dvdHdNwHdw912KOMl6GO5y9VgbWLZNhnF0+ezzGw28AHglcSXmHDx9vlbwJ8SvoLqeBFPn+cCTcD3I8NVD0UuB57sRtxndz8IfA3YDxwGWtz92VGsNVHiyaG4M2y8BH0sd7karE0s2yajePocXmlWBPwU+IK7tyawttEy4j6b2X8BGt19c+LLGlXx/J6zgCXAP7n7B4A2YDx8BxXP77mM8NHsHGAaUGhmn01wfaMhnhyKO8PGS9DHepergdrEsm0yiqfPmFk24ZD/d3f/2SjWmUjx9Plq4PfNbB/hj7Y1ZvZvo1dqwsT7b7vB3c98WvsJ4eBPdvH0+TrgHXdvcvdu4GfAVaNYa6LEk0PxZ1jQX1LE+EVGFrCX8F78zJcRF/Vr8zHe/+XNq7Fum4yPOPtswMPAt4Lux1j1uV+baxk/X8bG1Wfgt8CCyOsvA18Nuk+j2WfCd6/bTnhs3gh/Gf3HQfcpEX2Oavtl3v9lbNwZFvhfwDD+oj5K+OyRPcBfRJatAdZEXhvwQGT9VqD6fNuOh8dI+wxcQ/ij3ZvAlsjjo0H3Z7R/z1F/xrgJ+nj7DCwGaiO/658DZUH3Zwz6/NfATsK3Jf0hkBt0fxLU5ymEj95bgROR1yWDbTuchy6BICKS4sbLGL2IiIyQgl5EJMUp6EVEUpyCXkQkxSnoRURSnIJeRCTFKehFRFLc/we5Zr8U8jRI9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checking relation between var and infidelity\n",
    "\n",
    "fid = np.zeros([11, 1000])\n",
    "k=0\n",
    "for ij in range(11):\n",
    "    var = 0.01*ij\n",
    "    for jk in range(1000):\n",
    "        rho = mixed_state(d=8)\n",
    "        rho_noisy = noise_state(rho, var)\n",
    "\n",
    "        fid[ij, jk] = qt.fidelity(qt.Qobj(rho), qt.Qobj(rho_noisy))\n",
    "        \n",
    "plt.plot(0.01*np.arange(11), np.mean(fid, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dffd9d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pauli_basis():\n",
    "    \n",
    "    I = np.array([[1, 0],\n",
    "                  [0, 1]], dtype = 'complex')\n",
    "    X = np.array([[0, 1],\n",
    "                  [1, 0]], dtype = 'complex')\n",
    "    Y = np.array([[0, -1j],\n",
    "                  [1j, 0]], dtype = 'complex')\n",
    "    Z = np.array([[1, 0],\n",
    "                  [0, -1]], dtype = 'complex')\n",
    "    pauli = np.array([I, X, Y ,Z], dtype = 'complex')\n",
    "    \n",
    "    return pauli\n",
    "\n",
    "def pauli_words(optrs, qbts):\n",
    "    if qbts == 1:\n",
    "        return optrs\n",
    "    else:\n",
    "        d = optrs.shape[-1]\n",
    "        pauli = pauli_basis()\n",
    "        temp_optrs = np.zeros([(2*d)**2, d*2, d*2], dtype = 'complex')\n",
    "        optrs = optrs.reshape([d**2, d, d])\n",
    "        for i in range(4):\n",
    "            for j in range(d**2):\n",
    "                temp_optrs[(d**2)*i + j] = np.kron(pauli[i], optrs[j])\n",
    "        rep_4 = int(np.log10((2*d)**2)/np.log10(4))\n",
    "        temp_optrs = temp_optrs.reshape(np.append(np.repeat(4, rep_4), [d*2, d*2]))\n",
    "        return pauli_words(temp_optrs, qbts - 1)\n",
    "\n",
    "\n",
    "def counts_to_bloch(rho_noisy, optrs, shots, qbts):\n",
    "    \n",
    "    mn_rnd = np.zeros([4**qbts, 2**qbts])\n",
    "    mn_eval = np.zeros([4**qbts, 2**qbts])\n",
    "\n",
    "    for ii in range(4**qbts):\n",
    "        rr = np.zeros([1, 2**qbts])\n",
    "        for jj in range(2**qbts):\n",
    "            e_val, e_vec = np.linalg.eig(optrs.reshape(64, 8, 8)[ii])\n",
    "            e_vec = np.matrix(e_vec[:,jj]).transpose()\n",
    "            e_vec_mat = e_vec*e_vec.getH()\n",
    "            rr[0, jj] = np.real(np.trace(np.matmul(rho_noisy, e_vec_mat)))\n",
    "        rr = np.round(rr[0], 8)\n",
    "        rr[rr < 0] = 0\n",
    "        rr = rr/np.sum(rr)\n",
    "        rr[-1] = 1 - np.sum(rr[:-1])\n",
    "        if np.sum(rr < 0) > 0:\n",
    "            print(rr)\n",
    "        rr[rr < 0.0000000001] = 0\n",
    "        mn_rnd[ii, :] = np.random.multinomial(shots, rr)\n",
    "        mn_eval[ii, :] = e_val.real\n",
    "\n",
    "    return np.sum(mn_rnd*mn_eval, axis=1)/shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27ff179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-7b831e6ad38d>:44: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  temp_labels[i, j, k] = np.trace(np.matmul(optrs[i,j,k], rho))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoint: 0\n",
      "Datapoint: 100\n",
      "Datapoint: 200\n",
      "Datapoint: 300\n",
      "Datapoint: 400\n",
      "Datapoint: 500\n",
      "Datapoint: 600\n",
      "Datapoint: 700\n",
      "Datapoint: 800\n",
      "Datapoint: 900\n",
      "Datapoint: 1000\n",
      "Datapoint: 1100\n",
      "Datapoint: 1200\n",
      "Datapoint: 1300\n",
      "Datapoint: 1400\n",
      "Datapoint: 1500\n",
      "Datapoint: 1600\n",
      "Datapoint: 1700\n",
      "Datapoint: 1800\n",
      "Datapoint: 1900\n",
      "Datapoint: 2000\n",
      "Datapoint: 2100\n",
      "Datapoint: 2200\n",
      "Datapoint: 2300\n",
      "Datapoint: 2400\n",
      "Datapoint: 2500\n",
      "Datapoint: 2600\n",
      "Datapoint: 2700\n",
      "Datapoint: 2800\n",
      "Datapoint: 2900\n",
      "Datapoint: 3000\n",
      "Datapoint: 3100\n",
      "Datapoint: 3200\n",
      "Datapoint: 3300\n",
      "Datapoint: 3400\n",
      "Datapoint: 3500\n",
      "Datapoint: 3600\n",
      "Datapoint: 3700\n",
      "Datapoint: 3800\n",
      "Datapoint: 3900\n",
      "Datapoint: 4000\n",
      "Datapoint: 4100\n",
      "[ 1.73780215e-01  1.16681549e-01  2.14466498e-01  6.20100676e-02\n",
      "  1.53552965e-01  1.23332071e-01  1.56176633e-01 -2.22044605e-16]\n",
      "[ 1.16681549e-01  1.73780215e-01  2.14466498e-01  6.20100676e-02\n",
      "  1.23332071e-01  1.53552965e-01  1.56176633e-01 -2.22044605e-16]\n",
      "Datapoint: 4200\n",
      "Datapoint: 4300\n",
      "Datapoint: 4400\n",
      "Datapoint: 4500\n",
      "Datapoint: 4600\n",
      "Datapoint: 4700\n",
      "Datapoint: 4800\n",
      "[ 1.60105227e-01  1.63095369e-01  9.11045389e-02  1.16882802e-01\n",
      "  1.15027527e-01  2.01778187e-01  1.52006349e-01 -2.22044605e-16]\n",
      "Datapoint: 4900\n",
      "Datapoint: 5000\n",
      "Datapoint: 5100\n",
      "Datapoint: 5200\n",
      "Datapoint: 5300\n",
      "Datapoint: 5400\n",
      "Datapoint: 5500\n",
      "Datapoint: 5600\n",
      "Datapoint: 5700\n",
      "Datapoint: 5800\n",
      "Datapoint: 5900\n",
      "Datapoint: 6000\n",
      "Datapoint: 6100\n",
      "Datapoint: 6200\n",
      "Datapoint: 6300\n",
      "Datapoint: 6400\n",
      "Datapoint: 6500\n",
      "Datapoint: 6600\n",
      "Datapoint: 6700\n",
      "Datapoint: 6800\n",
      "Datapoint: 6900\n",
      "Datapoint: 7000\n",
      "Datapoint: 7100\n",
      "Datapoint: 7200\n",
      "Datapoint: 7300\n",
      "Datapoint: 7400\n",
      "Datapoint: 7500\n",
      "Datapoint: 7600\n",
      "Datapoint: 7700\n",
      "Datapoint: 7800\n",
      "Datapoint: 7900\n",
      "Datapoint: 8000\n",
      "Datapoint: 8100\n",
      "Datapoint: 8200\n",
      "Datapoint: 8300\n",
      "Datapoint: 8400\n",
      "Datapoint: 8500\n",
      "Datapoint: 8600\n",
      "Datapoint: 8700\n",
      "Datapoint: 8800\n",
      "Datapoint: 8900\n",
      "Datapoint: 9000\n",
      "Datapoint: 9100\n",
      "Datapoint: 9200\n",
      "Datapoint: 9300\n",
      "Datapoint: 9400\n",
      "Datapoint: 9500\n",
      "Datapoint: 9600\n",
      "Datapoint: 9700\n",
      "Datapoint: 9800\n",
      "Datapoint: 9900\n"
     ]
    }
   ],
   "source": [
    "data_pts = 10000\n",
    "d = 8\n",
    "var_vals = 5\n",
    "k = np.zeros(64)\n",
    "qbts = 3\n",
    "shots = 100\n",
    "\n",
    "optrs = pauli_words(pauli_basis(), qbts)\n",
    "\n",
    "var_0_L = np.zeros([data_pts, 64])\n",
    "var_0_I = np.zeros([data_pts, 64])\n",
    "v0 = 0\n",
    "\n",
    "var_1_L = np.zeros([data_pts, 64])\n",
    "var_1_I = np.zeros([data_pts, 64])\n",
    "v1 = 0\n",
    "\n",
    "var_2_L = np.zeros([data_pts, 64])\n",
    "var_2_I = np.zeros([data_pts, 64])\n",
    "v2 = 0\n",
    "\n",
    "var_3_L = np.zeros([data_pts, 64])\n",
    "var_3_I = np.zeros([data_pts, 64])\n",
    "v3 = 0\n",
    "\n",
    "var_4_L = np.zeros([data_pts, 64])\n",
    "var_4_I = np.zeros([data_pts, 64])\n",
    "v4 = 0\n",
    "\n",
    "for ij in range(data_pts):\n",
    "    \n",
    "    rho = mixed_state(d)\n",
    "    \n",
    "    for jk in range(var_vals):\n",
    "        var = 0.01*jk\n",
    "        rho_noisy = noise_state(rho, var)\n",
    "        \n",
    "        temp_labels = np.zeros([4,4,4])\n",
    "        temp_inputs = np.zeros([4,4,4])\n",
    "        \n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                for k in range(4):\n",
    "                    temp_labels[i, j, k] = np.trace(np.matmul(optrs[i,j,k], rho))        \n",
    "         \n",
    "        if jk == 0:\n",
    "            var_0_L[v0] = temp_labels.reshape(64)\n",
    "            var_0_I[v0] = counts_to_bloch(rho_noisy,optrs, shots, qbts)\n",
    "            v0 += 1\n",
    "        \n",
    "        if jk == 1:\n",
    "            var_1_L[v1] = temp_labels.reshape(64)\n",
    "            var_1_I[v1] = counts_to_bloch(rho_noisy,optrs, shots, qbts)\n",
    "            v1 += 1\n",
    "            \n",
    "        if jk == 2:\n",
    "            var_2_L[v2] = temp_labels.reshape(64)\n",
    "            var_2_I[v2] = counts_to_bloch(rho_noisy,optrs, shots, qbts)\n",
    "            v2 += 1\n",
    "        \n",
    "        if jk == 3:\n",
    "            var_3_L[v3] = temp_labels.reshape(64)\n",
    "            var_3_I[v3] = counts_to_bloch(rho_noisy,optrs, shots, qbts)\n",
    "            v3 += 1\n",
    "            \n",
    "        if jk == 4:\n",
    "            var_4_L[v4] = temp_labels.reshape(64)\n",
    "            var_4_I[v4] = counts_to_bloch(rho_noisy,optrs, shots, qbts)\n",
    "            v4 += 1\n",
    "    if ij%100 == 0:\n",
    "        print(\"Datapoint:\",ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "686c9209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96591918971348 0.7968009097585905 0.7439562079331457 0.722995050879582 0.7124064729925206\n",
      "0.009683743778738967 0.03404544584964945 0.043532250446862875 0.04781267772271946 0.050139277949469\n"
     ]
    }
   ],
   "source": [
    "def state_recon_bloch_vec(optrs, r):\n",
    "    \n",
    "    \"\"\"return state given pauli tensors of X, Y, Z and bloch vectors of size d**qbts\n",
    "    \"\"\"\n",
    "    var_0_L_a = r.reshape(4,4,4)\n",
    "    state_act = np.zeros([8,8], dtype = 'complex')\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(4):\n",
    "                state_act += var_0_L_a[i, j, k]*optrs[i,j,k]\n",
    "    \n",
    "    return state_act/8\n",
    "                \n",
    "state_act = state_recon_bloch_vec(optrs, var_4_L[2])\n",
    "state_est = state_recon_bloch_vec(optrs, var_4_I[2])\n",
    "            \n",
    "qt.fidelity(qt.Qobj(state_act), qt.Qobj(state_est))\n",
    "\n",
    "def fid_avg(r_noise, r_actual, l):\n",
    "    \n",
    "    fidelity = 0.0\n",
    "    for ij in range(l):\n",
    "        a = state_recon_bloch_vec(optrs, r_noise[ij])\n",
    "        b = state_recon_bloch_vec(optrs, r_actual[ij])\n",
    "        fidelity += qt.fidelity(qt.Qobj(a), qt.Qobj(b))\n",
    "    return fidelity/l\n",
    "\n",
    "def mse(x1, y1):\n",
    "    \n",
    "    MSE = np.square(np.subtract(x1,y1)).mean()\n",
    "    return MSE\n",
    "\n",
    "def mse_avg(X, Y):\n",
    "    \n",
    "    x_len = X.shape[0]\n",
    "    y_len = Y.shape[0]\n",
    "    \n",
    "    avg_mse = 0.0\n",
    "    \n",
    "    if x_len == y_len:\n",
    "        for ii in range(x_len):\n",
    "            avg_mse += mse(X[ii], Y[ii])\n",
    "    \n",
    "    return avg_mse/x_len\n",
    "\n",
    "a_f = fid_avg(var_0_I, var_0_L, data_pts)\n",
    "a_m = mse_avg(var_0_I, var_0_L)\n",
    "\n",
    "b_f = fid_avg(var_1_I, var_1_L, data_pts)\n",
    "b_m = mse_avg(var_1_I, var_1_L)\n",
    "\n",
    "c_f = fid_avg(var_2_I, var_2_L, data_pts)\n",
    "c_m = mse_avg(var_2_I, var_2_L)\n",
    "\n",
    "d_f = fid_avg(var_3_I, var_3_L, data_pts)\n",
    "d_m = mse_avg(var_3_I, var_3_L)\n",
    "\n",
    "e_f = fid_avg(var_4_I, var_4_L, data_pts)\n",
    "e_m = mse_avg(var_4_I, var_4_L)\n",
    "\n",
    "print(a_f, b_f, c_f, d_f, e_f)\n",
    "print(a_m, b_m, c_m, d_m, e_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084b8a4",
   "metadata": {},
   "source": [
    "#N = 10\n",
    "\n",
    "\n",
    "#N = 100    -> 10,000 copies     \n",
    "Infidelity: 0.96591918971348 0.7968009097585905 0.7439562079331457 0.722995050879582 0.7124064729925206     \n",
    "MSE: 0.009683743778738967 0.03404544584964945 0.043532250446862875 0.04781267772271946 0.050139277949469\n",
    "\n",
    "#N = 1000   -> 10,000 copies    \n",
    "Infidelity: 0.9912356387433503 0.805200699096597 0.7395621299442228 0.7147399115680038 0.7018975208374534      \n",
    "MSE: 0.0009700103463936643 0.025321809009177923 0.03520112095728505 0.03937028087760285 0.04150426725347966\n",
    "\n",
    "#N = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f9e89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c806fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets\n",
    "#200 DMs for pure and mixed each\n",
    "#perform 200 noisy meaurements on each with var=pi/6\n",
    "\n",
    "M_dataset = var_0_I #1000x64\n",
    "M_label = var_0_L #1000x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38c65ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_ds = M_dataset.reshape(data_pts,1, 8, 8).real\n",
    "M_l = M_label.reshape(data_pts, 64).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d068a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_len = data_pts\n",
    "test_size = 0.1\n",
    "indices = list(range(ds_len))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(test_size * ds_len))\n",
    "train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "M_testset = M_ds[test_idx]\n",
    "M_testlabel = M_l[test_idx]\n",
    "M_trainset = M_ds[train_idx]\n",
    "M_trainlabel = M_l[train_idx]\n",
    "\n",
    "train_len = int(ds_len*(1-test_size))\n",
    "valid_size = 0.2\n",
    "indices = list(range(train_len))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * train_len))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "M_validset = M_trainset[valid_idx]\n",
    "M_validlabel = M_trainlabel[valid_idx]\n",
    "M_trainset = M_trainset[train_idx]\n",
    "M_trainlabel = M_trainlabel[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38a00342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "#mixed_train_set\n",
    "M_trainset = torch.Tensor(M_trainset) # transform to torch tensor\n",
    "M_trainlabel = torch.tensor(M_trainlabel)\n",
    "\n",
    "#mixed_valid set\n",
    "M_validset = torch.Tensor(M_validset) # transform to torch tensor\n",
    "M_validlabel = torch.tensor(M_validlabel)\n",
    "\n",
    "#mixed_test set\n",
    "M_testset = torch.Tensor(M_testset) # transform to torch tensor\n",
    "M_testlabel = torch.tensor(M_testlabel)\n",
    "\n",
    "#datasets\n",
    "train_data = TensorDataset(M_trainset, M_trainlabel)\n",
    "valid_data = TensorDataset(M_validset, M_validlabel)\n",
    "test_data = TensorDataset(M_testset, M_testlabel)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e2300ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer (sees 8x8x1 image tensor)\n",
    "        self.conv1 = nn.Conv2d(1, 24, 3, padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(24)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         convolutional layer (sees 4x4x12 tensor)\n",
    "        self.conv2 = nn.Conv2d(24, 128, 3, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "#         # convolutional layer (sees 2x2x64 tensor)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "#         self.conv3_bn = nn.BatchNorm2d(128)\n",
    "\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # linear layer (128 * 2 * 2 -> 450)\n",
    "        self.fc1 = nn.Linear(2048, 256)\n",
    "        # linear layer (256 -> 64)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        # dropout layer (p=0.5)\n",
    "        self.dropout = nn.Dropout(0.10)\n",
    "        self.m = nn.ELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv1_bn(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv2_bn(x)\n",
    "#         x = self.pool(x)\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = self.conv3_bn(x)\n",
    "\n",
    "        # flatten input\n",
    "        x = x.view(-1, 2048)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = self.m(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = self.m(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be525afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01de8aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (conv2): Conv2d(24, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (m): ELU(alpha=1.0)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.071678 \tValidation Loss: 0.037496\n",
      "Validation loss decreased (inf --> 0.037496).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.056981 \tValidation Loss: 0.032321\n",
      "Validation loss decreased (0.037496 --> 0.032321).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.049830 \tValidation Loss: 0.029110\n",
      "Validation loss decreased (0.032321 --> 0.029110).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.045263 \tValidation Loss: 0.026464\n",
      "Validation loss decreased (0.029110 --> 0.026464).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.042170 \tValidation Loss: 0.024623\n",
      "Validation loss decreased (0.026464 --> 0.024623).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.039629 \tValidation Loss: 0.023324\n",
      "Validation loss decreased (0.024623 --> 0.023324).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.037797 \tValidation Loss: 0.022326\n",
      "Validation loss decreased (0.023324 --> 0.022326).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.036198 \tValidation Loss: 0.021567\n",
      "Validation loss decreased (0.022326 --> 0.021567).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.035087 \tValidation Loss: 0.020835\n",
      "Validation loss decreased (0.021567 --> 0.020835).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.034031 \tValidation Loss: 0.020291\n",
      "Validation loss decreased (0.020835 --> 0.020291).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.033275 \tValidation Loss: 0.019813\n",
      "Validation loss decreased (0.020291 --> 0.019813).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.032300 \tValidation Loss: 0.019419\n",
      "Validation loss decreased (0.019813 --> 0.019419).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.031823 \tValidation Loss: 0.019100\n",
      "Validation loss decreased (0.019419 --> 0.019100).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.031185 \tValidation Loss: 0.018882\n",
      "Validation loss decreased (0.019100 --> 0.018882).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.030799 \tValidation Loss: 0.018604\n",
      "Validation loss decreased (0.018882 --> 0.018604).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.030310 \tValidation Loss: 0.018405\n",
      "Validation loss decreased (0.018604 --> 0.018405).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.029867 \tValidation Loss: 0.018231\n",
      "Validation loss decreased (0.018405 --> 0.018231).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.029420 \tValidation Loss: 0.018018\n",
      "Validation loss decreased (0.018231 --> 0.018018).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.029052 \tValidation Loss: 0.017906\n",
      "Validation loss decreased (0.018018 --> 0.017906).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.028809 \tValidation Loss: 0.017810\n",
      "Validation loss decreased (0.017906 --> 0.017810).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.028579 \tValidation Loss: 0.017689\n",
      "Validation loss decreased (0.017810 --> 0.017689).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.028214 \tValidation Loss: 0.017564\n",
      "Validation loss decreased (0.017689 --> 0.017564).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.028034 \tValidation Loss: 0.017476\n",
      "Validation loss decreased (0.017564 --> 0.017476).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.027814 \tValidation Loss: 0.017408\n",
      "Validation loss decreased (0.017476 --> 0.017408).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.027463 \tValidation Loss: 0.017334\n",
      "Validation loss decreased (0.017408 --> 0.017334).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.027216 \tValidation Loss: 0.017234\n",
      "Validation loss decreased (0.017334 --> 0.017234).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.026976 \tValidation Loss: 0.017180\n",
      "Validation loss decreased (0.017234 --> 0.017180).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.026845 \tValidation Loss: 0.017106\n",
      "Validation loss decreased (0.017180 --> 0.017106).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.026592 \tValidation Loss: 0.017053\n",
      "Validation loss decreased (0.017106 --> 0.017053).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.026424 \tValidation Loss: 0.017024\n",
      "Validation loss decreased (0.017053 --> 0.017024).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.026281 \tValidation Loss: 0.016942\n",
      "Validation loss decreased (0.017024 --> 0.016942).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.026111 \tValidation Loss: 0.016909\n",
      "Validation loss decreased (0.016942 --> 0.016909).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.025816 \tValidation Loss: 0.016875\n",
      "Validation loss decreased (0.016909 --> 0.016875).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.025691 \tValidation Loss: 0.016839\n",
      "Validation loss decreased (0.016875 --> 0.016839).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.025572 \tValidation Loss: 0.016792\n",
      "Validation loss decreased (0.016839 --> 0.016792).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.025362 \tValidation Loss: 0.016739\n",
      "Validation loss decreased (0.016792 --> 0.016739).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.025180 \tValidation Loss: 0.016727\n",
      "Validation loss decreased (0.016739 --> 0.016727).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.024973 \tValidation Loss: 0.016675\n",
      "Validation loss decreased (0.016727 --> 0.016675).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.024869 \tValidation Loss: 0.016644\n",
      "Validation loss decreased (0.016675 --> 0.016644).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.024729 \tValidation Loss: 0.016608\n",
      "Validation loss decreased (0.016644 --> 0.016608).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.024581 \tValidation Loss: 0.016584\n",
      "Validation loss decreased (0.016608 --> 0.016584).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.024417 \tValidation Loss: 0.016581\n",
      "Validation loss decreased (0.016584 --> 0.016581).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.024293 \tValidation Loss: 0.016518\n",
      "Validation loss decreased (0.016581 --> 0.016518).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.024161 \tValidation Loss: 0.016495\n",
      "Validation loss decreased (0.016518 --> 0.016495).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 0.023973 \tValidation Loss: 0.016483\n",
      "Validation loss decreased (0.016495 --> 0.016483).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.023915 \tValidation Loss: 0.016464\n",
      "Validation loss decreased (0.016483 --> 0.016464).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.023760 \tValidation Loss: 0.016420\n",
      "Validation loss decreased (0.016464 --> 0.016420).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.023584 \tValidation Loss: 0.016401\n",
      "Validation loss decreased (0.016420 --> 0.016401).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.023576 \tValidation Loss: 0.016373\n",
      "Validation loss decreased (0.016401 --> 0.016373).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.023404 \tValidation Loss: 0.016356\n",
      "Validation loss decreased (0.016373 --> 0.016356).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 0.023238 \tValidation Loss: 0.016325\n",
      "Validation loss decreased (0.016356 --> 0.016325).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.023173 \tValidation Loss: 0.016302\n",
      "Validation loss decreased (0.016325 --> 0.016302).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.023016 \tValidation Loss: 0.016284\n",
      "Validation loss decreased (0.016302 --> 0.016284).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.022914 \tValidation Loss: 0.016283\n",
      "Validation loss decreased (0.016284 --> 0.016283).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.022870 \tValidation Loss: 0.016243\n",
      "Validation loss decreased (0.016283 --> 0.016243).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 0.022709 \tValidation Loss: 0.016231\n",
      "Validation loss decreased (0.016243 --> 0.016231).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.022688 \tValidation Loss: 0.016226\n",
      "Validation loss decreased (0.016231 --> 0.016226).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 0.022452 \tValidation Loss: 0.016198\n",
      "Validation loss decreased (0.016226 --> 0.016198).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 \tTraining Loss: 0.022456 \tValidation Loss: 0.016180\n",
      "Validation loss decreased (0.016198 --> 0.016180).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.022298 \tValidation Loss: 0.016147\n",
      "Validation loss decreased (0.016180 --> 0.016147).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 0.022247 \tValidation Loss: 0.016155\n",
      "Epoch: 62 \tTraining Loss: 0.022066 \tValidation Loss: 0.016121\n",
      "Validation loss decreased (0.016147 --> 0.016121).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.022005 \tValidation Loss: 0.016085\n",
      "Validation loss decreased (0.016121 --> 0.016085).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.021926 \tValidation Loss: 0.016095\n",
      "Epoch: 65 \tTraining Loss: 0.021804 \tValidation Loss: 0.016067\n",
      "Validation loss decreased (0.016085 --> 0.016067).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 0.021765 \tValidation Loss: 0.016053\n",
      "Validation loss decreased (0.016067 --> 0.016053).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 0.021694 \tValidation Loss: 0.016025\n",
      "Validation loss decreased (0.016053 --> 0.016025).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.021551 \tValidation Loss: 0.016024\n",
      "Validation loss decreased (0.016025 --> 0.016024).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.021531 \tValidation Loss: 0.015982\n",
      "Validation loss decreased (0.016024 --> 0.015982).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.021390 \tValidation Loss: 0.015997\n",
      "Epoch: 71 \tTraining Loss: 0.021326 \tValidation Loss: 0.015973\n",
      "Validation loss decreased (0.015982 --> 0.015973).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 0.021239 \tValidation Loss: 0.015958\n",
      "Validation loss decreased (0.015973 --> 0.015958).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.021141 \tValidation Loss: 0.015948\n",
      "Validation loss decreased (0.015958 --> 0.015948).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.021115 \tValidation Loss: 0.015926\n",
      "Validation loss decreased (0.015948 --> 0.015926).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 0.020982 \tValidation Loss: 0.015907\n",
      "Validation loss decreased (0.015926 --> 0.015907).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 0.021001 \tValidation Loss: 0.015920\n",
      "Epoch: 77 \tTraining Loss: 0.020832 \tValidation Loss: 0.015885\n",
      "Validation loss decreased (0.015907 --> 0.015885).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 0.020763 \tValidation Loss: 0.015881\n",
      "Validation loss decreased (0.015885 --> 0.015881).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 0.020694 \tValidation Loss: 0.015889\n",
      "Epoch: 80 \tTraining Loss: 0.020640 \tValidation Loss: 0.015859\n",
      "Validation loss decreased (0.015881 --> 0.015859).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 0.020615 \tValidation Loss: 0.015861\n",
      "Epoch: 82 \tTraining Loss: 0.020525 \tValidation Loss: 0.015840\n",
      "Validation loss decreased (0.015859 --> 0.015840).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 0.020441 \tValidation Loss: 0.015830\n",
      "Validation loss decreased (0.015840 --> 0.015830).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 0.020397 \tValidation Loss: 0.015803\n",
      "Validation loss decreased (0.015830 --> 0.015803).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 0.020339 \tValidation Loss: 0.015811\n",
      "Epoch: 86 \tTraining Loss: 0.020319 \tValidation Loss: 0.015789\n",
      "Validation loss decreased (0.015803 --> 0.015789).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 0.020240 \tValidation Loss: 0.015800\n",
      "Epoch: 88 \tTraining Loss: 0.020208 \tValidation Loss: 0.015760\n",
      "Validation loss decreased (0.015789 --> 0.015760).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 0.020118 \tValidation Loss: 0.015763\n",
      "Epoch: 90 \tTraining Loss: 0.020097 \tValidation Loss: 0.015742\n",
      "Validation loss decreased (0.015760 --> 0.015742).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 0.019995 \tValidation Loss: 0.015742\n",
      "Validation loss decreased (0.015742 --> 0.015742).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 0.019934 \tValidation Loss: 0.015730\n",
      "Validation loss decreased (0.015742 --> 0.015730).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 0.019860 \tValidation Loss: 0.015716\n",
      "Validation loss decreased (0.015730 --> 0.015716).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 0.019860 \tValidation Loss: 0.015721\n",
      "Epoch: 95 \tTraining Loss: 0.019839 \tValidation Loss: 0.015699\n",
      "Validation loss decreased (0.015716 --> 0.015699).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.019737 \tValidation Loss: 0.015705\n",
      "Epoch: 97 \tTraining Loss: 0.019679 \tValidation Loss: 0.015691\n",
      "Validation loss decreased (0.015699 --> 0.015691).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 0.019661 \tValidation Loss: 0.015672\n",
      "Validation loss decreased (0.015691 --> 0.015672).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 0.019585 \tValidation Loss: 0.015662\n",
      "Validation loss decreased (0.015672 --> 0.015662).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 0.019570 \tValidation Loss: 0.015667\n",
      "Epoch: 101 \tTraining Loss: 0.019501 \tValidation Loss: 0.015660\n",
      "Validation loss decreased (0.015662 --> 0.015660).  Saving model ...\n",
      "Epoch: 102 \tTraining Loss: 0.019451 \tValidation Loss: 0.015648\n",
      "Validation loss decreased (0.015660 --> 0.015648).  Saving model ...\n",
      "Epoch: 103 \tTraining Loss: 0.019380 \tValidation Loss: 0.015649\n",
      "Epoch: 104 \tTraining Loss: 0.019383 \tValidation Loss: 0.015642\n",
      "Validation loss decreased (0.015648 --> 0.015642).  Saving model ...\n",
      "Epoch: 105 \tTraining Loss: 0.019347 \tValidation Loss: 0.015619\n",
      "Validation loss decreased (0.015642 --> 0.015619).  Saving model ...\n",
      "Epoch: 106 \tTraining Loss: 0.019284 \tValidation Loss: 0.015631\n",
      "Epoch: 107 \tTraining Loss: 0.019245 \tValidation Loss: 0.015606\n",
      "Validation loss decreased (0.015619 --> 0.015606).  Saving model ...\n",
      "Epoch: 108 \tTraining Loss: 0.019223 \tValidation Loss: 0.015601\n",
      "Validation loss decreased (0.015606 --> 0.015601).  Saving model ...\n",
      "Epoch: 109 \tTraining Loss: 0.019198 \tValidation Loss: 0.015595\n",
      "Validation loss decreased (0.015601 --> 0.015595).  Saving model ...\n",
      "Epoch: 110 \tTraining Loss: 0.019151 \tValidation Loss: 0.015599\n",
      "Epoch: 111 \tTraining Loss: 0.019100 \tValidation Loss: 0.015576\n",
      "Validation loss decreased (0.015595 --> 0.015576).  Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 0.019052 \tValidation Loss: 0.015588\n",
      "Epoch: 113 \tTraining Loss: 0.018955 \tValidation Loss: 0.015576\n",
      "Validation loss decreased (0.015576 --> 0.015576).  Saving model ...\n",
      "Epoch: 114 \tTraining Loss: 0.018974 \tValidation Loss: 0.015574\n",
      "Validation loss decreased (0.015576 --> 0.015574).  Saving model ...\n",
      "Epoch: 115 \tTraining Loss: 0.018926 \tValidation Loss: 0.015575\n",
      "Epoch: 116 \tTraining Loss: 0.018929 \tValidation Loss: 0.015554\n",
      "Validation loss decreased (0.015574 --> 0.015554).  Saving model ...\n",
      "Epoch: 117 \tTraining Loss: 0.018825 \tValidation Loss: 0.015542\n",
      "Validation loss decreased (0.015554 --> 0.015542).  Saving model ...\n",
      "Epoch: 118 \tTraining Loss: 0.018790 \tValidation Loss: 0.015535\n",
      "Validation loss decreased (0.015542 --> 0.015535).  Saving model ...\n",
      "Epoch: 119 \tTraining Loss: 0.018816 \tValidation Loss: 0.015552\n",
      "Epoch: 120 \tTraining Loss: 0.018753 \tValidation Loss: 0.015534\n",
      "Validation loss decreased (0.015535 --> 0.015534).  Saving model ...\n",
      "Epoch: 121 \tTraining Loss: 0.018741 \tValidation Loss: 0.015526\n",
      "Validation loss decreased (0.015534 --> 0.015526).  Saving model ...\n",
      "Epoch: 122 \tTraining Loss: 0.018689 \tValidation Loss: 0.015502\n",
      "Validation loss decreased (0.015526 --> 0.015502).  Saving model ...\n",
      "Epoch: 123 \tTraining Loss: 0.018643 \tValidation Loss: 0.015523\n",
      "Epoch: 124 \tTraining Loss: 0.018633 \tValidation Loss: 0.015514\n",
      "Epoch: 125 \tTraining Loss: 0.018571 \tValidation Loss: 0.015506\n",
      "Epoch: 126 \tTraining Loss: 0.018563 \tValidation Loss: 0.015496\n",
      "Validation loss decreased (0.015502 --> 0.015496).  Saving model ...\n",
      "Epoch: 127 \tTraining Loss: 0.018561 \tValidation Loss: 0.015479\n",
      "Validation loss decreased (0.015496 --> 0.015479).  Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 0.018481 \tValidation Loss: 0.015478\n",
      "Validation loss decreased (0.015479 --> 0.015478).  Saving model ...\n",
      "Epoch: 129 \tTraining Loss: 0.018435 \tValidation Loss: 0.015476\n",
      "Validation loss decreased (0.015478 --> 0.015476).  Saving model ...\n",
      "Epoch: 130 \tTraining Loss: 0.018416 \tValidation Loss: 0.015479\n",
      "Epoch: 131 \tTraining Loss: 0.018381 \tValidation Loss: 0.015475\n",
      "Validation loss decreased (0.015476 --> 0.015475).  Saving model ...\n",
      "Epoch: 132 \tTraining Loss: 0.018410 \tValidation Loss: 0.015470\n",
      "Validation loss decreased (0.015475 --> 0.015470).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 133 \tTraining Loss: 0.018381 \tValidation Loss: 0.015464\n",
      "Validation loss decreased (0.015470 --> 0.015464).  Saving model ...\n",
      "Epoch: 134 \tTraining Loss: 0.018287 \tValidation Loss: 0.015448\n",
      "Validation loss decreased (0.015464 --> 0.015448).  Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 0.018315 \tValidation Loss: 0.015454\n",
      "Epoch: 136 \tTraining Loss: 0.018255 \tValidation Loss: 0.015452\n",
      "Epoch: 137 \tTraining Loss: 0.018203 \tValidation Loss: 0.015447\n",
      "Validation loss decreased (0.015448 --> 0.015447).  Saving model ...\n",
      "Epoch: 138 \tTraining Loss: 0.018207 \tValidation Loss: 0.015446\n",
      "Validation loss decreased (0.015447 --> 0.015446).  Saving model ...\n",
      "Epoch: 139 \tTraining Loss: 0.018186 \tValidation Loss: 0.015435\n",
      "Validation loss decreased (0.015446 --> 0.015435).  Saving model ...\n",
      "Epoch: 140 \tTraining Loss: 0.018171 \tValidation Loss: 0.015438\n",
      "Epoch: 141 \tTraining Loss: 0.018175 \tValidation Loss: 0.015439\n",
      "Epoch: 142 \tTraining Loss: 0.018142 \tValidation Loss: 0.015424\n",
      "Validation loss decreased (0.015435 --> 0.015424).  Saving model ...\n",
      "Epoch: 143 \tTraining Loss: 0.018092 \tValidation Loss: 0.015424\n",
      "Epoch: 144 \tTraining Loss: 0.018072 \tValidation Loss: 0.015413\n",
      "Validation loss decreased (0.015424 --> 0.015413).  Saving model ...\n",
      "Epoch: 145 \tTraining Loss: 0.018082 \tValidation Loss: 0.015412\n",
      "Validation loss decreased (0.015413 --> 0.015412).  Saving model ...\n",
      "Epoch: 146 \tTraining Loss: 0.018012 \tValidation Loss: 0.015424\n",
      "Epoch: 147 \tTraining Loss: 0.017974 \tValidation Loss: 0.015401\n",
      "Validation loss decreased (0.015412 --> 0.015401).  Saving model ...\n",
      "Epoch: 148 \tTraining Loss: 0.017979 \tValidation Loss: 0.015401\n",
      "Validation loss decreased (0.015401 --> 0.015401).  Saving model ...\n",
      "Epoch: 149 \tTraining Loss: 0.017955 \tValidation Loss: 0.015394\n",
      "Validation loss decreased (0.015401 --> 0.015394).  Saving model ...\n",
      "Epoch: 150 \tTraining Loss: 0.017969 \tValidation Loss: 0.015402\n",
      "Epoch: 151 \tTraining Loss: 0.017910 \tValidation Loss: 0.015397\n",
      "Epoch: 152 \tTraining Loss: 0.017916 \tValidation Loss: 0.015388\n",
      "Validation loss decreased (0.015394 --> 0.015388).  Saving model ...\n",
      "Epoch: 153 \tTraining Loss: 0.017884 \tValidation Loss: 0.015385\n",
      "Validation loss decreased (0.015388 --> 0.015385).  Saving model ...\n",
      "Epoch: 154 \tTraining Loss: 0.017852 \tValidation Loss: 0.015385\n",
      "Epoch: 155 \tTraining Loss: 0.017834 \tValidation Loss: 0.015374\n",
      "Validation loss decreased (0.015385 --> 0.015374).  Saving model ...\n",
      "Epoch: 156 \tTraining Loss: 0.017798 \tValidation Loss: 0.015386\n",
      "Epoch: 157 \tTraining Loss: 0.017772 \tValidation Loss: 0.015378\n",
      "Epoch: 158 \tTraining Loss: 0.017816 \tValidation Loss: 0.015373\n",
      "Validation loss decreased (0.015374 --> 0.015373).  Saving model ...\n",
      "Epoch: 159 \tTraining Loss: 0.017745 \tValidation Loss: 0.015368\n",
      "Validation loss decreased (0.015373 --> 0.015368).  Saving model ...\n",
      "Epoch: 160 \tTraining Loss: 0.017731 \tValidation Loss: 0.015369\n",
      "Epoch: 161 \tTraining Loss: 0.017740 \tValidation Loss: 0.015367\n",
      "Validation loss decreased (0.015368 --> 0.015367).  Saving model ...\n",
      "Epoch: 162 \tTraining Loss: 0.017689 \tValidation Loss: 0.015361\n",
      "Validation loss decreased (0.015367 --> 0.015361).  Saving model ...\n",
      "Epoch: 163 \tTraining Loss: 0.017649 \tValidation Loss: 0.015353\n",
      "Validation loss decreased (0.015361 --> 0.015353).  Saving model ...\n",
      "Epoch: 164 \tTraining Loss: 0.017695 \tValidation Loss: 0.015371\n",
      "Epoch: 165 \tTraining Loss: 0.017642 \tValidation Loss: 0.015356\n",
      "Epoch: 166 \tTraining Loss: 0.017634 \tValidation Loss: 0.015354\n",
      "Epoch: 167 \tTraining Loss: 0.017615 \tValidation Loss: 0.015345\n",
      "Validation loss decreased (0.015353 --> 0.015345).  Saving model ...\n",
      "Epoch: 168 \tTraining Loss: 0.017571 \tValidation Loss: 0.015350\n",
      "Epoch: 169 \tTraining Loss: 0.017597 \tValidation Loss: 0.015342\n",
      "Validation loss decreased (0.015345 --> 0.015342).  Saving model ...\n",
      "Epoch: 170 \tTraining Loss: 0.017590 \tValidation Loss: 0.015340\n",
      "Validation loss decreased (0.015342 --> 0.015340).  Saving model ...\n",
      "Epoch: 171 \tTraining Loss: 0.017583 \tValidation Loss: 0.015340\n",
      "Epoch: 172 \tTraining Loss: 0.017551 \tValidation Loss: 0.015334\n",
      "Validation loss decreased (0.015340 --> 0.015334).  Saving model ...\n",
      "Epoch: 173 \tTraining Loss: 0.017512 \tValidation Loss: 0.015334\n",
      "Validation loss decreased (0.015334 --> 0.015334).  Saving model ...\n",
      "Epoch: 174 \tTraining Loss: 0.017490 \tValidation Loss: 0.015329\n",
      "Validation loss decreased (0.015334 --> 0.015329).  Saving model ...\n",
      "Epoch: 175 \tTraining Loss: 0.017450 \tValidation Loss: 0.015322\n",
      "Validation loss decreased (0.015329 --> 0.015322).  Saving model ...\n",
      "Epoch: 176 \tTraining Loss: 0.017461 \tValidation Loss: 0.015329\n",
      "Epoch: 177 \tTraining Loss: 0.017437 \tValidation Loss: 0.015318\n",
      "Validation loss decreased (0.015322 --> 0.015318).  Saving model ...\n",
      "Epoch: 178 \tTraining Loss: 0.017407 \tValidation Loss: 0.015325\n",
      "Epoch: 179 \tTraining Loss: 0.017433 \tValidation Loss: 0.015325\n",
      "Epoch: 180 \tTraining Loss: 0.017412 \tValidation Loss: 0.015310\n",
      "Validation loss decreased (0.015318 --> 0.015310).  Saving model ...\n",
      "Epoch: 181 \tTraining Loss: 0.017363 \tValidation Loss: 0.015314\n",
      "Epoch: 182 \tTraining Loss: 0.017397 \tValidation Loss: 0.015323\n",
      "Epoch: 183 \tTraining Loss: 0.017331 \tValidation Loss: 0.015320\n",
      "Epoch: 184 \tTraining Loss: 0.017331 \tValidation Loss: 0.015305\n",
      "Validation loss decreased (0.015310 --> 0.015305).  Saving model ...\n",
      "Epoch: 185 \tTraining Loss: 0.017336 \tValidation Loss: 0.015303\n",
      "Validation loss decreased (0.015305 --> 0.015303).  Saving model ...\n",
      "Epoch: 186 \tTraining Loss: 0.017309 \tValidation Loss: 0.015311\n",
      "Epoch: 187 \tTraining Loss: 0.017350 \tValidation Loss: 0.015293\n",
      "Validation loss decreased (0.015303 --> 0.015293).  Saving model ...\n",
      "Epoch: 188 \tTraining Loss: 0.017281 \tValidation Loss: 0.015284\n",
      "Validation loss decreased (0.015293 --> 0.015284).  Saving model ...\n",
      "Epoch: 189 \tTraining Loss: 0.017276 \tValidation Loss: 0.015297\n",
      "Epoch: 190 \tTraining Loss: 0.017270 \tValidation Loss: 0.015304\n",
      "Epoch: 191 \tTraining Loss: 0.017242 \tValidation Loss: 0.015293\n",
      "Epoch: 192 \tTraining Loss: 0.017257 \tValidation Loss: 0.015293\n",
      "Epoch: 193 \tTraining Loss: 0.017204 \tValidation Loss: 0.015295\n",
      "Epoch: 194 \tTraining Loss: 0.017209 \tValidation Loss: 0.015290\n",
      "Epoch: 195 \tTraining Loss: 0.017177 \tValidation Loss: 0.015286\n",
      "Epoch: 196 \tTraining Loss: 0.017204 \tValidation Loss: 0.015284\n",
      "Epoch: 197 \tTraining Loss: 0.017151 \tValidation Loss: 0.015283\n",
      "Validation loss decreased (0.015284 --> 0.015283).  Saving model ...\n",
      "Epoch: 198 \tTraining Loss: 0.017132 \tValidation Loss: 0.015270\n",
      "Validation loss decreased (0.015283 --> 0.015270).  Saving model ...\n",
      "Epoch: 199 \tTraining Loss: 0.017145 \tValidation Loss: 0.015285\n",
      "Epoch: 200 \tTraining Loss: 0.017127 \tValidation Loss: 0.015271\n",
      "Epoch: 201 \tTraining Loss: 0.017150 \tValidation Loss: 0.015273\n",
      "Epoch: 202 \tTraining Loss: 0.017076 \tValidation Loss: 0.015281\n",
      "Epoch: 203 \tTraining Loss: 0.017105 \tValidation Loss: 0.015275\n",
      "Epoch: 204 \tTraining Loss: 0.017057 \tValidation Loss: 0.015276\n",
      "Epoch: 205 \tTraining Loss: 0.017056 \tValidation Loss: 0.015266\n",
      "Validation loss decreased (0.015270 --> 0.015266).  Saving model ...\n",
      "Epoch: 206 \tTraining Loss: 0.017061 \tValidation Loss: 0.015276\n",
      "Epoch: 207 \tTraining Loss: 0.017083 \tValidation Loss: 0.015264\n",
      "Validation loss decreased (0.015266 --> 0.015264).  Saving model ...\n",
      "Epoch: 208 \tTraining Loss: 0.017047 \tValidation Loss: 0.015263\n",
      "Validation loss decreased (0.015264 --> 0.015263).  Saving model ...\n",
      "Epoch: 209 \tTraining Loss: 0.017060 \tValidation Loss: 0.015259\n",
      "Validation loss decreased (0.015263 --> 0.015259).  Saving model ...\n",
      "Epoch: 210 \tTraining Loss: 0.017036 \tValidation Loss: 0.015253\n",
      "Validation loss decreased (0.015259 --> 0.015253).  Saving model ...\n",
      "Epoch: 211 \tTraining Loss: 0.017016 \tValidation Loss: 0.015258\n",
      "Epoch: 212 \tTraining Loss: 0.017004 \tValidation Loss: 0.015257\n",
      "Epoch: 213 \tTraining Loss: 0.016984 \tValidation Loss: 0.015259\n",
      "Epoch: 214 \tTraining Loss: 0.016984 \tValidation Loss: 0.015245\n",
      "Validation loss decreased (0.015253 --> 0.015245).  Saving model ...\n",
      "Epoch: 215 \tTraining Loss: 0.016950 \tValidation Loss: 0.015245\n",
      "Epoch: 216 \tTraining Loss: 0.016946 \tValidation Loss: 0.015258\n",
      "Epoch: 217 \tTraining Loss: 0.016930 \tValidation Loss: 0.015254\n",
      "Epoch: 218 \tTraining Loss: 0.016911 \tValidation Loss: 0.015253\n",
      "Epoch: 219 \tTraining Loss: 0.016909 \tValidation Loss: 0.015252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 220 \tTraining Loss: 0.016915 \tValidation Loss: 0.015244\n",
      "Validation loss decreased (0.015245 --> 0.015244).  Saving model ...\n",
      "Epoch: 221 \tTraining Loss: 0.016906 \tValidation Loss: 0.015252\n",
      "Epoch: 222 \tTraining Loss: 0.016908 \tValidation Loss: 0.015244\n",
      "Epoch: 223 \tTraining Loss: 0.016877 \tValidation Loss: 0.015239\n",
      "Validation loss decreased (0.015244 --> 0.015239).  Saving model ...\n",
      "Epoch: 224 \tTraining Loss: 0.016897 \tValidation Loss: 0.015242\n",
      "Epoch: 225 \tTraining Loss: 0.016851 \tValidation Loss: 0.015229\n",
      "Validation loss decreased (0.015239 --> 0.015229).  Saving model ...\n",
      "Epoch: 226 \tTraining Loss: 0.016873 \tValidation Loss: 0.015234\n",
      "Epoch: 227 \tTraining Loss: 0.016826 \tValidation Loss: 0.015242\n",
      "Epoch: 228 \tTraining Loss: 0.016852 \tValidation Loss: 0.015230\n",
      "Epoch: 229 \tTraining Loss: 0.016847 \tValidation Loss: 0.015229\n",
      "Epoch: 230 \tTraining Loss: 0.016821 \tValidation Loss: 0.015239\n",
      "Epoch: 231 \tTraining Loss: 0.016822 \tValidation Loss: 0.015230\n",
      "Epoch: 232 \tTraining Loss: 0.016797 \tValidation Loss: 0.015225\n",
      "Validation loss decreased (0.015229 --> 0.015225).  Saving model ...\n",
      "Epoch: 233 \tTraining Loss: 0.016795 \tValidation Loss: 0.015234\n",
      "Epoch: 234 \tTraining Loss: 0.016811 \tValidation Loss: 0.015224\n",
      "Validation loss decreased (0.015225 --> 0.015224).  Saving model ...\n",
      "Epoch: 235 \tTraining Loss: 0.016797 \tValidation Loss: 0.015224\n",
      "Validation loss decreased (0.015224 --> 0.015224).  Saving model ...\n",
      "Epoch: 236 \tTraining Loss: 0.016763 \tValidation Loss: 0.015229\n",
      "Epoch: 237 \tTraining Loss: 0.016781 \tValidation Loss: 0.015217\n",
      "Validation loss decreased (0.015224 --> 0.015217).  Saving model ...\n",
      "Epoch: 238 \tTraining Loss: 0.016747 \tValidation Loss: 0.015227\n",
      "Epoch: 239 \tTraining Loss: 0.016723 \tValidation Loss: 0.015221\n",
      "Epoch: 240 \tTraining Loss: 0.016737 \tValidation Loss: 0.015218\n",
      "Epoch: 241 \tTraining Loss: 0.016736 \tValidation Loss: 0.015224\n",
      "Epoch: 242 \tTraining Loss: 0.016722 \tValidation Loss: 0.015214\n",
      "Validation loss decreased (0.015217 --> 0.015214).  Saving model ...\n",
      "Epoch: 243 \tTraining Loss: 0.016757 \tValidation Loss: 0.015226\n",
      "Epoch: 244 \tTraining Loss: 0.016715 \tValidation Loss: 0.015212\n",
      "Validation loss decreased (0.015214 --> 0.015212).  Saving model ...\n",
      "Epoch: 245 \tTraining Loss: 0.016702 \tValidation Loss: 0.015209\n",
      "Validation loss decreased (0.015212 --> 0.015209).  Saving model ...\n",
      "Epoch: 246 \tTraining Loss: 0.016710 \tValidation Loss: 0.015211\n",
      "Epoch: 247 \tTraining Loss: 0.016694 \tValidation Loss: 0.015220\n",
      "Epoch: 248 \tTraining Loss: 0.016672 \tValidation Loss: 0.015210\n",
      "Epoch: 249 \tTraining Loss: 0.016687 \tValidation Loss: 0.015207\n",
      "Validation loss decreased (0.015209 --> 0.015207).  Saving model ...\n",
      "Epoch: 250 \tTraining Loss: 0.016634 \tValidation Loss: 0.015201\n",
      "Validation loss decreased (0.015207 --> 0.015201).  Saving model ...\n",
      "Epoch: 251 \tTraining Loss: 0.016648 \tValidation Loss: 0.015211\n",
      "Epoch: 252 \tTraining Loss: 0.016666 \tValidation Loss: 0.015206\n",
      "Epoch: 253 \tTraining Loss: 0.016628 \tValidation Loss: 0.015203\n",
      "Epoch: 254 \tTraining Loss: 0.016658 \tValidation Loss: 0.015195\n",
      "Validation loss decreased (0.015201 --> 0.015195).  Saving model ...\n",
      "Epoch: 255 \tTraining Loss: 0.016628 \tValidation Loss: 0.015202\n",
      "Epoch: 256 \tTraining Loss: 0.016636 \tValidation Loss: 0.015206\n",
      "Epoch: 257 \tTraining Loss: 0.016590 \tValidation Loss: 0.015207\n",
      "Epoch: 258 \tTraining Loss: 0.016610 \tValidation Loss: 0.015215\n",
      "Epoch: 259 \tTraining Loss: 0.016599 \tValidation Loss: 0.015200\n",
      "Epoch: 260 \tTraining Loss: 0.016629 \tValidation Loss: 0.015198\n",
      "Epoch: 261 \tTraining Loss: 0.016605 \tValidation Loss: 0.015198\n",
      "Epoch: 262 \tTraining Loss: 0.016594 \tValidation Loss: 0.015196\n",
      "Epoch: 263 \tTraining Loss: 0.016586 \tValidation Loss: 0.015201\n",
      "Epoch: 264 \tTraining Loss: 0.016592 \tValidation Loss: 0.015201\n",
      "Epoch: 265 \tTraining Loss: 0.016578 \tValidation Loss: 0.015200\n",
      "Epoch: 266 \tTraining Loss: 0.016570 \tValidation Loss: 0.015192\n",
      "Validation loss decreased (0.015195 --> 0.015192).  Saving model ...\n",
      "Epoch: 267 \tTraining Loss: 0.016546 \tValidation Loss: 0.015199\n",
      "Epoch: 268 \tTraining Loss: 0.016542 \tValidation Loss: 0.015194\n",
      "Epoch: 269 \tTraining Loss: 0.016558 \tValidation Loss: 0.015200\n",
      "Epoch: 270 \tTraining Loss: 0.016517 \tValidation Loss: 0.015182\n",
      "Validation loss decreased (0.015192 --> 0.015182).  Saving model ...\n",
      "Epoch: 271 \tTraining Loss: 0.016508 \tValidation Loss: 0.015187\n",
      "Epoch: 272 \tTraining Loss: 0.016528 \tValidation Loss: 0.015199\n",
      "Epoch: 273 \tTraining Loss: 0.016508 \tValidation Loss: 0.015184\n",
      "Epoch: 274 \tTraining Loss: 0.016535 \tValidation Loss: 0.015192\n",
      "Epoch: 275 \tTraining Loss: 0.016503 \tValidation Loss: 0.015188\n",
      "Epoch: 276 \tTraining Loss: 0.016508 \tValidation Loss: 0.015186\n",
      "Epoch: 277 \tTraining Loss: 0.016490 \tValidation Loss: 0.015174\n",
      "Validation loss decreased (0.015182 --> 0.015174).  Saving model ...\n",
      "Epoch: 278 \tTraining Loss: 0.016460 \tValidation Loss: 0.015183\n",
      "Epoch: 279 \tTraining Loss: 0.016489 \tValidation Loss: 0.015171\n",
      "Validation loss decreased (0.015174 --> 0.015171).  Saving model ...\n",
      "Epoch: 280 \tTraining Loss: 0.016472 \tValidation Loss: 0.015182\n",
      "Epoch: 281 \tTraining Loss: 0.016475 \tValidation Loss: 0.015178\n",
      "Epoch: 282 \tTraining Loss: 0.016459 \tValidation Loss: 0.015177\n",
      "Epoch: 283 \tTraining Loss: 0.016452 \tValidation Loss: 0.015175\n",
      "Epoch: 284 \tTraining Loss: 0.016450 \tValidation Loss: 0.015174\n",
      "Epoch: 285 \tTraining Loss: 0.016444 \tValidation Loss: 0.015184\n",
      "Epoch: 286 \tTraining Loss: 0.016448 \tValidation Loss: 0.015182\n",
      "Epoch: 287 \tTraining Loss: 0.016441 \tValidation Loss: 0.015178\n",
      "Epoch: 288 \tTraining Loss: 0.016429 \tValidation Loss: 0.015178\n",
      "Epoch: 289 \tTraining Loss: 0.016442 \tValidation Loss: 0.015174\n",
      "Epoch: 290 \tTraining Loss: 0.016402 \tValidation Loss: 0.015171\n",
      "Epoch: 291 \tTraining Loss: 0.016422 \tValidation Loss: 0.015166\n",
      "Validation loss decreased (0.015171 --> 0.015166).  Saving model ...\n",
      "Epoch: 292 \tTraining Loss: 0.016437 \tValidation Loss: 0.015175\n",
      "Epoch: 293 \tTraining Loss: 0.016417 \tValidation Loss: 0.015169\n",
      "Epoch: 294 \tTraining Loss: 0.016415 \tValidation Loss: 0.015168\n",
      "Epoch: 295 \tTraining Loss: 0.016402 \tValidation Loss: 0.015174\n",
      "Epoch: 296 \tTraining Loss: 0.016387 \tValidation Loss: 0.015168\n",
      "Epoch: 297 \tTraining Loss: 0.016367 \tValidation Loss: 0.015161\n",
      "Validation loss decreased (0.015166 --> 0.015161).  Saving model ...\n",
      "Epoch: 298 \tTraining Loss: 0.016391 \tValidation Loss: 0.015168\n",
      "Epoch: 299 \tTraining Loss: 0.016362 \tValidation Loss: 0.015170\n",
      "Epoch: 300 \tTraining Loss: 0.016352 \tValidation Loss: 0.015161\n",
      "Epoch: 301 \tTraining Loss: 0.016387 \tValidation Loss: 0.015162\n",
      "Epoch: 302 \tTraining Loss: 0.016365 \tValidation Loss: 0.015171\n",
      "Epoch: 303 \tTraining Loss: 0.016374 \tValidation Loss: 0.015164\n",
      "Epoch: 304 \tTraining Loss: 0.016345 \tValidation Loss: 0.015159\n",
      "Validation loss decreased (0.015161 --> 0.015159).  Saving model ...\n",
      "Epoch: 305 \tTraining Loss: 0.016345 \tValidation Loss: 0.015168\n",
      "Epoch: 306 \tTraining Loss: 0.016358 \tValidation Loss: 0.015159\n",
      "Validation loss decreased (0.015159 --> 0.015159).  Saving model ...\n",
      "Epoch: 307 \tTraining Loss: 0.016355 \tValidation Loss: 0.015167\n",
      "Epoch: 308 \tTraining Loss: 0.016338 \tValidation Loss: 0.015162\n",
      "Epoch: 309 \tTraining Loss: 0.016334 \tValidation Loss: 0.015160\n",
      "Epoch: 310 \tTraining Loss: 0.016303 \tValidation Loss: 0.015170\n",
      "Epoch: 311 \tTraining Loss: 0.016327 \tValidation Loss: 0.015162\n",
      "Epoch: 312 \tTraining Loss: 0.016343 \tValidation Loss: 0.015159\n",
      "Epoch: 313 \tTraining Loss: 0.016297 \tValidation Loss: 0.015155\n",
      "Validation loss decreased (0.015159 --> 0.015155).  Saving model ...\n",
      "Epoch: 314 \tTraining Loss: 0.016302 \tValidation Loss: 0.015160\n",
      "Epoch: 315 \tTraining Loss: 0.016310 \tValidation Loss: 0.015165\n",
      "Epoch: 316 \tTraining Loss: 0.016317 \tValidation Loss: 0.015154\n",
      "Validation loss decreased (0.015155 --> 0.015154).  Saving model ...\n",
      "Epoch: 317 \tTraining Loss: 0.016276 \tValidation Loss: 0.015160\n",
      "Epoch: 318 \tTraining Loss: 0.016312 \tValidation Loss: 0.015150\n",
      "Validation loss decreased (0.015154 --> 0.015150).  Saving model ...\n",
      "Epoch: 319 \tTraining Loss: 0.016297 \tValidation Loss: 0.015151\n",
      "Epoch: 320 \tTraining Loss: 0.016305 \tValidation Loss: 0.015144\n",
      "Validation loss decreased (0.015150 --> 0.015144).  Saving model ...\n",
      "Epoch: 321 \tTraining Loss: 0.016291 \tValidation Loss: 0.015154\n",
      "Epoch: 322 \tTraining Loss: 0.016288 \tValidation Loss: 0.015157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 323 \tTraining Loss: 0.016265 \tValidation Loss: 0.015146\n",
      "Epoch: 324 \tTraining Loss: 0.016268 \tValidation Loss: 0.015145\n",
      "Epoch: 325 \tTraining Loss: 0.016244 \tValidation Loss: 0.015147\n",
      "Epoch: 326 \tTraining Loss: 0.016266 \tValidation Loss: 0.015152\n",
      "Epoch: 327 \tTraining Loss: 0.016248 \tValidation Loss: 0.015145\n",
      "Epoch: 328 \tTraining Loss: 0.016234 \tValidation Loss: 0.015150\n",
      "Epoch: 329 \tTraining Loss: 0.016268 \tValidation Loss: 0.015157\n",
      "Epoch: 330 \tTraining Loss: 0.016236 \tValidation Loss: 0.015152\n",
      "Epoch: 331 \tTraining Loss: 0.016234 \tValidation Loss: 0.015153\n",
      "Epoch: 332 \tTraining Loss: 0.016242 \tValidation Loss: 0.015154\n",
      "Epoch: 333 \tTraining Loss: 0.016229 \tValidation Loss: 0.015146\n",
      "Epoch: 334 \tTraining Loss: 0.016225 \tValidation Loss: 0.015144\n",
      "Epoch: 335 \tTraining Loss: 0.016245 \tValidation Loss: 0.015148\n",
      "Epoch: 336 \tTraining Loss: 0.016227 \tValidation Loss: 0.015140\n",
      "Validation loss decreased (0.015144 --> 0.015140).  Saving model ...\n",
      "Epoch: 337 \tTraining Loss: 0.016228 \tValidation Loss: 0.015151\n",
      "Epoch: 338 \tTraining Loss: 0.016232 \tValidation Loss: 0.015137\n",
      "Validation loss decreased (0.015140 --> 0.015137).  Saving model ...\n",
      "Epoch: 339 \tTraining Loss: 0.016215 \tValidation Loss: 0.015142\n",
      "Epoch: 340 \tTraining Loss: 0.016211 \tValidation Loss: 0.015146\n",
      "Epoch: 341 \tTraining Loss: 0.016208 \tValidation Loss: 0.015140\n",
      "Epoch: 342 \tTraining Loss: 0.016194 \tValidation Loss: 0.015143\n",
      "Epoch: 343 \tTraining Loss: 0.016197 \tValidation Loss: 0.015142\n",
      "Epoch: 344 \tTraining Loss: 0.016207 \tValidation Loss: 0.015136\n",
      "Validation loss decreased (0.015137 --> 0.015136).  Saving model ...\n",
      "Epoch: 345 \tTraining Loss: 0.016202 \tValidation Loss: 0.015140\n",
      "Epoch: 346 \tTraining Loss: 0.016173 \tValidation Loss: 0.015141\n",
      "Epoch: 347 \tTraining Loss: 0.016178 \tValidation Loss: 0.015142\n",
      "Epoch: 348 \tTraining Loss: 0.016191 \tValidation Loss: 0.015138\n",
      "Epoch: 349 \tTraining Loss: 0.016193 \tValidation Loss: 0.015136\n",
      "Validation loss decreased (0.015136 --> 0.015136).  Saving model ...\n",
      "Epoch: 350 \tTraining Loss: 0.016185 \tValidation Loss: 0.015137\n",
      "Epoch: 351 \tTraining Loss: 0.016186 \tValidation Loss: 0.015139\n",
      "Epoch: 352 \tTraining Loss: 0.016151 \tValidation Loss: 0.015136\n",
      "Validation loss decreased (0.015136 --> 0.015136).  Saving model ...\n",
      "Epoch: 353 \tTraining Loss: 0.016163 \tValidation Loss: 0.015129\n",
      "Validation loss decreased (0.015136 --> 0.015129).  Saving model ...\n",
      "Epoch: 354 \tTraining Loss: 0.016157 \tValidation Loss: 0.015127\n",
      "Validation loss decreased (0.015129 --> 0.015127).  Saving model ...\n",
      "Epoch: 355 \tTraining Loss: 0.016170 \tValidation Loss: 0.015144\n",
      "Epoch: 356 \tTraining Loss: 0.016139 \tValidation Loss: 0.015133\n",
      "Epoch: 357 \tTraining Loss: 0.016161 \tValidation Loss: 0.015119\n",
      "Validation loss decreased (0.015127 --> 0.015119).  Saving model ...\n",
      "Epoch: 358 \tTraining Loss: 0.016172 \tValidation Loss: 0.015142\n",
      "Epoch: 359 \tTraining Loss: 0.016139 \tValidation Loss: 0.015134\n",
      "Epoch: 360 \tTraining Loss: 0.016117 \tValidation Loss: 0.015131\n",
      "Epoch: 361 \tTraining Loss: 0.016145 \tValidation Loss: 0.015130\n",
      "Epoch: 362 \tTraining Loss: 0.016137 \tValidation Loss: 0.015130\n",
      "Epoch: 363 \tTraining Loss: 0.016169 \tValidation Loss: 0.015131\n",
      "Epoch: 364 \tTraining Loss: 0.016141 \tValidation Loss: 0.015124\n",
      "Epoch: 365 \tTraining Loss: 0.016145 \tValidation Loss: 0.015129\n",
      "Epoch: 366 \tTraining Loss: 0.016140 \tValidation Loss: 0.015129\n",
      "Epoch: 367 \tTraining Loss: 0.016111 \tValidation Loss: 0.015126\n",
      "Epoch: 368 \tTraining Loss: 0.016114 \tValidation Loss: 0.015135\n",
      "Epoch: 369 \tTraining Loss: 0.016090 \tValidation Loss: 0.015126\n",
      "Epoch: 370 \tTraining Loss: 0.016121 \tValidation Loss: 0.015127\n",
      "Epoch: 371 \tTraining Loss: 0.016126 \tValidation Loss: 0.015123\n",
      "Epoch: 372 \tTraining Loss: 0.016105 \tValidation Loss: 0.015126\n",
      "Epoch: 373 \tTraining Loss: 0.016100 \tValidation Loss: 0.015129\n",
      "Epoch: 374 \tTraining Loss: 0.016095 \tValidation Loss: 0.015130\n",
      "Epoch: 375 \tTraining Loss: 0.016088 \tValidation Loss: 0.015134\n",
      "Epoch: 376 \tTraining Loss: 0.016080 \tValidation Loss: 0.015117\n",
      "Validation loss decreased (0.015119 --> 0.015117).  Saving model ...\n",
      "Epoch: 377 \tTraining Loss: 0.016098 \tValidation Loss: 0.015130\n",
      "Epoch: 378 \tTraining Loss: 0.016050 \tValidation Loss: 0.015127\n",
      "Epoch: 379 \tTraining Loss: 0.016093 \tValidation Loss: 0.015125\n",
      "Epoch: 380 \tTraining Loss: 0.016063 \tValidation Loss: 0.015129\n",
      "Epoch: 381 \tTraining Loss: 0.016109 \tValidation Loss: 0.015121\n",
      "Epoch: 382 \tTraining Loss: 0.016075 \tValidation Loss: 0.015120\n",
      "Epoch: 383 \tTraining Loss: 0.016062 \tValidation Loss: 0.015123\n",
      "Epoch: 384 \tTraining Loss: 0.016057 \tValidation Loss: 0.015124\n",
      "Epoch: 385 \tTraining Loss: 0.016071 \tValidation Loss: 0.015127\n",
      "Epoch: 386 \tTraining Loss: 0.016059 \tValidation Loss: 0.015122\n",
      "Epoch: 387 \tTraining Loss: 0.016058 \tValidation Loss: 0.015120\n",
      "Epoch: 388 \tTraining Loss: 0.016091 \tValidation Loss: 0.015121\n",
      "Epoch: 389 \tTraining Loss: 0.016040 \tValidation Loss: 0.015112\n",
      "Validation loss decreased (0.015117 --> 0.015112).  Saving model ...\n",
      "Epoch: 390 \tTraining Loss: 0.016042 \tValidation Loss: 0.015118\n",
      "Epoch: 391 \tTraining Loss: 0.016063 \tValidation Loss: 0.015121\n",
      "Epoch: 392 \tTraining Loss: 0.016021 \tValidation Loss: 0.015119\n",
      "Epoch: 393 \tTraining Loss: 0.016039 \tValidation Loss: 0.015120\n",
      "Epoch: 394 \tTraining Loss: 0.016055 \tValidation Loss: 0.015116\n",
      "Epoch: 395 \tTraining Loss: 0.016048 \tValidation Loss: 0.015119\n",
      "Epoch: 396 \tTraining Loss: 0.016032 \tValidation Loss: 0.015116\n",
      "Epoch: 397 \tTraining Loss: 0.016014 \tValidation Loss: 0.015124\n",
      "Epoch: 398 \tTraining Loss: 0.016020 \tValidation Loss: 0.015117\n",
      "Epoch: 399 \tTraining Loss: 0.016021 \tValidation Loss: 0.015117\n",
      "Epoch: 400 \tTraining Loss: 0.016028 \tValidation Loss: 0.015113\n",
      "Epoch: 401 \tTraining Loss: 0.016028 \tValidation Loss: 0.015114\n",
      "Epoch: 402 \tTraining Loss: 0.016012 \tValidation Loss: 0.015112\n",
      "Validation loss decreased (0.015112 --> 0.015112).  Saving model ...\n",
      "Epoch: 403 \tTraining Loss: 0.016008 \tValidation Loss: 0.015116\n",
      "Epoch: 404 \tTraining Loss: 0.015993 \tValidation Loss: 0.015108\n",
      "Validation loss decreased (0.015112 --> 0.015108).  Saving model ...\n",
      "Epoch: 405 \tTraining Loss: 0.016011 \tValidation Loss: 0.015116\n",
      "Epoch: 406 \tTraining Loss: 0.016021 \tValidation Loss: 0.015110\n",
      "Epoch: 407 \tTraining Loss: 0.016019 \tValidation Loss: 0.015114\n",
      "Epoch: 408 \tTraining Loss: 0.015991 \tValidation Loss: 0.015108\n",
      "Validation loss decreased (0.015108 --> 0.015108).  Saving model ...\n",
      "Epoch: 409 \tTraining Loss: 0.015982 \tValidation Loss: 0.015120\n",
      "Epoch: 410 \tTraining Loss: 0.015993 \tValidation Loss: 0.015114\n",
      "Epoch: 411 \tTraining Loss: 0.015982 \tValidation Loss: 0.015115\n",
      "Epoch: 412 \tTraining Loss: 0.015992 \tValidation Loss: 0.015112\n",
      "Epoch: 413 \tTraining Loss: 0.016002 \tValidation Loss: 0.015109\n",
      "Epoch: 414 \tTraining Loss: 0.016016 \tValidation Loss: 0.015117\n",
      "Epoch: 415 \tTraining Loss: 0.015997 \tValidation Loss: 0.015108\n",
      "Epoch: 416 \tTraining Loss: 0.015972 \tValidation Loss: 0.015108\n",
      "Epoch: 417 \tTraining Loss: 0.015988 \tValidation Loss: 0.015109\n",
      "Epoch: 418 \tTraining Loss: 0.015997 \tValidation Loss: 0.015110\n",
      "Epoch: 419 \tTraining Loss: 0.015976 \tValidation Loss: 0.015111\n",
      "Epoch: 420 \tTraining Loss: 0.015993 \tValidation Loss: 0.015109\n",
      "Epoch: 421 \tTraining Loss: 0.015957 \tValidation Loss: 0.015110\n",
      "Epoch: 422 \tTraining Loss: 0.015957 \tValidation Loss: 0.015105\n",
      "Validation loss decreased (0.015108 --> 0.015105).  Saving model ...\n",
      "Epoch: 423 \tTraining Loss: 0.015969 \tValidation Loss: 0.015110\n",
      "Epoch: 424 \tTraining Loss: 0.015943 \tValidation Loss: 0.015107\n",
      "Epoch: 425 \tTraining Loss: 0.015972 \tValidation Loss: 0.015107\n",
      "Epoch: 426 \tTraining Loss: 0.015960 \tValidation Loss: 0.015107\n",
      "Epoch: 427 \tTraining Loss: 0.015977 \tValidation Loss: 0.015108\n",
      "Epoch: 428 \tTraining Loss: 0.015951 \tValidation Loss: 0.015105\n",
      "Epoch: 429 \tTraining Loss: 0.015952 \tValidation Loss: 0.015100\n",
      "Validation loss decreased (0.015105 --> 0.015100).  Saving model ...\n",
      "Epoch: 430 \tTraining Loss: 0.015964 \tValidation Loss: 0.015105\n",
      "Epoch: 431 \tTraining Loss: 0.015946 \tValidation Loss: 0.015102\n",
      "Epoch: 432 \tTraining Loss: 0.015962 \tValidation Loss: 0.015110\n",
      "Epoch: 433 \tTraining Loss: 0.015964 \tValidation Loss: 0.015109\n",
      "Epoch: 434 \tTraining Loss: 0.015937 \tValidation Loss: 0.015109\n",
      "Epoch: 435 \tTraining Loss: 0.015952 \tValidation Loss: 0.015101\n",
      "Epoch: 436 \tTraining Loss: 0.015911 \tValidation Loss: 0.015105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 437 \tTraining Loss: 0.015952 \tValidation Loss: 0.015097\n",
      "Validation loss decreased (0.015100 --> 0.015097).  Saving model ...\n",
      "Epoch: 438 \tTraining Loss: 0.015935 \tValidation Loss: 0.015104\n",
      "Epoch: 439 \tTraining Loss: 0.015906 \tValidation Loss: 0.015107\n",
      "Epoch: 440 \tTraining Loss: 0.015931 \tValidation Loss: 0.015109\n",
      "Epoch: 441 \tTraining Loss: 0.015916 \tValidation Loss: 0.015100\n",
      "Epoch: 442 \tTraining Loss: 0.015921 \tValidation Loss: 0.015111\n",
      "Epoch: 443 \tTraining Loss: 0.015919 \tValidation Loss: 0.015105\n",
      "Epoch: 444 \tTraining Loss: 0.015924 \tValidation Loss: 0.015101\n",
      "Epoch: 445 \tTraining Loss: 0.015880 \tValidation Loss: 0.015102\n",
      "Epoch: 446 \tTraining Loss: 0.015919 \tValidation Loss: 0.015102\n",
      "Epoch: 447 \tTraining Loss: 0.015922 \tValidation Loss: 0.015101\n",
      "Epoch: 448 \tTraining Loss: 0.015900 \tValidation Loss: 0.015094\n",
      "Validation loss decreased (0.015097 --> 0.015094).  Saving model ...\n",
      "Epoch: 449 \tTraining Loss: 0.015895 \tValidation Loss: 0.015097\n",
      "Epoch: 450 \tTraining Loss: 0.015907 \tValidation Loss: 0.015088\n",
      "Validation loss decreased (0.015094 --> 0.015088).  Saving model ...\n",
      "Epoch: 451 \tTraining Loss: 0.015868 \tValidation Loss: 0.015095\n",
      "Epoch: 452 \tTraining Loss: 0.015895 \tValidation Loss: 0.015099\n",
      "Epoch: 453 \tTraining Loss: 0.015883 \tValidation Loss: 0.015107\n",
      "Epoch: 454 \tTraining Loss: 0.015870 \tValidation Loss: 0.015096\n",
      "Epoch: 455 \tTraining Loss: 0.015912 \tValidation Loss: 0.015102\n",
      "Epoch: 456 \tTraining Loss: 0.015867 \tValidation Loss: 0.015091\n",
      "Epoch: 457 \tTraining Loss: 0.015891 \tValidation Loss: 0.015097\n",
      "Epoch: 458 \tTraining Loss: 0.015886 \tValidation Loss: 0.015097\n",
      "Epoch: 459 \tTraining Loss: 0.015866 \tValidation Loss: 0.015096\n",
      "Epoch: 460 \tTraining Loss: 0.015881 \tValidation Loss: 0.015098\n",
      "Epoch: 461 \tTraining Loss: 0.015861 \tValidation Loss: 0.015084\n",
      "Validation loss decreased (0.015088 --> 0.015084).  Saving model ...\n",
      "Epoch: 462 \tTraining Loss: 0.015867 \tValidation Loss: 0.015096\n",
      "Epoch: 463 \tTraining Loss: 0.015857 \tValidation Loss: 0.015098\n",
      "Epoch: 464 \tTraining Loss: 0.015858 \tValidation Loss: 0.015098\n",
      "Epoch: 465 \tTraining Loss: 0.015874 \tValidation Loss: 0.015106\n",
      "Epoch: 466 \tTraining Loss: 0.015865 \tValidation Loss: 0.015097\n",
      "Epoch: 467 \tTraining Loss: 0.015874 \tValidation Loss: 0.015092\n",
      "Epoch: 468 \tTraining Loss: 0.015877 \tValidation Loss: 0.015094\n",
      "Epoch: 469 \tTraining Loss: 0.015873 \tValidation Loss: 0.015094\n",
      "Epoch: 470 \tTraining Loss: 0.015850 \tValidation Loss: 0.015089\n",
      "Epoch: 471 \tTraining Loss: 0.015838 \tValidation Loss: 0.015096\n",
      "Epoch: 472 \tTraining Loss: 0.015852 \tValidation Loss: 0.015103\n",
      "Epoch: 473 \tTraining Loss: 0.015867 \tValidation Loss: 0.015090\n",
      "Epoch: 474 \tTraining Loss: 0.015870 \tValidation Loss: 0.015085\n",
      "Epoch: 475 \tTraining Loss: 0.015855 \tValidation Loss: 0.015098\n",
      "Epoch: 476 \tTraining Loss: 0.015846 \tValidation Loss: 0.015087\n",
      "Epoch: 477 \tTraining Loss: 0.015876 \tValidation Loss: 0.015089\n",
      "Epoch: 478 \tTraining Loss: 0.015854 \tValidation Loss: 0.015086\n",
      "Epoch: 479 \tTraining Loss: 0.015841 \tValidation Loss: 0.015094\n",
      "Epoch: 480 \tTraining Loss: 0.015832 \tValidation Loss: 0.015089\n",
      "Epoch: 481 \tTraining Loss: 0.015842 \tValidation Loss: 0.015086\n",
      "Epoch: 482 \tTraining Loss: 0.015827 \tValidation Loss: 0.015088\n",
      "Epoch: 483 \tTraining Loss: 0.015844 \tValidation Loss: 0.015087\n",
      "Epoch: 484 \tTraining Loss: 0.015845 \tValidation Loss: 0.015095\n",
      "Epoch: 485 \tTraining Loss: 0.015831 \tValidation Loss: 0.015096\n",
      "Epoch: 486 \tTraining Loss: 0.015844 \tValidation Loss: 0.015094\n",
      "Epoch: 487 \tTraining Loss: 0.015836 \tValidation Loss: 0.015085\n",
      "Epoch: 488 \tTraining Loss: 0.015845 \tValidation Loss: 0.015083\n",
      "Validation loss decreased (0.015084 --> 0.015083).  Saving model ...\n",
      "Epoch: 489 \tTraining Loss: 0.015822 \tValidation Loss: 0.015087\n",
      "Epoch: 490 \tTraining Loss: 0.015833 \tValidation Loss: 0.015089\n",
      "Epoch: 491 \tTraining Loss: 0.015801 \tValidation Loss: 0.015089\n",
      "Epoch: 492 \tTraining Loss: 0.015838 \tValidation Loss: 0.015083\n",
      "Validation loss decreased (0.015083 --> 0.015083).  Saving model ...\n",
      "Epoch: 493 \tTraining Loss: 0.015809 \tValidation Loss: 0.015080\n",
      "Validation loss decreased (0.015083 --> 0.015080).  Saving model ...\n",
      "Epoch: 494 \tTraining Loss: 0.015821 \tValidation Loss: 0.015090\n",
      "Epoch: 495 \tTraining Loss: 0.015818 \tValidation Loss: 0.015079\n",
      "Validation loss decreased (0.015080 --> 0.015079).  Saving model ...\n",
      "Epoch: 496 \tTraining Loss: 0.015818 \tValidation Loss: 0.015088\n",
      "Epoch: 497 \tTraining Loss: 0.015794 \tValidation Loss: 0.015079\n",
      "Validation loss decreased (0.015079 --> 0.015079).  Saving model ...\n",
      "Epoch: 498 \tTraining Loss: 0.015809 \tValidation Loss: 0.015091\n",
      "Epoch: 499 \tTraining Loss: 0.015818 \tValidation Loss: 0.015086\n",
      "Epoch: 500 \tTraining Loss: 0.015799 \tValidation Loss: 0.015084\n",
      "Epoch: 501 \tTraining Loss: 0.015788 \tValidation Loss: 0.015094\n",
      "Epoch: 502 \tTraining Loss: 0.015814 \tValidation Loss: 0.015085\n",
      "Epoch: 503 \tTraining Loss: 0.015804 \tValidation Loss: 0.015085\n",
      "Epoch: 504 \tTraining Loss: 0.015793 \tValidation Loss: 0.015082\n",
      "Epoch: 505 \tTraining Loss: 0.015810 \tValidation Loss: 0.015089\n",
      "Epoch: 506 \tTraining Loss: 0.015804 \tValidation Loss: 0.015088\n",
      "Epoch: 507 \tTraining Loss: 0.015783 \tValidation Loss: 0.015081\n",
      "Epoch: 508 \tTraining Loss: 0.015789 \tValidation Loss: 0.015077\n",
      "Validation loss decreased (0.015079 --> 0.015077).  Saving model ...\n",
      "Epoch: 509 \tTraining Loss: 0.015793 \tValidation Loss: 0.015078\n",
      "Epoch: 510 \tTraining Loss: 0.015768 \tValidation Loss: 0.015084\n",
      "Epoch: 511 \tTraining Loss: 0.015796 \tValidation Loss: 0.015086\n",
      "Epoch: 512 \tTraining Loss: 0.015775 \tValidation Loss: 0.015089\n",
      "Epoch: 513 \tTraining Loss: 0.015789 \tValidation Loss: 0.015083\n",
      "Epoch: 514 \tTraining Loss: 0.015783 \tValidation Loss: 0.015077\n",
      "Validation loss decreased (0.015077 --> 0.015077).  Saving model ...\n",
      "Epoch: 515 \tTraining Loss: 0.015751 \tValidation Loss: 0.015082\n",
      "Epoch: 516 \tTraining Loss: 0.015792 \tValidation Loss: 0.015086\n",
      "Epoch: 517 \tTraining Loss: 0.015773 \tValidation Loss: 0.015081\n",
      "Epoch: 518 \tTraining Loss: 0.015790 \tValidation Loss: 0.015079\n",
      "Epoch: 519 \tTraining Loss: 0.015780 \tValidation Loss: 0.015083\n",
      "Epoch: 520 \tTraining Loss: 0.015748 \tValidation Loss: 0.015073\n",
      "Validation loss decreased (0.015077 --> 0.015073).  Saving model ...\n",
      "Epoch: 521 \tTraining Loss: 0.015771 \tValidation Loss: 0.015081\n",
      "Epoch: 522 \tTraining Loss: 0.015751 \tValidation Loss: 0.015081\n",
      "Epoch: 523 \tTraining Loss: 0.015776 \tValidation Loss: 0.015079\n",
      "Epoch: 524 \tTraining Loss: 0.015770 \tValidation Loss: 0.015081\n",
      "Epoch: 525 \tTraining Loss: 0.015770 \tValidation Loss: 0.015079\n",
      "Epoch: 526 \tTraining Loss: 0.015774 \tValidation Loss: 0.015081\n",
      "Epoch: 527 \tTraining Loss: 0.015760 \tValidation Loss: 0.015077\n",
      "Epoch: 528 \tTraining Loss: 0.015744 \tValidation Loss: 0.015083\n",
      "Epoch: 529 \tTraining Loss: 0.015759 \tValidation Loss: 0.015075\n",
      "Epoch: 530 \tTraining Loss: 0.015776 \tValidation Loss: 0.015076\n",
      "Epoch: 531 \tTraining Loss: 0.015730 \tValidation Loss: 0.015077\n",
      "Epoch: 532 \tTraining Loss: 0.015741 \tValidation Loss: 0.015072\n",
      "Validation loss decreased (0.015073 --> 0.015072).  Saving model ...\n",
      "Epoch: 533 \tTraining Loss: 0.015727 \tValidation Loss: 0.015081\n",
      "Epoch: 534 \tTraining Loss: 0.015736 \tValidation Loss: 0.015076\n",
      "Epoch: 535 \tTraining Loss: 0.015734 \tValidation Loss: 0.015082\n",
      "Epoch: 536 \tTraining Loss: 0.015745 \tValidation Loss: 0.015079\n",
      "Epoch: 537 \tTraining Loss: 0.015734 \tValidation Loss: 0.015078\n",
      "Epoch: 538 \tTraining Loss: 0.015752 \tValidation Loss: 0.015086\n",
      "Epoch: 539 \tTraining Loss: 0.015751 \tValidation Loss: 0.015078\n",
      "Epoch: 540 \tTraining Loss: 0.015753 \tValidation Loss: 0.015078\n",
      "Epoch: 541 \tTraining Loss: 0.015729 \tValidation Loss: 0.015080\n",
      "Epoch: 542 \tTraining Loss: 0.015738 \tValidation Loss: 0.015077\n",
      "Epoch: 543 \tTraining Loss: 0.015732 \tValidation Loss: 0.015073\n",
      "Epoch: 544 \tTraining Loss: 0.015735 \tValidation Loss: 0.015071\n",
      "Validation loss decreased (0.015072 --> 0.015071).  Saving model ...\n",
      "Epoch: 545 \tTraining Loss: 0.015736 \tValidation Loss: 0.015076\n",
      "Epoch: 546 \tTraining Loss: 0.015733 \tValidation Loss: 0.015070\n",
      "Validation loss decreased (0.015071 --> 0.015070).  Saving model ...\n",
      "Epoch: 547 \tTraining Loss: 0.015723 \tValidation Loss: 0.015077\n",
      "Epoch: 548 \tTraining Loss: 0.015743 \tValidation Loss: 0.015081\n",
      "Epoch: 549 \tTraining Loss: 0.015728 \tValidation Loss: 0.015070\n",
      "Validation loss decreased (0.015070 --> 0.015070).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 550 \tTraining Loss: 0.015715 \tValidation Loss: 0.015073\n",
      "Epoch: 551 \tTraining Loss: 0.015745 \tValidation Loss: 0.015079\n",
      "Epoch: 552 \tTraining Loss: 0.015711 \tValidation Loss: 0.015075\n",
      "Epoch: 553 \tTraining Loss: 0.015740 \tValidation Loss: 0.015071\n",
      "Epoch: 554 \tTraining Loss: 0.015723 \tValidation Loss: 0.015069\n",
      "Validation loss decreased (0.015070 --> 0.015069).  Saving model ...\n",
      "Epoch: 555 \tTraining Loss: 0.015731 \tValidation Loss: 0.015072\n",
      "Epoch: 556 \tTraining Loss: 0.015718 \tValidation Loss: 0.015070\n",
      "Epoch: 557 \tTraining Loss: 0.015709 \tValidation Loss: 0.015078\n",
      "Epoch: 558 \tTraining Loss: 0.015722 \tValidation Loss: 0.015074\n",
      "Epoch: 559 \tTraining Loss: 0.015732 \tValidation Loss: 0.015074\n",
      "Epoch: 560 \tTraining Loss: 0.015720 \tValidation Loss: 0.015072\n",
      "Epoch: 561 \tTraining Loss: 0.015694 \tValidation Loss: 0.015063\n",
      "Validation loss decreased (0.015069 --> 0.015063).  Saving model ...\n",
      "Epoch: 562 \tTraining Loss: 0.015699 \tValidation Loss: 0.015070\n",
      "Epoch: 563 \tTraining Loss: 0.015732 \tValidation Loss: 0.015070\n",
      "Epoch: 564 \tTraining Loss: 0.015701 \tValidation Loss: 0.015079\n",
      "Epoch: 565 \tTraining Loss: 0.015686 \tValidation Loss: 0.015067\n",
      "Epoch: 566 \tTraining Loss: 0.015704 \tValidation Loss: 0.015073\n",
      "Epoch: 567 \tTraining Loss: 0.015701 \tValidation Loss: 0.015072\n",
      "Epoch: 568 \tTraining Loss: 0.015701 \tValidation Loss: 0.015063\n",
      "Validation loss decreased (0.015063 --> 0.015063).  Saving model ...\n",
      "Epoch: 569 \tTraining Loss: 0.015679 \tValidation Loss: 0.015074\n",
      "Epoch: 570 \tTraining Loss: 0.015707 \tValidation Loss: 0.015072\n",
      "Epoch: 571 \tTraining Loss: 0.015709 \tValidation Loss: 0.015073\n",
      "Epoch: 572 \tTraining Loss: 0.015684 \tValidation Loss: 0.015069\n",
      "Epoch: 573 \tTraining Loss: 0.015671 \tValidation Loss: 0.015069\n",
      "Epoch: 574 \tTraining Loss: 0.015683 \tValidation Loss: 0.015059\n",
      "Validation loss decreased (0.015063 --> 0.015059).  Saving model ...\n",
      "Epoch: 575 \tTraining Loss: 0.015707 \tValidation Loss: 0.015069\n",
      "Epoch: 576 \tTraining Loss: 0.015677 \tValidation Loss: 0.015072\n",
      "Epoch: 577 \tTraining Loss: 0.015689 \tValidation Loss: 0.015067\n",
      "Epoch: 578 \tTraining Loss: 0.015667 \tValidation Loss: 0.015067\n",
      "Epoch: 579 \tTraining Loss: 0.015683 \tValidation Loss: 0.015081\n",
      "Epoch: 580 \tTraining Loss: 0.015715 \tValidation Loss: 0.015069\n",
      "Epoch: 581 \tTraining Loss: 0.015689 \tValidation Loss: 0.015061\n",
      "Epoch: 582 \tTraining Loss: 0.015681 \tValidation Loss: 0.015060\n",
      "Epoch: 583 \tTraining Loss: 0.015681 \tValidation Loss: 0.015066\n",
      "Epoch: 584 \tTraining Loss: 0.015661 \tValidation Loss: 0.015067\n",
      "Epoch: 585 \tTraining Loss: 0.015656 \tValidation Loss: 0.015078\n",
      "Epoch: 586 \tTraining Loss: 0.015676 \tValidation Loss: 0.015064\n",
      "Epoch: 587 \tTraining Loss: 0.015673 \tValidation Loss: 0.015067\n",
      "Epoch: 588 \tTraining Loss: 0.015674 \tValidation Loss: 0.015061\n",
      "Epoch: 589 \tTraining Loss: 0.015680 \tValidation Loss: 0.015060\n",
      "Epoch: 590 \tTraining Loss: 0.015665 \tValidation Loss: 0.015064\n",
      "Epoch: 591 \tTraining Loss: 0.015675 \tValidation Loss: 0.015066\n",
      "Epoch: 592 \tTraining Loss: 0.015678 \tValidation Loss: 0.015057\n",
      "Validation loss decreased (0.015059 --> 0.015057).  Saving model ...\n",
      "Epoch: 593 \tTraining Loss: 0.015670 \tValidation Loss: 0.015071\n",
      "Epoch: 594 \tTraining Loss: 0.015675 \tValidation Loss: 0.015064\n",
      "Epoch: 595 \tTraining Loss: 0.015666 \tValidation Loss: 0.015067\n",
      "Epoch: 596 \tTraining Loss: 0.015679 \tValidation Loss: 0.015073\n",
      "Epoch: 597 \tTraining Loss: 0.015635 \tValidation Loss: 0.015065\n",
      "Epoch: 598 \tTraining Loss: 0.015652 \tValidation Loss: 0.015071\n",
      "Epoch: 599 \tTraining Loss: 0.015657 \tValidation Loss: 0.015060\n",
      "Epoch: 600 \tTraining Loss: 0.015643 \tValidation Loss: 0.015062\n",
      "Epoch: 601 \tTraining Loss: 0.015639 \tValidation Loss: 0.015058\n",
      "Epoch: 602 \tTraining Loss: 0.015639 \tValidation Loss: 0.015070\n",
      "Epoch: 603 \tTraining Loss: 0.015653 \tValidation Loss: 0.015070\n",
      "Epoch: 604 \tTraining Loss: 0.015653 \tValidation Loss: 0.015057\n",
      "Epoch: 605 \tTraining Loss: 0.015650 \tValidation Loss: 0.015063\n",
      "Epoch: 606 \tTraining Loss: 0.015652 \tValidation Loss: 0.015059\n",
      "Epoch: 607 \tTraining Loss: 0.015656 \tValidation Loss: 0.015055\n",
      "Validation loss decreased (0.015057 --> 0.015055).  Saving model ...\n",
      "Epoch: 608 \tTraining Loss: 0.015646 \tValidation Loss: 0.015056\n",
      "Epoch: 609 \tTraining Loss: 0.015635 \tValidation Loss: 0.015061\n",
      "Epoch: 610 \tTraining Loss: 0.015635 \tValidation Loss: 0.015060\n",
      "Epoch: 611 \tTraining Loss: 0.015647 \tValidation Loss: 0.015055\n",
      "Validation loss decreased (0.015055 --> 0.015055).  Saving model ...\n",
      "Epoch: 612 \tTraining Loss: 0.015659 \tValidation Loss: 0.015058\n",
      "Epoch: 613 \tTraining Loss: 0.015652 \tValidation Loss: 0.015066\n",
      "Epoch: 614 \tTraining Loss: 0.015639 \tValidation Loss: 0.015064\n",
      "Epoch: 615 \tTraining Loss: 0.015647 \tValidation Loss: 0.015065\n",
      "Epoch: 616 \tTraining Loss: 0.015647 \tValidation Loss: 0.015066\n",
      "Epoch: 617 \tTraining Loss: 0.015628 \tValidation Loss: 0.015060\n",
      "Epoch: 618 \tTraining Loss: 0.015633 \tValidation Loss: 0.015058\n",
      "Epoch: 619 \tTraining Loss: 0.015613 \tValidation Loss: 0.015059\n",
      "Epoch: 620 \tTraining Loss: 0.015630 \tValidation Loss: 0.015059\n",
      "Epoch: 621 \tTraining Loss: 0.015616 \tValidation Loss: 0.015059\n",
      "Epoch: 622 \tTraining Loss: 0.015606 \tValidation Loss: 0.015065\n",
      "Epoch: 623 \tTraining Loss: 0.015638 \tValidation Loss: 0.015057\n",
      "Epoch: 624 \tTraining Loss: 0.015622 \tValidation Loss: 0.015067\n",
      "Epoch: 625 \tTraining Loss: 0.015621 \tValidation Loss: 0.015054\n",
      "Validation loss decreased (0.015055 --> 0.015054).  Saving model ...\n",
      "Epoch: 626 \tTraining Loss: 0.015604 \tValidation Loss: 0.015053\n",
      "Validation loss decreased (0.015054 --> 0.015053).  Saving model ...\n",
      "Epoch: 627 \tTraining Loss: 0.015610 \tValidation Loss: 0.015063\n",
      "Epoch: 628 \tTraining Loss: 0.015617 \tValidation Loss: 0.015060\n",
      "Epoch: 629 \tTraining Loss: 0.015646 \tValidation Loss: 0.015057\n",
      "Epoch: 630 \tTraining Loss: 0.015619 \tValidation Loss: 0.015054\n",
      "Epoch: 631 \tTraining Loss: 0.015646 \tValidation Loss: 0.015057\n",
      "Epoch: 632 \tTraining Loss: 0.015605 \tValidation Loss: 0.015050\n",
      "Validation loss decreased (0.015053 --> 0.015050).  Saving model ...\n",
      "Epoch: 633 \tTraining Loss: 0.015613 \tValidation Loss: 0.015061\n",
      "Epoch: 634 \tTraining Loss: 0.015626 \tValidation Loss: 0.015059\n",
      "Epoch: 635 \tTraining Loss: 0.015614 \tValidation Loss: 0.015055\n",
      "Epoch: 636 \tTraining Loss: 0.015606 \tValidation Loss: 0.015063\n",
      "Epoch: 637 \tTraining Loss: 0.015586 \tValidation Loss: 0.015058\n",
      "Epoch: 638 \tTraining Loss: 0.015600 \tValidation Loss: 0.015057\n",
      "Epoch: 639 \tTraining Loss: 0.015605 \tValidation Loss: 0.015055\n",
      "Epoch: 640 \tTraining Loss: 0.015582 \tValidation Loss: 0.015050\n",
      "Validation loss decreased (0.015050 --> 0.015050).  Saving model ...\n",
      "Epoch: 641 \tTraining Loss: 0.015604 \tValidation Loss: 0.015055\n",
      "Epoch: 642 \tTraining Loss: 0.015597 \tValidation Loss: 0.015058\n",
      "Epoch: 643 \tTraining Loss: 0.015595 \tValidation Loss: 0.015052\n",
      "Epoch: 644 \tTraining Loss: 0.015578 \tValidation Loss: 0.015057\n",
      "Epoch: 645 \tTraining Loss: 0.015585 \tValidation Loss: 0.015052\n",
      "Epoch: 646 \tTraining Loss: 0.015616 \tValidation Loss: 0.015053\n",
      "Epoch: 647 \tTraining Loss: 0.015604 \tValidation Loss: 0.015046\n",
      "Validation loss decreased (0.015050 --> 0.015046).  Saving model ...\n",
      "Epoch: 648 \tTraining Loss: 0.015610 \tValidation Loss: 0.015058\n",
      "Epoch: 649 \tTraining Loss: 0.015595 \tValidation Loss: 0.015051\n",
      "Epoch: 650 \tTraining Loss: 0.015609 \tValidation Loss: 0.015054\n",
      "Epoch: 651 \tTraining Loss: 0.015606 \tValidation Loss: 0.015056\n",
      "Epoch: 652 \tTraining Loss: 0.015575 \tValidation Loss: 0.015057\n",
      "Epoch: 653 \tTraining Loss: 0.015590 \tValidation Loss: 0.015051\n",
      "Epoch: 654 \tTraining Loss: 0.015603 \tValidation Loss: 0.015055\n",
      "Epoch: 655 \tTraining Loss: 0.015579 \tValidation Loss: 0.015051\n",
      "Epoch: 656 \tTraining Loss: 0.015592 \tValidation Loss: 0.015050\n",
      "Epoch: 657 \tTraining Loss: 0.015565 \tValidation Loss: 0.015054\n",
      "Epoch: 658 \tTraining Loss: 0.015595 \tValidation Loss: 0.015053\n",
      "Epoch: 659 \tTraining Loss: 0.015577 \tValidation Loss: 0.015050\n",
      "Epoch: 660 \tTraining Loss: 0.015592 \tValidation Loss: 0.015042\n",
      "Validation loss decreased (0.015046 --> 0.015042).  Saving model ...\n",
      "Epoch: 661 \tTraining Loss: 0.015584 \tValidation Loss: 0.015052\n",
      "Epoch: 662 \tTraining Loss: 0.015589 \tValidation Loss: 0.015050\n",
      "Epoch: 663 \tTraining Loss: 0.015589 \tValidation Loss: 0.015052\n",
      "Epoch: 664 \tTraining Loss: 0.015573 \tValidation Loss: 0.015058\n",
      "Epoch: 665 \tTraining Loss: 0.015581 \tValidation Loss: 0.015054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 666 \tTraining Loss: 0.015582 \tValidation Loss: 0.015054\n",
      "Epoch: 667 \tTraining Loss: 0.015588 \tValidation Loss: 0.015048\n",
      "Epoch: 668 \tTraining Loss: 0.015563 \tValidation Loss: 0.015046\n",
      "Epoch: 669 \tTraining Loss: 0.015577 \tValidation Loss: 0.015061\n",
      "Epoch: 670 \tTraining Loss: 0.015588 \tValidation Loss: 0.015046\n",
      "Epoch: 671 \tTraining Loss: 0.015581 \tValidation Loss: 0.015047\n",
      "Epoch: 672 \tTraining Loss: 0.015555 \tValidation Loss: 0.015049\n",
      "Epoch: 673 \tTraining Loss: 0.015552 \tValidation Loss: 0.015043\n",
      "Epoch: 674 \tTraining Loss: 0.015574 \tValidation Loss: 0.015047\n",
      "Epoch: 675 \tTraining Loss: 0.015585 \tValidation Loss: 0.015053\n",
      "Epoch: 676 \tTraining Loss: 0.015573 \tValidation Loss: 0.015055\n",
      "Epoch: 677 \tTraining Loss: 0.015581 \tValidation Loss: 0.015051\n",
      "Epoch: 678 \tTraining Loss: 0.015567 \tValidation Loss: 0.015051\n",
      "Epoch: 679 \tTraining Loss: 0.015570 \tValidation Loss: 0.015046\n",
      "Epoch: 680 \tTraining Loss: 0.015545 \tValidation Loss: 0.015053\n",
      "Epoch: 681 \tTraining Loss: 0.015559 \tValidation Loss: 0.015049\n",
      "Epoch: 682 \tTraining Loss: 0.015568 \tValidation Loss: 0.015044\n",
      "Epoch: 683 \tTraining Loss: 0.015553 \tValidation Loss: 0.015045\n",
      "Epoch: 684 \tTraining Loss: 0.015554 \tValidation Loss: 0.015044\n",
      "Epoch: 685 \tTraining Loss: 0.015542 \tValidation Loss: 0.015042\n",
      "Epoch: 686 \tTraining Loss: 0.015556 \tValidation Loss: 0.015040\n",
      "Validation loss decreased (0.015042 --> 0.015040).  Saving model ...\n",
      "Epoch: 687 \tTraining Loss: 0.015539 \tValidation Loss: 0.015051\n",
      "Epoch: 688 \tTraining Loss: 0.015562 \tValidation Loss: 0.015048\n",
      "Epoch: 689 \tTraining Loss: 0.015549 \tValidation Loss: 0.015045\n",
      "Epoch: 690 \tTraining Loss: 0.015557 \tValidation Loss: 0.015039\n",
      "Validation loss decreased (0.015040 --> 0.015039).  Saving model ...\n",
      "Epoch: 691 \tTraining Loss: 0.015557 \tValidation Loss: 0.015049\n",
      "Epoch: 692 \tTraining Loss: 0.015560 \tValidation Loss: 0.015046\n",
      "Epoch: 693 \tTraining Loss: 0.015547 \tValidation Loss: 0.015046\n",
      "Epoch: 694 \tTraining Loss: 0.015548 \tValidation Loss: 0.015045\n",
      "Epoch: 695 \tTraining Loss: 0.015552 \tValidation Loss: 0.015043\n",
      "Epoch: 696 \tTraining Loss: 0.015548 \tValidation Loss: 0.015043\n",
      "Epoch: 697 \tTraining Loss: 0.015531 \tValidation Loss: 0.015045\n",
      "Epoch: 698 \tTraining Loss: 0.015535 \tValidation Loss: 0.015040\n",
      "Epoch: 699 \tTraining Loss: 0.015562 \tValidation Loss: 0.015050\n",
      "Epoch: 700 \tTraining Loss: 0.015529 \tValidation Loss: 0.015044\n",
      "Epoch: 701 \tTraining Loss: 0.015561 \tValidation Loss: 0.015048\n",
      "Epoch: 702 \tTraining Loss: 0.015541 \tValidation Loss: 0.015037\n",
      "Validation loss decreased (0.015039 --> 0.015037).  Saving model ...\n",
      "Epoch: 703 \tTraining Loss: 0.015548 \tValidation Loss: 0.015045\n",
      "Epoch: 704 \tTraining Loss: 0.015538 \tValidation Loss: 0.015044\n",
      "Epoch: 705 \tTraining Loss: 0.015540 \tValidation Loss: 0.015039\n",
      "Epoch: 706 \tTraining Loss: 0.015540 \tValidation Loss: 0.015048\n",
      "Epoch: 707 \tTraining Loss: 0.015520 \tValidation Loss: 0.015053\n",
      "Epoch: 708 \tTraining Loss: 0.015534 \tValidation Loss: 0.015053\n",
      "Epoch: 709 \tTraining Loss: 0.015537 \tValidation Loss: 0.015044\n",
      "Epoch: 710 \tTraining Loss: 0.015552 \tValidation Loss: 0.015046\n",
      "Epoch: 711 \tTraining Loss: 0.015528 \tValidation Loss: 0.015037\n",
      "Validation loss decreased (0.015037 --> 0.015037).  Saving model ...\n",
      "Epoch: 712 \tTraining Loss: 0.015536 \tValidation Loss: 0.015044\n",
      "Epoch: 713 \tTraining Loss: 0.015529 \tValidation Loss: 0.015050\n",
      "Epoch: 714 \tTraining Loss: 0.015532 \tValidation Loss: 0.015046\n",
      "Epoch: 715 \tTraining Loss: 0.015535 \tValidation Loss: 0.015041\n",
      "Epoch: 716 \tTraining Loss: 0.015515 \tValidation Loss: 0.015042\n",
      "Epoch: 717 \tTraining Loss: 0.015528 \tValidation Loss: 0.015041\n",
      "Epoch: 718 \tTraining Loss: 0.015518 \tValidation Loss: 0.015045\n",
      "Epoch: 719 \tTraining Loss: 0.015514 \tValidation Loss: 0.015045\n",
      "Epoch: 720 \tTraining Loss: 0.015502 \tValidation Loss: 0.015036\n",
      "Validation loss decreased (0.015037 --> 0.015036).  Saving model ...\n",
      "Epoch: 721 \tTraining Loss: 0.015528 \tValidation Loss: 0.015054\n",
      "Epoch: 722 \tTraining Loss: 0.015515 \tValidation Loss: 0.015044\n",
      "Epoch: 723 \tTraining Loss: 0.015519 \tValidation Loss: 0.015037\n",
      "Epoch: 724 \tTraining Loss: 0.015531 \tValidation Loss: 0.015042\n",
      "Epoch: 725 \tTraining Loss: 0.015524 \tValidation Loss: 0.015049\n",
      "Epoch: 726 \tTraining Loss: 0.015498 \tValidation Loss: 0.015038\n",
      "Epoch: 727 \tTraining Loss: 0.015525 \tValidation Loss: 0.015038\n",
      "Epoch: 728 \tTraining Loss: 0.015508 \tValidation Loss: 0.015043\n",
      "Epoch: 729 \tTraining Loss: 0.015507 \tValidation Loss: 0.015037\n",
      "Epoch: 730 \tTraining Loss: 0.015494 \tValidation Loss: 0.015047\n",
      "Epoch: 731 \tTraining Loss: 0.015504 \tValidation Loss: 0.015041\n",
      "Epoch: 732 \tTraining Loss: 0.015518 \tValidation Loss: 0.015036\n",
      "Validation loss decreased (0.015036 --> 0.015036).  Saving model ...\n",
      "Epoch: 733 \tTraining Loss: 0.015516 \tValidation Loss: 0.015041\n",
      "Epoch: 734 \tTraining Loss: 0.015494 \tValidation Loss: 0.015051\n",
      "Epoch: 735 \tTraining Loss: 0.015517 \tValidation Loss: 0.015041\n",
      "Epoch: 736 \tTraining Loss: 0.015512 \tValidation Loss: 0.015042\n",
      "Epoch: 737 \tTraining Loss: 0.015508 \tValidation Loss: 0.015037\n",
      "Epoch: 738 \tTraining Loss: 0.015494 \tValidation Loss: 0.015047\n",
      "Epoch: 739 \tTraining Loss: 0.015503 \tValidation Loss: 0.015041\n",
      "Epoch: 740 \tTraining Loss: 0.015514 \tValidation Loss: 0.015032\n",
      "Validation loss decreased (0.015036 --> 0.015032).  Saving model ...\n",
      "Epoch: 741 \tTraining Loss: 0.015495 \tValidation Loss: 0.015050\n",
      "Epoch: 742 \tTraining Loss: 0.015505 \tValidation Loss: 0.015038\n",
      "Epoch: 743 \tTraining Loss: 0.015485 \tValidation Loss: 0.015035\n",
      "Epoch: 744 \tTraining Loss: 0.015503 \tValidation Loss: 0.015039\n",
      "Epoch: 745 \tTraining Loss: 0.015485 \tValidation Loss: 0.015041\n",
      "Epoch: 746 \tTraining Loss: 0.015490 \tValidation Loss: 0.015041\n",
      "Epoch: 747 \tTraining Loss: 0.015487 \tValidation Loss: 0.015043\n",
      "Epoch: 748 \tTraining Loss: 0.015491 \tValidation Loss: 0.015046\n",
      "Epoch: 749 \tTraining Loss: 0.015525 \tValidation Loss: 0.015038\n",
      "Epoch: 750 \tTraining Loss: 0.015482 \tValidation Loss: 0.015038\n",
      "Epoch: 751 \tTraining Loss: 0.015498 \tValidation Loss: 0.015033\n",
      "Epoch: 752 \tTraining Loss: 0.015500 \tValidation Loss: 0.015033\n",
      "Epoch: 753 \tTraining Loss: 0.015500 \tValidation Loss: 0.015029\n",
      "Validation loss decreased (0.015032 --> 0.015029).  Saving model ...\n",
      "Epoch: 754 \tTraining Loss: 0.015492 \tValidation Loss: 0.015032\n",
      "Epoch: 755 \tTraining Loss: 0.015476 \tValidation Loss: 0.015029\n",
      "Epoch: 756 \tTraining Loss: 0.015485 \tValidation Loss: 0.015044\n",
      "Epoch: 757 \tTraining Loss: 0.015481 \tValidation Loss: 0.015033\n",
      "Epoch: 758 \tTraining Loss: 0.015466 \tValidation Loss: 0.015031\n",
      "Epoch: 759 \tTraining Loss: 0.015468 \tValidation Loss: 0.015034\n",
      "Epoch: 760 \tTraining Loss: 0.015507 \tValidation Loss: 0.015029\n",
      "Epoch: 761 \tTraining Loss: 0.015482 \tValidation Loss: 0.015040\n",
      "Epoch: 762 \tTraining Loss: 0.015471 \tValidation Loss: 0.015047\n",
      "Epoch: 763 \tTraining Loss: 0.015467 \tValidation Loss: 0.015040\n",
      "Epoch: 764 \tTraining Loss: 0.015472 \tValidation Loss: 0.015031\n",
      "Epoch: 765 \tTraining Loss: 0.015466 \tValidation Loss: 0.015042\n",
      "Epoch: 766 \tTraining Loss: 0.015471 \tValidation Loss: 0.015030\n",
      "Epoch: 767 \tTraining Loss: 0.015478 \tValidation Loss: 0.015038\n",
      "Epoch: 768 \tTraining Loss: 0.015481 \tValidation Loss: 0.015036\n",
      "Epoch: 769 \tTraining Loss: 0.015460 \tValidation Loss: 0.015032\n",
      "Epoch: 770 \tTraining Loss: 0.015474 \tValidation Loss: 0.015030\n",
      "Epoch: 771 \tTraining Loss: 0.015467 \tValidation Loss: 0.015036\n",
      "Epoch: 772 \tTraining Loss: 0.015465 \tValidation Loss: 0.015041\n",
      "Epoch: 773 \tTraining Loss: 0.015453 \tValidation Loss: 0.015030\n",
      "Epoch: 774 \tTraining Loss: 0.015479 \tValidation Loss: 0.015029\n",
      "Validation loss decreased (0.015029 --> 0.015029).  Saving model ...\n",
      "Epoch: 775 \tTraining Loss: 0.015471 \tValidation Loss: 0.015028\n",
      "Validation loss decreased (0.015029 --> 0.015028).  Saving model ...\n",
      "Epoch: 776 \tTraining Loss: 0.015456 \tValidation Loss: 0.015039\n",
      "Epoch: 777 \tTraining Loss: 0.015458 \tValidation Loss: 0.015033\n",
      "Epoch: 778 \tTraining Loss: 0.015469 \tValidation Loss: 0.015034\n",
      "Epoch: 779 \tTraining Loss: 0.015477 \tValidation Loss: 0.015049\n",
      "Epoch: 780 \tTraining Loss: 0.015466 \tValidation Loss: 0.015034\n",
      "Epoch: 781 \tTraining Loss: 0.015462 \tValidation Loss: 0.015029\n",
      "Epoch: 782 \tTraining Loss: 0.015455 \tValidation Loss: 0.015039\n",
      "Epoch: 783 \tTraining Loss: 0.015471 \tValidation Loss: 0.015035\n",
      "Epoch: 784 \tTraining Loss: 0.015465 \tValidation Loss: 0.015039\n",
      "Epoch: 785 \tTraining Loss: 0.015452 \tValidation Loss: 0.015035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 786 \tTraining Loss: 0.015477 \tValidation Loss: 0.015031\n",
      "Epoch: 787 \tTraining Loss: 0.015479 \tValidation Loss: 0.015030\n",
      "Epoch: 788 \tTraining Loss: 0.015452 \tValidation Loss: 0.015032\n",
      "Epoch: 789 \tTraining Loss: 0.015464 \tValidation Loss: 0.015029\n",
      "Epoch: 790 \tTraining Loss: 0.015467 \tValidation Loss: 0.015030\n",
      "Epoch: 791 \tTraining Loss: 0.015472 \tValidation Loss: 0.015034\n",
      "Epoch: 792 \tTraining Loss: 0.015459 \tValidation Loss: 0.015039\n",
      "Epoch: 793 \tTraining Loss: 0.015467 \tValidation Loss: 0.015032\n",
      "Epoch: 794 \tTraining Loss: 0.015447 \tValidation Loss: 0.015035\n",
      "Epoch: 795 \tTraining Loss: 0.015451 \tValidation Loss: 0.015032\n",
      "Epoch: 796 \tTraining Loss: 0.015466 \tValidation Loss: 0.015036\n",
      "Epoch: 797 \tTraining Loss: 0.015456 \tValidation Loss: 0.015029\n",
      "Epoch: 798 \tTraining Loss: 0.015442 \tValidation Loss: 0.015028\n",
      "Validation loss decreased (0.015028 --> 0.015028).  Saving model ...\n",
      "Epoch: 799 \tTraining Loss: 0.015452 \tValidation Loss: 0.015035\n",
      "Epoch: 800 \tTraining Loss: 0.015460 \tValidation Loss: 0.015032\n",
      "Epoch: 801 \tTraining Loss: 0.015451 \tValidation Loss: 0.015031\n",
      "Epoch: 802 \tTraining Loss: 0.015442 \tValidation Loss: 0.015029\n",
      "Epoch: 803 \tTraining Loss: 0.015465 \tValidation Loss: 0.015025\n",
      "Validation loss decreased (0.015028 --> 0.015025).  Saving model ...\n",
      "Epoch: 804 \tTraining Loss: 0.015434 \tValidation Loss: 0.015029\n",
      "Epoch: 805 \tTraining Loss: 0.015442 \tValidation Loss: 0.015024\n",
      "Validation loss decreased (0.015025 --> 0.015024).  Saving model ...\n",
      "Epoch: 806 \tTraining Loss: 0.015455 \tValidation Loss: 0.015029\n",
      "Epoch: 807 \tTraining Loss: 0.015444 \tValidation Loss: 0.015032\n",
      "Epoch: 808 \tTraining Loss: 0.015458 \tValidation Loss: 0.015030\n",
      "Epoch: 809 \tTraining Loss: 0.015430 \tValidation Loss: 0.015035\n",
      "Epoch: 810 \tTraining Loss: 0.015437 \tValidation Loss: 0.015027\n",
      "Epoch: 811 \tTraining Loss: 0.015424 \tValidation Loss: 0.015032\n",
      "Epoch: 812 \tTraining Loss: 0.015430 \tValidation Loss: 0.015027\n",
      "Epoch: 813 \tTraining Loss: 0.015454 \tValidation Loss: 0.015031\n",
      "Epoch: 814 \tTraining Loss: 0.015446 \tValidation Loss: 0.015034\n",
      "Epoch: 815 \tTraining Loss: 0.015452 \tValidation Loss: 0.015037\n",
      "Epoch: 816 \tTraining Loss: 0.015423 \tValidation Loss: 0.015031\n",
      "Epoch: 817 \tTraining Loss: 0.015426 \tValidation Loss: 0.015029\n",
      "Epoch: 818 \tTraining Loss: 0.015411 \tValidation Loss: 0.015025\n",
      "Epoch: 819 \tTraining Loss: 0.015431 \tValidation Loss: 0.015026\n",
      "Epoch: 820 \tTraining Loss: 0.015417 \tValidation Loss: 0.015032\n",
      "Epoch: 821 \tTraining Loss: 0.015434 \tValidation Loss: 0.015030\n",
      "Epoch: 822 \tTraining Loss: 0.015417 \tValidation Loss: 0.015032\n",
      "Epoch: 823 \tTraining Loss: 0.015436 \tValidation Loss: 0.015031\n",
      "Epoch: 824 \tTraining Loss: 0.015439 \tValidation Loss: 0.015026\n",
      "Epoch: 825 \tTraining Loss: 0.015423 \tValidation Loss: 0.015025\n",
      "Epoch: 826 \tTraining Loss: 0.015428 \tValidation Loss: 0.015024\n",
      "Epoch: 827 \tTraining Loss: 0.015422 \tValidation Loss: 0.015028\n",
      "Epoch: 828 \tTraining Loss: 0.015423 \tValidation Loss: 0.015027\n",
      "Epoch: 829 \tTraining Loss: 0.015423 \tValidation Loss: 0.015034\n",
      "Epoch: 830 \tTraining Loss: 0.015420 \tValidation Loss: 0.015031\n",
      "Epoch: 831 \tTraining Loss: 0.015429 \tValidation Loss: 0.015034\n",
      "Epoch: 832 \tTraining Loss: 0.015426 \tValidation Loss: 0.015026\n",
      "Epoch: 833 \tTraining Loss: 0.015403 \tValidation Loss: 0.015027\n",
      "Epoch: 834 \tTraining Loss: 0.015411 \tValidation Loss: 0.015022\n",
      "Validation loss decreased (0.015024 --> 0.015022).  Saving model ...\n",
      "Epoch: 835 \tTraining Loss: 0.015428 \tValidation Loss: 0.015029\n",
      "Epoch: 836 \tTraining Loss: 0.015431 \tValidation Loss: 0.015030\n",
      "Epoch: 837 \tTraining Loss: 0.015413 \tValidation Loss: 0.015030\n",
      "Epoch: 838 \tTraining Loss: 0.015402 \tValidation Loss: 0.015033\n",
      "Epoch: 839 \tTraining Loss: 0.015410 \tValidation Loss: 0.015021\n",
      "Validation loss decreased (0.015022 --> 0.015021).  Saving model ...\n",
      "Epoch: 840 \tTraining Loss: 0.015424 \tValidation Loss: 0.015026\n",
      "Epoch: 841 \tTraining Loss: 0.015427 \tValidation Loss: 0.015026\n",
      "Epoch: 842 \tTraining Loss: 0.015413 \tValidation Loss: 0.015027\n",
      "Epoch: 843 \tTraining Loss: 0.015421 \tValidation Loss: 0.015024\n",
      "Epoch: 844 \tTraining Loss: 0.015414 \tValidation Loss: 0.015023\n",
      "Epoch: 845 \tTraining Loss: 0.015400 \tValidation Loss: 0.015032\n",
      "Epoch: 846 \tTraining Loss: 0.015425 \tValidation Loss: 0.015023\n",
      "Epoch: 847 \tTraining Loss: 0.015386 \tValidation Loss: 0.015022\n",
      "Epoch: 848 \tTraining Loss: 0.015397 \tValidation Loss: 0.015022\n",
      "Epoch: 849 \tTraining Loss: 0.015416 \tValidation Loss: 0.015026\n",
      "Epoch: 850 \tTraining Loss: 0.015433 \tValidation Loss: 0.015025\n",
      "Epoch: 851 \tTraining Loss: 0.015401 \tValidation Loss: 0.015024\n",
      "Epoch: 852 \tTraining Loss: 0.015408 \tValidation Loss: 0.015019\n",
      "Validation loss decreased (0.015021 --> 0.015019).  Saving model ...\n",
      "Epoch: 853 \tTraining Loss: 0.015409 \tValidation Loss: 0.015021\n",
      "Epoch: 854 \tTraining Loss: 0.015406 \tValidation Loss: 0.015028\n",
      "Epoch: 855 \tTraining Loss: 0.015399 \tValidation Loss: 0.015025\n",
      "Epoch: 856 \tTraining Loss: 0.015406 \tValidation Loss: 0.015022\n",
      "Epoch: 857 \tTraining Loss: 0.015416 \tValidation Loss: 0.015024\n",
      "Epoch: 858 \tTraining Loss: 0.015386 \tValidation Loss: 0.015017\n",
      "Validation loss decreased (0.015019 --> 0.015017).  Saving model ...\n",
      "Epoch: 859 \tTraining Loss: 0.015413 \tValidation Loss: 0.015023\n",
      "Epoch: 860 \tTraining Loss: 0.015407 \tValidation Loss: 0.015021\n",
      "Epoch: 861 \tTraining Loss: 0.015404 \tValidation Loss: 0.015030\n",
      "Epoch: 862 \tTraining Loss: 0.015404 \tValidation Loss: 0.015026\n",
      "Epoch: 863 \tTraining Loss: 0.015406 \tValidation Loss: 0.015017\n",
      "Epoch: 864 \tTraining Loss: 0.015399 \tValidation Loss: 0.015022\n",
      "Epoch: 865 \tTraining Loss: 0.015392 \tValidation Loss: 0.015023\n",
      "Epoch: 866 \tTraining Loss: 0.015392 \tValidation Loss: 0.015022\n",
      "Epoch: 867 \tTraining Loss: 0.015393 \tValidation Loss: 0.015019\n",
      "Epoch: 868 \tTraining Loss: 0.015398 \tValidation Loss: 0.015026\n",
      "Epoch: 869 \tTraining Loss: 0.015388 \tValidation Loss: 0.015026\n",
      "Epoch: 870 \tTraining Loss: 0.015408 \tValidation Loss: 0.015020\n",
      "Epoch: 871 \tTraining Loss: 0.015389 \tValidation Loss: 0.015024\n",
      "Epoch: 872 \tTraining Loss: 0.015410 \tValidation Loss: 0.015023\n",
      "Epoch: 873 \tTraining Loss: 0.015407 \tValidation Loss: 0.015021\n",
      "Epoch: 874 \tTraining Loss: 0.015388 \tValidation Loss: 0.015022\n",
      "Epoch: 875 \tTraining Loss: 0.015408 \tValidation Loss: 0.015028\n",
      "Epoch: 876 \tTraining Loss: 0.015397 \tValidation Loss: 0.015024\n",
      "Epoch: 877 \tTraining Loss: 0.015394 \tValidation Loss: 0.015019\n",
      "Epoch: 878 \tTraining Loss: 0.015371 \tValidation Loss: 0.015023\n",
      "Epoch: 879 \tTraining Loss: 0.015386 \tValidation Loss: 0.015013\n",
      "Validation loss decreased (0.015017 --> 0.015013).  Saving model ...\n",
      "Epoch: 880 \tTraining Loss: 0.015398 \tValidation Loss: 0.015023\n",
      "Epoch: 881 \tTraining Loss: 0.015383 \tValidation Loss: 0.015017\n",
      "Epoch: 882 \tTraining Loss: 0.015378 \tValidation Loss: 0.015015\n",
      "Epoch: 883 \tTraining Loss: 0.015382 \tValidation Loss: 0.015020\n",
      "Epoch: 884 \tTraining Loss: 0.015389 \tValidation Loss: 0.015025\n",
      "Epoch: 885 \tTraining Loss: 0.015389 \tValidation Loss: 0.015024\n",
      "Epoch: 886 \tTraining Loss: 0.015388 \tValidation Loss: 0.015024\n",
      "Epoch: 887 \tTraining Loss: 0.015367 \tValidation Loss: 0.015020\n",
      "Epoch: 888 \tTraining Loss: 0.015373 \tValidation Loss: 0.015018\n",
      "Epoch: 889 \tTraining Loss: 0.015375 \tValidation Loss: 0.015023\n",
      "Epoch: 890 \tTraining Loss: 0.015357 \tValidation Loss: 0.015024\n",
      "Epoch: 891 \tTraining Loss: 0.015379 \tValidation Loss: 0.015017\n",
      "Epoch: 892 \tTraining Loss: 0.015378 \tValidation Loss: 0.015018\n",
      "Epoch: 893 \tTraining Loss: 0.015392 \tValidation Loss: 0.015020\n",
      "Epoch: 894 \tTraining Loss: 0.015383 \tValidation Loss: 0.015016\n",
      "Epoch: 895 \tTraining Loss: 0.015360 \tValidation Loss: 0.015016\n",
      "Epoch: 896 \tTraining Loss: 0.015366 \tValidation Loss: 0.015023\n",
      "Epoch: 897 \tTraining Loss: 0.015392 \tValidation Loss: 0.015019\n",
      "Epoch: 898 \tTraining Loss: 0.015387 \tValidation Loss: 0.015027\n",
      "Epoch: 899 \tTraining Loss: 0.015375 \tValidation Loss: 0.015023\n",
      "Epoch: 900 \tTraining Loss: 0.015359 \tValidation Loss: 0.015019\n",
      "Epoch: 901 \tTraining Loss: 0.015362 \tValidation Loss: 0.015017\n",
      "Epoch: 902 \tTraining Loss: 0.015381 \tValidation Loss: 0.015015\n",
      "Epoch: 903 \tTraining Loss: 0.015375 \tValidation Loss: 0.015010\n",
      "Validation loss decreased (0.015013 --> 0.015010).  Saving model ...\n",
      "Epoch: 904 \tTraining Loss: 0.015378 \tValidation Loss: 0.015022\n",
      "Epoch: 905 \tTraining Loss: 0.015382 \tValidation Loss: 0.015024\n",
      "Epoch: 906 \tTraining Loss: 0.015356 \tValidation Loss: 0.015025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 907 \tTraining Loss: 0.015375 \tValidation Loss: 0.015014\n",
      "Epoch: 908 \tTraining Loss: 0.015361 \tValidation Loss: 0.015023\n",
      "Epoch: 909 \tTraining Loss: 0.015375 \tValidation Loss: 0.015018\n",
      "Epoch: 910 \tTraining Loss: 0.015365 \tValidation Loss: 0.015013\n",
      "Epoch: 911 \tTraining Loss: 0.015384 \tValidation Loss: 0.015012\n",
      "Epoch: 912 \tTraining Loss: 0.015362 \tValidation Loss: 0.015012\n",
      "Epoch: 913 \tTraining Loss: 0.015359 \tValidation Loss: 0.015018\n",
      "Epoch: 914 \tTraining Loss: 0.015359 \tValidation Loss: 0.015025\n",
      "Epoch: 915 \tTraining Loss: 0.015363 \tValidation Loss: 0.015018\n",
      "Epoch: 916 \tTraining Loss: 0.015346 \tValidation Loss: 0.015018\n",
      "Epoch: 917 \tTraining Loss: 0.015375 \tValidation Loss: 0.015012\n",
      "Epoch: 918 \tTraining Loss: 0.015365 \tValidation Loss: 0.015015\n",
      "Epoch: 919 \tTraining Loss: 0.015357 \tValidation Loss: 0.015021\n",
      "Epoch: 920 \tTraining Loss: 0.015358 \tValidation Loss: 0.015014\n",
      "Epoch: 921 \tTraining Loss: 0.015347 \tValidation Loss: 0.015010\n",
      "Validation loss decreased (0.015010 --> 0.015010).  Saving model ...\n",
      "Epoch: 922 \tTraining Loss: 0.015355 \tValidation Loss: 0.015014\n",
      "Epoch: 923 \tTraining Loss: 0.015356 \tValidation Loss: 0.015018\n",
      "Epoch: 924 \tTraining Loss: 0.015354 \tValidation Loss: 0.015015\n",
      "Epoch: 925 \tTraining Loss: 0.015347 \tValidation Loss: 0.015020\n",
      "Epoch: 926 \tTraining Loss: 0.015362 \tValidation Loss: 0.015017\n",
      "Epoch: 927 \tTraining Loss: 0.015356 \tValidation Loss: 0.015020\n",
      "Epoch: 928 \tTraining Loss: 0.015369 \tValidation Loss: 0.015021\n",
      "Epoch: 929 \tTraining Loss: 0.015361 \tValidation Loss: 0.015016\n",
      "Epoch: 930 \tTraining Loss: 0.015376 \tValidation Loss: 0.015012\n",
      "Epoch: 931 \tTraining Loss: 0.015343 \tValidation Loss: 0.015017\n",
      "Epoch: 932 \tTraining Loss: 0.015367 \tValidation Loss: 0.015016\n",
      "Epoch: 933 \tTraining Loss: 0.015355 \tValidation Loss: 0.015019\n",
      "Epoch: 934 \tTraining Loss: 0.015343 \tValidation Loss: 0.015018\n",
      "Epoch: 935 \tTraining Loss: 0.015346 \tValidation Loss: 0.015018\n",
      "Epoch: 936 \tTraining Loss: 0.015370 \tValidation Loss: 0.015017\n",
      "Epoch: 937 \tTraining Loss: 0.015332 \tValidation Loss: 0.015015\n",
      "Epoch: 938 \tTraining Loss: 0.015347 \tValidation Loss: 0.015011\n",
      "Epoch: 939 \tTraining Loss: 0.015350 \tValidation Loss: 0.015017\n",
      "Epoch: 940 \tTraining Loss: 0.015333 \tValidation Loss: 0.015012\n",
      "Epoch: 941 \tTraining Loss: 0.015340 \tValidation Loss: 0.015014\n",
      "Epoch: 942 \tTraining Loss: 0.015351 \tValidation Loss: 0.015016\n",
      "Epoch: 943 \tTraining Loss: 0.015321 \tValidation Loss: 0.015021\n",
      "Epoch: 944 \tTraining Loss: 0.015335 \tValidation Loss: 0.015011\n",
      "Epoch: 945 \tTraining Loss: 0.015322 \tValidation Loss: 0.015013\n",
      "Epoch: 946 \tTraining Loss: 0.015337 \tValidation Loss: 0.015010\n",
      "Epoch: 947 \tTraining Loss: 0.015343 \tValidation Loss: 0.015005\n",
      "Validation loss decreased (0.015010 --> 0.015005).  Saving model ...\n",
      "Epoch: 948 \tTraining Loss: 0.015342 \tValidation Loss: 0.015013\n",
      "Epoch: 949 \tTraining Loss: 0.015343 \tValidation Loss: 0.015010\n",
      "Epoch: 950 \tTraining Loss: 0.015309 \tValidation Loss: 0.015008\n",
      "Epoch: 951 \tTraining Loss: 0.015334 \tValidation Loss: 0.015009\n",
      "Epoch: 952 \tTraining Loss: 0.015322 \tValidation Loss: 0.015016\n",
      "Epoch: 953 \tTraining Loss: 0.015324 \tValidation Loss: 0.015014\n",
      "Epoch: 954 \tTraining Loss: 0.015339 \tValidation Loss: 0.015006\n",
      "Epoch: 955 \tTraining Loss: 0.015345 \tValidation Loss: 0.015008\n",
      "Epoch: 956 \tTraining Loss: 0.015333 \tValidation Loss: 0.015016\n",
      "Epoch: 957 \tTraining Loss: 0.015328 \tValidation Loss: 0.015019\n",
      "Epoch: 958 \tTraining Loss: 0.015318 \tValidation Loss: 0.015014\n",
      "Epoch: 959 \tTraining Loss: 0.015347 \tValidation Loss: 0.015019\n",
      "Epoch: 960 \tTraining Loss: 0.015323 \tValidation Loss: 0.015014\n",
      "Epoch: 961 \tTraining Loss: 0.015356 \tValidation Loss: 0.015006\n",
      "Epoch: 962 \tTraining Loss: 0.015336 \tValidation Loss: 0.015008\n",
      "Epoch: 963 \tTraining Loss: 0.015327 \tValidation Loss: 0.015010\n",
      "Epoch: 964 \tTraining Loss: 0.015336 \tValidation Loss: 0.015013\n",
      "Epoch: 965 \tTraining Loss: 0.015324 \tValidation Loss: 0.015009\n",
      "Epoch: 966 \tTraining Loss: 0.015304 \tValidation Loss: 0.015014\n",
      "Epoch: 967 \tTraining Loss: 0.015320 \tValidation Loss: 0.015014\n",
      "Epoch: 968 \tTraining Loss: 0.015322 \tValidation Loss: 0.015009\n",
      "Epoch: 969 \tTraining Loss: 0.015340 \tValidation Loss: 0.015010\n",
      "Epoch: 970 \tTraining Loss: 0.015309 \tValidation Loss: 0.015014\n",
      "Epoch: 971 \tTraining Loss: 0.015326 \tValidation Loss: 0.015005\n",
      "Validation loss decreased (0.015005 --> 0.015005).  Saving model ...\n",
      "Epoch: 972 \tTraining Loss: 0.015321 \tValidation Loss: 0.015008\n",
      "Epoch: 973 \tTraining Loss: 0.015310 \tValidation Loss: 0.015019\n",
      "Epoch: 974 \tTraining Loss: 0.015313 \tValidation Loss: 0.015009\n",
      "Epoch: 975 \tTraining Loss: 0.015317 \tValidation Loss: 0.015007\n",
      "Epoch: 976 \tTraining Loss: 0.015288 \tValidation Loss: 0.015008\n",
      "Epoch: 977 \tTraining Loss: 0.015321 \tValidation Loss: 0.015010\n",
      "Epoch: 978 \tTraining Loss: 0.015324 \tValidation Loss: 0.015011\n",
      "Epoch: 979 \tTraining Loss: 0.015312 \tValidation Loss: 0.015009\n",
      "Epoch: 980 \tTraining Loss: 0.015338 \tValidation Loss: 0.015012\n",
      "Epoch: 981 \tTraining Loss: 0.015306 \tValidation Loss: 0.015010\n",
      "Epoch: 982 \tTraining Loss: 0.015308 \tValidation Loss: 0.015014\n",
      "Epoch: 983 \tTraining Loss: 0.015300 \tValidation Loss: 0.015007\n",
      "Epoch: 984 \tTraining Loss: 0.015310 \tValidation Loss: 0.015011\n",
      "Epoch: 985 \tTraining Loss: 0.015323 \tValidation Loss: 0.015006\n",
      "Epoch: 986 \tTraining Loss: 0.015318 \tValidation Loss: 0.015010\n",
      "Epoch: 987 \tTraining Loss: 0.015330 \tValidation Loss: 0.015006\n",
      "Epoch: 988 \tTraining Loss: 0.015315 \tValidation Loss: 0.015011\n",
      "Epoch: 989 \tTraining Loss: 0.015302 \tValidation Loss: 0.015009\n",
      "Epoch: 990 \tTraining Loss: 0.015320 \tValidation Loss: 0.015010\n",
      "Epoch: 991 \tTraining Loss: 0.015306 \tValidation Loss: 0.015007\n",
      "Epoch: 992 \tTraining Loss: 0.015313 \tValidation Loss: 0.015017\n",
      "Epoch: 993 \tTraining Loss: 0.015308 \tValidation Loss: 0.015011\n",
      "Epoch: 994 \tTraining Loss: 0.015294 \tValidation Loss: 0.015017\n",
      "Epoch: 995 \tTraining Loss: 0.015311 \tValidation Loss: 0.015009\n",
      "Epoch: 996 \tTraining Loss: 0.015308 \tValidation Loss: 0.015003\n",
      "Validation loss decreased (0.015005 --> 0.015003).  Saving model ...\n",
      "Epoch: 997 \tTraining Loss: 0.015305 \tValidation Loss: 0.015008\n",
      "Epoch: 998 \tTraining Loss: 0.015299 \tValidation Loss: 0.015006\n",
      "Epoch: 999 \tTraining Loss: 0.015305 \tValidation Loss: 0.015001\n",
      "Validation loss decreased (0.015003 --> 0.015001).  Saving model ...\n",
      "Epoch: 1000 \tTraining Loss: 0.015293 \tValidation Loss: 0.015006\n"
     ]
    }
   ],
   "source": [
    "# create a complete CNN\n",
    "# trained pn v1\n",
    "\n",
    "print(model)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 1000\n",
    "# model = model.float()\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            data, target = data.float(), target.float()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    \n",
    "    # At completion of epoch\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'NN_QST_1_KICS_21.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb2fcdd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'V2')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log10(train_losses[10:2000]), label='Training loss')\n",
    "plt.plot(np.log10(valid_losses[10:2000]), label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.title('V2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03707bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('NN_QST_2_KICS_20.pt'))\n",
    "# model.load_state_dict(torch.load('NN_QST_2_KICS_21.pt'))\n",
    "model.load_state_dict(torch.load('NN_QST_1_KICS_21.pt'))\n",
    "# model.load_state_dict(torch.load('NN_QST_2_KICS_23.pt'))\n",
    "# model.load_state_dict(torch.load('NN_QST_2_KICS_24.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b284495f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.014748\n",
      "Loss per Batch: 0.015138\n",
      "Loss per Batch: 0.014997\n",
      "Loss per Batch: 0.014322\n",
      "Loss per Batch: 0.014914\n",
      "Loss per Batch: 0.015093\n",
      "Loss per Batch: 0.015291\n",
      "Loss per Batch: 0.014616\n",
      "Loss per Batch: 0.015485\n",
      "Loss per Batch: 0.014929\n",
      "Loss per Batch: 0.015050\n",
      "Loss per Batch: 0.014848\n",
      "Loss per Batch: 0.014976\n",
      "Loss per Batch: 0.014498\n",
      "Loss per Batch: 0.015107\n",
      "Loss per Batch: 0.014949\n",
      "Loss per Batch: 0.014872\n",
      "Loss per Batch: 0.014424\n",
      "Loss per Batch: 0.014384\n",
      "Loss per Batch: 0.014671\n",
      "Final Test Loss: 0.014866\n",
      "Average Fidelity: 0.866213\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v1\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(optrs, aa)\n",
    "                rho2 = state_recon_bloch_vec(optrs, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3fc413d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.015717\n",
      "Loss per Batch: 0.015187\n",
      "Loss per Batch: 0.015278\n",
      "Loss per Batch: 0.015642\n",
      "Loss per Batch: 0.014897\n",
      "Loss per Batch: 0.015022\n",
      "Loss per Batch: 0.015067\n",
      "Loss per Batch: 0.014960\n",
      "Loss per Batch: 0.015032\n",
      "Loss per Batch: 0.014797\n",
      "Loss per Batch: 0.015521\n",
      "Loss per Batch: 0.015701\n",
      "Loss per Batch: 0.015343\n",
      "Loss per Batch: 0.015215\n",
      "Loss per Batch: 0.016237\n",
      "Loss per Batch: 0.015471\n",
      "Loss per Batch: 0.015256\n",
      "Loss per Batch: 0.015356\n",
      "Loss per Batch: 0.015536\n",
      "Loss per Batch: 0.015525\n",
      "Final Test Loss: 0.015338\n",
      "Average Fidelity: 0.863413\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v2\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(optrs, aa)\n",
    "                rho2 = state_recon_bloch_vec(optrs, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "044b2e1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.015256\n",
      "Loss per Batch: 0.015978\n",
      "Loss per Batch: 0.015641\n",
      "Loss per Batch: 0.014936\n",
      "Loss per Batch: 0.015387\n",
      "Loss per Batch: 0.015763\n",
      "Loss per Batch: 0.015022\n",
      "Loss per Batch: 0.015456\n",
      "Loss per Batch: 0.015427\n",
      "Loss per Batch: 0.015973\n",
      "Loss per Batch: 0.015734\n",
      "Loss per Batch: 0.015075\n",
      "Loss per Batch: 0.015332\n",
      "Loss per Batch: 0.015182\n",
      "Loss per Batch: 0.015454\n",
      "Loss per Batch: 0.015416\n",
      "Loss per Batch: 0.015190\n",
      "Loss per Batch: 0.015688\n",
      "Loss per Batch: 0.014999\n",
      "Loss per Batch: 0.014994\n",
      "Final Test Loss: 0.015395\n",
      "Average Fidelity: 0.861959\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v3\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(optrs, aa)\n",
    "                rho2 = state_recon_bloch_vec(optrs, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87228f00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.015787\n",
      "Loss per Batch: 0.015503\n",
      "Loss per Batch: 0.016185\n",
      "Loss per Batch: 0.014855\n",
      "Loss per Batch: 0.015537\n",
      "Loss per Batch: 0.015609\n",
      "Loss per Batch: 0.015375\n",
      "Loss per Batch: 0.016347\n",
      "Loss per Batch: 0.015546\n",
      "Loss per Batch: 0.015253\n",
      "Loss per Batch: 0.015645\n",
      "Loss per Batch: 0.015597\n",
      "Loss per Batch: 0.015717\n",
      "Loss per Batch: 0.015743\n",
      "Loss per Batch: 0.015160\n",
      "Loss per Batch: 0.016151\n",
      "Loss per Batch: 0.015472\n",
      "Loss per Batch: 0.015429\n",
      "Loss per Batch: 0.015288\n",
      "Loss per Batch: 0.015440\n",
      "Final Test Loss: 0.015582\n",
      "Average Fidelity: 0.861372\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v4\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(optrs, aa)\n",
    "                rho2 = state_recon_bloch_vec(optrs, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "528a6fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.013896\n",
      "Loss per Batch: 0.014144\n",
      "Loss per Batch: 0.013346\n",
      "Loss per Batch: 0.013323\n",
      "Loss per Batch: 0.013234\n",
      "Loss per Batch: 0.013995\n",
      "Loss per Batch: 0.014145\n",
      "Loss per Batch: 0.014581\n",
      "Loss per Batch: 0.013522\n",
      "Loss per Batch: 0.014386\n",
      "Loss per Batch: 0.013901\n",
      "Loss per Batch: 0.013970\n",
      "Loss per Batch: 0.014480\n",
      "Loss per Batch: 0.014131\n",
      "Loss per Batch: 0.014090\n",
      "Loss per Batch: 0.013350\n",
      "Loss per Batch: 0.013228\n",
      "Loss per Batch: 0.013289\n",
      "Loss per Batch: 0.013803\n",
      "Loss per Batch: 0.014040\n",
      "Final Test Loss: 0.013843\n",
      "Average Fidelity: 0.873512\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v0\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(optrs, aa)\n",
    "                rho2 = state_recon_bloch_vec(optrs, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae66c9c",
   "metadata": {},
   "source": [
    "Trained on v1 for 1000 epochs with 1000 copies and 10,000 data points\n",
    "\n",
    "#v0       \n",
    "Final Test Loss: 0.013843     \n",
    "Average Fidelity: 0.873512\n",
    "\n",
    "#v1   \n",
    "Final Test Loss: 0.014866    \n",
    "Average Fidelity: 0.866213\n",
    "    \n",
    "#v2   \n",
    "Final Test Loss: 0.015338     \n",
    "Average Fidelity: 0.863413 \n",
    "    \n",
    "#v3   \n",
    "Final Test Loss: 0.015395     \n",
    "Average Fidelity: 0.861959\n",
    "    \n",
    "#v4   \n",
    "Final Test Loss: 0.015582     \n",
    "Average Fidelity: 0.861372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1bed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
