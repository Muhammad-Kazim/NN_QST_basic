{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cacf45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Quantum States\n",
    "\n",
    "import numpy as np\n",
    "import qutip as qt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pure_state():\n",
    "    eps = 1e-7\n",
    "    rand_ket = qt.rand_ket_haar(4)\n",
    "    rho_p = qt.ket2dm(qt.Qobj(rand_ket))\n",
    "    rho_p = (1-eps)*rho_p + (eps/4)*np.eye(4)\n",
    "    return np.array(rho_p)\n",
    "\n",
    "def mixed_state(d):\n",
    "    \n",
    "    \"\"\"d = dimension of matrix\n",
    "    \"\"\"\n",
    "    G = np.random.normal(0, 1, [d,d]) + 1j*np.random.normal(0, 1, [d,d])\n",
    "    G = np.matrix(G)\n",
    "    rho_m = (G*G.H)/np.trace(G*G.H)\n",
    "    return rho_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ea72639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate Noise via Variance\n",
    "\n",
    "def eig_val_corr(state):\n",
    "    \n",
    "    \"\"\" state = nxn dimensional state\n",
    "        Corrects state by truncating negative eigenvalues and reconstructs normailized positive\n",
    "        eigenvalued eigenvectos' states\n",
    "    \"\"\"\n",
    "    if np.sum(np.linalg.eig(state)[0].real < 0) > 0:\n",
    "        eig_val, eig_vec = np.linalg.eig(state)\n",
    "        eig_val = eig_val.real\n",
    "        eig_val[eig_val < 0] = 0\n",
    "        eig_val = eig_val/np.sum(eig_val)\n",
    "        \n",
    "        d = state.shape[0]\n",
    "        state = np.zeros([d,d], dtype = 'complex')\n",
    "        for ij in range(d):\n",
    "            state += eig_val[ij]*np.matmul(eig_vec[ij].reshape(d,1), np.matrix(eig_vec[ij].reshape(d,1)).H)\n",
    "            \n",
    "    return state\n",
    "            \n",
    "def noise_state(state, var):\n",
    "    \n",
    "    \"\"\" state = 4x4 quantum state\n",
    "        var = scalar value. High var = high noise in state to be measured.\n",
    "    \"\"\"\n",
    "    d = state.shape[0]\n",
    "    noisy_state_raw = state + np.random.normal(0, var, [d,d]) + 1j*np.random.normal(0, var, [d,d])\n",
    "    \n",
    "    if np.sum(np.linalg.eig(noisy_state_raw)[0].real < 0) > 0:\n",
    "        state = eig_val_corr(noisy_state_raw)\n",
    "    else:\n",
    "        state = noisy_state_raw\n",
    "            \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "6a7bf204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1be6fc04a00>]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checking relation between var and infidelity\n",
    "\n",
    "fid = np.zeros([11, 1000])\n",
    "k=0\n",
    "for ij in range(11):\n",
    "    var = 0.01*ij\n",
    "    for jk in range(1000):\n",
    "        rho = mixed_state(d=8)\n",
    "        rho_noisy = noise_state(rho, var)\n",
    "\n",
    "        fid[ij, jk] = qt.fidelity(qt.Qobj(rho), qt.Qobj(rho_noisy))\n",
    "        \n",
    "plt.plot(0.01*np.arange(11), np.mean(fid, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "e27ff179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-533-e73a570f515f>:52: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  temp_labels[i, j, k] = np.trace(np.matmul(optrs, rho))\n",
      "<ipython-input-533-e73a570f515f>:53: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  temp_inputs[i, j, k] = np.trace(np.matmul(optrs, rho_noisy))\n"
     ]
    }
   ],
   "source": [
    "data_pts = 10000\n",
    "d = 8\n",
    "var_vals = 5\n",
    "k = np.zeros(64)\n",
    "\n",
    "I = np.array([[1, 0],\n",
    "              [0, 1]], dtype = 'complex')\n",
    "X = np.array([[0, 1],\n",
    "              [1, 0]], dtype = 'complex')\n",
    "Y = np.array([[0, -1j],\n",
    "              [1j, 0]], dtype = 'complex')\n",
    "Z = np.array([[1, 0],\n",
    "              [0, -1]], dtype = 'complex')\n",
    "\n",
    "pauli = np.array([I, X, Y ,Z], dtype = 'complex')\n",
    "\n",
    "var_0_L = np.zeros([data_pts, 64])\n",
    "var_0_I = np.zeros([data_pts, 64])\n",
    "v0 = 0\n",
    "\n",
    "var_1_L = np.zeros([data_pts, 64])\n",
    "var_1_I = np.zeros([data_pts, 64])\n",
    "v1 = 0\n",
    "\n",
    "var_2_L = np.zeros([data_pts, 64])\n",
    "var_2_I = np.zeros([data_pts, 64])\n",
    "v2 = 0\n",
    "\n",
    "var_3_L = np.zeros([data_pts, 64])\n",
    "var_3_I = np.zeros([data_pts, 64])\n",
    "v3 = 0\n",
    "\n",
    "var_4_L = np.zeros([data_pts, 64])\n",
    "var_4_I = np.zeros([data_pts, 64])\n",
    "v4 = 0\n",
    "\n",
    "for ij in range(data_pts):\n",
    "    \n",
    "    rho = mixed_state(d)\n",
    "    \n",
    "    for jk in range(var_vals):\n",
    "        var = 0.01*jk\n",
    "        rho_noisy = noise_state(rho, var)\n",
    "        \n",
    "        temp_labels = np.zeros([4,4,4])\n",
    "        temp_inputs = np.zeros([4,4,4])\n",
    "        \n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                for k in range(4):\n",
    "                    optrs = np.kron(pauli[i], np.kron(pauli[j], pauli[k]))\n",
    "                    temp_labels[i, j, k] = np.trace(np.matmul(optrs, rho))\n",
    "                    temp_inputs[i, j, k] = np.trace(np.matmul(optrs, rho_noisy))\n",
    "         \n",
    "        if jk == 0:\n",
    "            var_0_L[v0] = temp_labels.reshape(64)\n",
    "            var_0_I[v0] = temp_inputs.reshape(64)\n",
    "            v0 += 1\n",
    "        \n",
    "        if jk == 1:\n",
    "            var_1_L[v1] = temp_labels.reshape(64)\n",
    "            var_1_I[v1] = temp_inputs.reshape(64)\n",
    "            v1 += 1\n",
    "            \n",
    "        if jk == 2:\n",
    "            var_2_L[v2] = temp_labels.reshape(64)\n",
    "            var_2_I[v2] = temp_inputs.reshape(64)\n",
    "            v2 += 1\n",
    "        \n",
    "        if jk == 3:\n",
    "            var_3_L[v3] = temp_labels.reshape(64)\n",
    "            var_3_I[v3] = temp_inputs.reshape(64)\n",
    "            v3 += 1\n",
    "            \n",
    "        if jk == 4:\n",
    "            var_4_L[v4] = temp_labels.reshape(64)\n",
    "            var_4_I[v4] = temp_inputs.reshape(64)\n",
    "            v4 += 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "686c9209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999971 0.8133257710769483 0.7470423130329151 0.7132102512398958 0.698046986930631\n"
     ]
    }
   ],
   "source": [
    "def state_recon_bloch_vec(pauli, r):\n",
    "    \n",
    "    \"\"\"return state given pauli X, Y, Z and bloch vectors of size d**qbts\n",
    "    \"\"\"\n",
    "    var_0_L_a = r.reshape(4,4,4)\n",
    "    state_act = np.zeros([8,8], dtype = 'complex')\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(4):\n",
    "                optrs = np.kron(pauli[i], np.kron(pauli[j], pauli[k]))\n",
    "                state_act += var_0_L_a[i, j, k]*optrs\n",
    "    \n",
    "    return state_act/8\n",
    "                \n",
    "state_act = state_recon_bloch_vec(pauli, var_4_L[2])\n",
    "state_est = state_recon_bloch_vec(pauli, var_4_I[2])\n",
    "            \n",
    "qt.fidelity(qt.Qobj(state_act), qt.Qobj(state_est))\n",
    "\n",
    "def fid_avg(pauli, r_noise, r_actual, l):\n",
    "    \n",
    "    fidelity = 0.0\n",
    "    for ij in range(l):\n",
    "        a = state_recon_bloch_vec(pauli, r_noise[ij])\n",
    "        b = state_recon_bloch_vec(pauli, r_actual[ij])\n",
    "        fidelity += qt.fidelity(qt.Qobj(a), qt.Qobj(b))\n",
    "    return fidelity/l\n",
    "\n",
    "a = fid_avg(pauli, var_0_I, var_0_L, data_pts)\n",
    "b = fid_avg(pauli, var_1_I, var_1_L, data_pts)\n",
    "c = fid_avg(pauli, var_2_I, var_2_L, data_pts)\n",
    "d = fid_avg(pauli, var_3_I, var_3_L, data_pts)\n",
    "e = fid_avg(pauli, var_4_I, var_4_L, data_pts)\n",
    "\n",
    "print(a, b, c, d, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "31f9e89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "id": "c806fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets\n",
    "#200 DMs for pure and mixed each\n",
    "#perform 200 noisy meaurements on each with var=pi/6\n",
    "\n",
    "M_dataset = var_0_I #1000x64\n",
    "M_label = var_0_L #1000x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "38c65ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_ds = M_dataset.reshape(data_pts,1, 8, 8).real\n",
    "M_l = M_label.reshape(data_pts, 64).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "0d068a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_len = data_pts\n",
    "test_size = 0.1\n",
    "indices = list(range(ds_len))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(test_size * ds_len))\n",
    "train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "M_testset = M_ds[test_idx]\n",
    "M_testlabel = M_l[test_idx]\n",
    "M_trainset = M_ds[train_idx]\n",
    "M_trainlabel = M_l[train_idx]\n",
    "\n",
    "train_len = int(ds_len*(1-test_size))\n",
    "valid_size = 0.2\n",
    "indices = list(range(train_len))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * train_len))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "M_validset = M_trainset[valid_idx]\n",
    "M_validlabel = M_trainlabel[valid_idx]\n",
    "M_trainset = M_trainset[train_idx]\n",
    "M_trainlabel = M_trainlabel[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "38a00342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "#mixed_train_set\n",
    "M_trainset = torch.Tensor(M_trainset) # transform to torch tensor\n",
    "M_trainlabel = torch.tensor(M_trainlabel)\n",
    "\n",
    "#mixed_valid set\n",
    "M_validset = torch.Tensor(M_validset) # transform to torch tensor\n",
    "M_validlabel = torch.tensor(M_validlabel)\n",
    "\n",
    "#mixed_test set\n",
    "M_testset = torch.Tensor(M_testset) # transform to torch tensor\n",
    "M_testlabel = torch.tensor(M_testlabel)\n",
    "\n",
    "#datasets\n",
    "train_data = TensorDataset(M_trainset, M_trainlabel)\n",
    "valid_data = TensorDataset(M_validset, M_validlabel)\n",
    "test_data = TensorDataset(M_testset, M_testlabel)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "2e2300ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer (sees 8x8x1 image tensor)\n",
    "        self.conv1 = nn.Conv2d(1, 24, 3, padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(24)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         convolutional layer (sees 4x4x12 tensor)\n",
    "        self.conv2 = nn.Conv2d(24, 128, 3, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "#         # convolutional layer (sees 2x2x64 tensor)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "#         self.conv3_bn = nn.BatchNorm2d(128)\n",
    "\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # linear layer (128 * 2 * 2 -> 450)\n",
    "        self.fc1 = nn.Linear(2048, 256)\n",
    "        # linear layer (256 -> 64)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        # dropout layer (p=0.5)\n",
    "        self.dropout = nn.Dropout(0.10)\n",
    "        self.m = nn.ELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv1_bn(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv2_bn(x)\n",
    "#         x = self.pool(x)\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = self.conv3_bn(x)\n",
    "\n",
    "        # flatten input\n",
    "        x = x.view(-1, 2048)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = self.m(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = self.m(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "be525afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "01de8aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (conv2): Conv2d(24, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (m): ELU(alpha=1.0)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.044005 \tValidation Loss: 0.019841\n",
      "Validation loss decreased (inf --> 0.019841).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.030422 \tValidation Loss: 0.017511\n",
      "Validation loss decreased (0.019841 --> 0.017511).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.027382 \tValidation Loss: 0.016755\n",
      "Validation loss decreased (0.017511 --> 0.016755).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.025582 \tValidation Loss: 0.016442\n",
      "Validation loss decreased (0.016755 --> 0.016442).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.024027 \tValidation Loss: 0.016302\n",
      "Validation loss decreased (0.016442 --> 0.016302).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.023013 \tValidation Loss: 0.016159\n",
      "Validation loss decreased (0.016302 --> 0.016159).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.022072 \tValidation Loss: 0.015863\n",
      "Validation loss decreased (0.016159 --> 0.015863).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.021231 \tValidation Loss: 0.015869\n",
      "Epoch: 9 \tTraining Loss: 0.020635 \tValidation Loss: 0.015727\n",
      "Validation loss decreased (0.015863 --> 0.015727).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.020060 \tValidation Loss: 0.015647\n",
      "Validation loss decreased (0.015727 --> 0.015647).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.019617 \tValidation Loss: 0.015643\n",
      "Validation loss decreased (0.015647 --> 0.015643).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.019258 \tValidation Loss: 0.015602\n",
      "Validation loss decreased (0.015643 --> 0.015602).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.018955 \tValidation Loss: 0.015649\n",
      "Epoch: 14 \tTraining Loss: 0.018607 \tValidation Loss: 0.015519\n",
      "Validation loss decreased (0.015602 --> 0.015519).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.018343 \tValidation Loss: 0.015509\n",
      "Validation loss decreased (0.015519 --> 0.015509).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.018105 \tValidation Loss: 0.015395\n",
      "Validation loss decreased (0.015509 --> 0.015395).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.017968 \tValidation Loss: 0.015535\n",
      "Epoch: 18 \tTraining Loss: 0.017756 \tValidation Loss: 0.015407\n",
      "Epoch: 19 \tTraining Loss: 0.017635 \tValidation Loss: 0.015366\n",
      "Validation loss decreased (0.015395 --> 0.015366).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.017455 \tValidation Loss: 0.015461\n",
      "Epoch: 21 \tTraining Loss: 0.017363 \tValidation Loss: 0.015332\n",
      "Validation loss decreased (0.015366 --> 0.015332).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.017222 \tValidation Loss: 0.015341\n",
      "Epoch: 23 \tTraining Loss: 0.017189 \tValidation Loss: 0.015246\n",
      "Validation loss decreased (0.015332 --> 0.015246).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.017051 \tValidation Loss: 0.015281\n",
      "Epoch: 25 \tTraining Loss: 0.016962 \tValidation Loss: 0.015241\n",
      "Validation loss decreased (0.015246 --> 0.015241).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.016893 \tValidation Loss: 0.015314\n",
      "Epoch: 27 \tTraining Loss: 0.016797 \tValidation Loss: 0.015179\n",
      "Validation loss decreased (0.015241 --> 0.015179).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.016742 \tValidation Loss: 0.015221\n",
      "Epoch: 29 \tTraining Loss: 0.016695 \tValidation Loss: 0.015251\n",
      "Epoch: 30 \tTraining Loss: 0.016629 \tValidation Loss: 0.015239\n",
      "Epoch: 31 \tTraining Loss: 0.016604 \tValidation Loss: 0.015169\n",
      "Validation loss decreased (0.015179 --> 0.015169).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.016516 \tValidation Loss: 0.015259\n",
      "Epoch: 33 \tTraining Loss: 0.016472 \tValidation Loss: 0.015202\n",
      "Epoch: 34 \tTraining Loss: 0.016416 \tValidation Loss: 0.015141\n",
      "Validation loss decreased (0.015169 --> 0.015141).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.016382 \tValidation Loss: 0.015212\n",
      "Epoch: 36 \tTraining Loss: 0.016339 \tValidation Loss: 0.015146\n",
      "Epoch: 37 \tTraining Loss: 0.016306 \tValidation Loss: 0.015253\n",
      "Epoch: 38 \tTraining Loss: 0.016273 \tValidation Loss: 0.015088\n",
      "Validation loss decreased (0.015141 --> 0.015088).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.016221 \tValidation Loss: 0.015119\n",
      "Epoch: 40 \tTraining Loss: 0.016184 \tValidation Loss: 0.015091\n",
      "Epoch: 41 \tTraining Loss: 0.016150 \tValidation Loss: 0.015049\n",
      "Validation loss decreased (0.015088 --> 0.015049).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.016155 \tValidation Loss: 0.015018\n",
      "Validation loss decreased (0.015049 --> 0.015018).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.016111 \tValidation Loss: 0.015078\n",
      "Epoch: 44 \tTraining Loss: 0.016075 \tValidation Loss: 0.015108\n",
      "Epoch: 45 \tTraining Loss: 0.016048 \tValidation Loss: 0.015092\n",
      "Epoch: 46 \tTraining Loss: 0.015991 \tValidation Loss: 0.015033\n",
      "Epoch: 47 \tTraining Loss: 0.016001 \tValidation Loss: 0.015005\n",
      "Validation loss decreased (0.015018 --> 0.015005).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.015935 \tValidation Loss: 0.015118\n",
      "Epoch: 49 \tTraining Loss: 0.015940 \tValidation Loss: 0.014979\n",
      "Validation loss decreased (0.015005 --> 0.014979).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.015899 \tValidation Loss: 0.015069\n",
      "Epoch: 51 \tTraining Loss: 0.015895 \tValidation Loss: 0.015011\n",
      "Epoch: 52 \tTraining Loss: 0.015872 \tValidation Loss: 0.015024\n",
      "Epoch: 53 \tTraining Loss: 0.015861 \tValidation Loss: 0.014959\n",
      "Validation loss decreased (0.014979 --> 0.014959).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.015820 \tValidation Loss: 0.015053\n",
      "Epoch: 55 \tTraining Loss: 0.015810 \tValidation Loss: 0.015041\n",
      "Epoch: 56 \tTraining Loss: 0.015776 \tValidation Loss: 0.014945\n",
      "Validation loss decreased (0.014959 --> 0.014945).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.015754 \tValidation Loss: 0.015008\n",
      "Epoch: 58 \tTraining Loss: 0.015762 \tValidation Loss: 0.014945\n",
      "Validation loss decreased (0.014945 --> 0.014945).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.015730 \tValidation Loss: 0.014959\n",
      "Epoch: 60 \tTraining Loss: 0.015692 \tValidation Loss: 0.014921\n",
      "Validation loss decreased (0.014945 --> 0.014921).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 0.015729 \tValidation Loss: 0.014938\n",
      "Epoch: 62 \tTraining Loss: 0.015652 \tValidation Loss: 0.014958\n",
      "Epoch: 63 \tTraining Loss: 0.015674 \tValidation Loss: 0.014923\n",
      "Epoch: 64 \tTraining Loss: 0.015648 \tValidation Loss: 0.014920\n",
      "Validation loss decreased (0.014921 --> 0.014920).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.015622 \tValidation Loss: 0.015020\n",
      "Epoch: 66 \tTraining Loss: 0.015600 \tValidation Loss: 0.014976\n",
      "Epoch: 67 \tTraining Loss: 0.015607 \tValidation Loss: 0.014891\n",
      "Validation loss decreased (0.014920 --> 0.014891).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.015566 \tValidation Loss: 0.014896\n",
      "Epoch: 69 \tTraining Loss: 0.015565 \tValidation Loss: 0.014895\n",
      "Epoch: 70 \tTraining Loss: 0.015574 \tValidation Loss: 0.014864\n",
      "Validation loss decreased (0.014891 --> 0.014864).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 0.015552 \tValidation Loss: 0.014833\n",
      "Validation loss decreased (0.014864 --> 0.014833).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 0.015535 \tValidation Loss: 0.014871\n",
      "Epoch: 73 \tTraining Loss: 0.015513 \tValidation Loss: 0.014836\n",
      "Epoch: 74 \tTraining Loss: 0.015505 \tValidation Loss: 0.014852\n",
      "Epoch: 75 \tTraining Loss: 0.015507 \tValidation Loss: 0.014881\n",
      "Epoch: 76 \tTraining Loss: 0.015482 \tValidation Loss: 0.014919\n",
      "Epoch: 77 \tTraining Loss: 0.015491 \tValidation Loss: 0.014846\n",
      "Epoch: 78 \tTraining Loss: 0.015459 \tValidation Loss: 0.014838\n",
      "Epoch: 79 \tTraining Loss: 0.015439 \tValidation Loss: 0.014823\n",
      "Validation loss decreased (0.014833 --> 0.014823).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 0.015445 \tValidation Loss: 0.014797\n",
      "Validation loss decreased (0.014823 --> 0.014797).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 0.015436 \tValidation Loss: 0.014858\n",
      "Epoch: 82 \tTraining Loss: 0.015422 \tValidation Loss: 0.014809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 \tTraining Loss: 0.015399 \tValidation Loss: 0.014837\n",
      "Epoch: 84 \tTraining Loss: 0.015404 \tValidation Loss: 0.014819\n",
      "Epoch: 85 \tTraining Loss: 0.015370 \tValidation Loss: 0.014833\n",
      "Epoch: 86 \tTraining Loss: 0.015367 \tValidation Loss: 0.014804\n",
      "Epoch: 87 \tTraining Loss: 0.015393 \tValidation Loss: 0.014790\n",
      "Validation loss decreased (0.014797 --> 0.014790).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.015356 \tValidation Loss: 0.014804\n",
      "Epoch: 89 \tTraining Loss: 0.015361 \tValidation Loss: 0.014826\n",
      "Epoch: 90 \tTraining Loss: 0.015314 \tValidation Loss: 0.014803\n",
      "Epoch: 91 \tTraining Loss: 0.015318 \tValidation Loss: 0.014801\n",
      "Epoch: 92 \tTraining Loss: 0.015302 \tValidation Loss: 0.014835\n",
      "Epoch: 93 \tTraining Loss: 0.015311 \tValidation Loss: 0.014767\n",
      "Validation loss decreased (0.014790 --> 0.014767).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 0.015301 \tValidation Loss: 0.014813\n",
      "Epoch: 95 \tTraining Loss: 0.015289 \tValidation Loss: 0.014759\n",
      "Validation loss decreased (0.014767 --> 0.014759).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.015290 \tValidation Loss: 0.014848\n",
      "Epoch: 97 \tTraining Loss: 0.015290 \tValidation Loss: 0.014784\n",
      "Epoch: 98 \tTraining Loss: 0.015268 \tValidation Loss: 0.014778\n",
      "Epoch: 99 \tTraining Loss: 0.015249 \tValidation Loss: 0.014735\n",
      "Validation loss decreased (0.014759 --> 0.014735).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 0.015271 \tValidation Loss: 0.014789\n",
      "Epoch: 101 \tTraining Loss: 0.015222 \tValidation Loss: 0.014754\n",
      "Epoch: 102 \tTraining Loss: 0.015226 \tValidation Loss: 0.014789\n",
      "Epoch: 103 \tTraining Loss: 0.015209 \tValidation Loss: 0.014776\n",
      "Epoch: 104 \tTraining Loss: 0.015205 \tValidation Loss: 0.014755\n",
      "Epoch: 105 \tTraining Loss: 0.015197 \tValidation Loss: 0.014733\n",
      "Validation loss decreased (0.014735 --> 0.014733).  Saving model ...\n",
      "Epoch: 106 \tTraining Loss: 0.015173 \tValidation Loss: 0.014741\n",
      "Epoch: 107 \tTraining Loss: 0.015178 \tValidation Loss: 0.014761\n",
      "Epoch: 108 \tTraining Loss: 0.015202 \tValidation Loss: 0.014743\n",
      "Epoch: 109 \tTraining Loss: 0.015186 \tValidation Loss: 0.014788\n",
      "Epoch: 110 \tTraining Loss: 0.015162 \tValidation Loss: 0.014721\n",
      "Validation loss decreased (0.014733 --> 0.014721).  Saving model ...\n",
      "Epoch: 111 \tTraining Loss: 0.015150 \tValidation Loss: 0.014714\n",
      "Validation loss decreased (0.014721 --> 0.014714).  Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 0.015150 \tValidation Loss: 0.014702\n",
      "Validation loss decreased (0.014714 --> 0.014702).  Saving model ...\n",
      "Epoch: 113 \tTraining Loss: 0.015148 \tValidation Loss: 0.014772\n",
      "Epoch: 114 \tTraining Loss: 0.015138 \tValidation Loss: 0.014730\n",
      "Epoch: 115 \tTraining Loss: 0.015152 \tValidation Loss: 0.014751\n",
      "Epoch: 116 \tTraining Loss: 0.015118 \tValidation Loss: 0.014693\n",
      "Validation loss decreased (0.014702 --> 0.014693).  Saving model ...\n",
      "Epoch: 117 \tTraining Loss: 0.015109 \tValidation Loss: 0.014687\n",
      "Validation loss decreased (0.014693 --> 0.014687).  Saving model ...\n",
      "Epoch: 118 \tTraining Loss: 0.015115 \tValidation Loss: 0.014735\n",
      "Epoch: 119 \tTraining Loss: 0.015090 \tValidation Loss: 0.014687\n",
      "Epoch: 120 \tTraining Loss: 0.015082 \tValidation Loss: 0.014701\n",
      "Epoch: 121 \tTraining Loss: 0.015068 \tValidation Loss: 0.014713\n",
      "Epoch: 122 \tTraining Loss: 0.015075 \tValidation Loss: 0.014686\n",
      "Validation loss decreased (0.014687 --> 0.014686).  Saving model ...\n",
      "Epoch: 123 \tTraining Loss: 0.015048 \tValidation Loss: 0.014686\n",
      "Validation loss decreased (0.014686 --> 0.014686).  Saving model ...\n",
      "Epoch: 124 \tTraining Loss: 0.015038 \tValidation Loss: 0.014688\n",
      "Epoch: 125 \tTraining Loss: 0.015049 \tValidation Loss: 0.014712\n",
      "Epoch: 126 \tTraining Loss: 0.015064 \tValidation Loss: 0.014669\n",
      "Validation loss decreased (0.014686 --> 0.014669).  Saving model ...\n",
      "Epoch: 127 \tTraining Loss: 0.015052 \tValidation Loss: 0.014686\n",
      "Epoch: 128 \tTraining Loss: 0.015043 \tValidation Loss: 0.014700\n",
      "Epoch: 129 \tTraining Loss: 0.015035 \tValidation Loss: 0.014667\n",
      "Validation loss decreased (0.014669 --> 0.014667).  Saving model ...\n",
      "Epoch: 130 \tTraining Loss: 0.015017 \tValidation Loss: 0.014656\n",
      "Validation loss decreased (0.014667 --> 0.014656).  Saving model ...\n",
      "Epoch: 131 \tTraining Loss: 0.015008 \tValidation Loss: 0.014682\n",
      "Epoch: 132 \tTraining Loss: 0.015011 \tValidation Loss: 0.014671\n",
      "Epoch: 133 \tTraining Loss: 0.015005 \tValidation Loss: 0.014662\n",
      "Epoch: 134 \tTraining Loss: 0.015005 \tValidation Loss: 0.014681\n",
      "Epoch: 135 \tTraining Loss: 0.015008 \tValidation Loss: 0.014646\n",
      "Validation loss decreased (0.014656 --> 0.014646).  Saving model ...\n",
      "Epoch: 136 \tTraining Loss: 0.014996 \tValidation Loss: 0.014670\n",
      "Epoch: 137 \tTraining Loss: 0.014965 \tValidation Loss: 0.014705\n",
      "Epoch: 138 \tTraining Loss: 0.014973 \tValidation Loss: 0.014663\n",
      "Epoch: 139 \tTraining Loss: 0.014967 \tValidation Loss: 0.014651\n",
      "Epoch: 140 \tTraining Loss: 0.014974 \tValidation Loss: 0.014677\n",
      "Epoch: 141 \tTraining Loss: 0.014954 \tValidation Loss: 0.014685\n",
      "Epoch: 142 \tTraining Loss: 0.014955 \tValidation Loss: 0.014651\n",
      "Epoch: 143 \tTraining Loss: 0.014959 \tValidation Loss: 0.014636\n",
      "Validation loss decreased (0.014646 --> 0.014636).  Saving model ...\n",
      "Epoch: 144 \tTraining Loss: 0.014939 \tValidation Loss: 0.014650\n",
      "Epoch: 145 \tTraining Loss: 0.014944 \tValidation Loss: 0.014646\n",
      "Epoch: 146 \tTraining Loss: 0.014944 \tValidation Loss: 0.014631\n",
      "Validation loss decreased (0.014636 --> 0.014631).  Saving model ...\n",
      "Epoch: 147 \tTraining Loss: 0.014934 \tValidation Loss: 0.014638\n",
      "Epoch: 148 \tTraining Loss: 0.014931 \tValidation Loss: 0.014655\n",
      "Epoch: 149 \tTraining Loss: 0.014931 \tValidation Loss: 0.014623\n",
      "Validation loss decreased (0.014631 --> 0.014623).  Saving model ...\n",
      "Epoch: 150 \tTraining Loss: 0.014918 \tValidation Loss: 0.014633\n",
      "Epoch: 151 \tTraining Loss: 0.014895 \tValidation Loss: 0.014629\n",
      "Epoch: 152 \tTraining Loss: 0.014889 \tValidation Loss: 0.014624\n",
      "Epoch: 153 \tTraining Loss: 0.014908 \tValidation Loss: 0.014649\n",
      "Epoch: 154 \tTraining Loss: 0.014898 \tValidation Loss: 0.014671\n",
      "Epoch: 155 \tTraining Loss: 0.014873 \tValidation Loss: 0.014645\n",
      "Epoch: 156 \tTraining Loss: 0.014884 \tValidation Loss: 0.014628\n",
      "Epoch: 157 \tTraining Loss: 0.014878 \tValidation Loss: 0.014629\n",
      "Epoch: 158 \tTraining Loss: 0.014891 \tValidation Loss: 0.014630\n",
      "Epoch: 159 \tTraining Loss: 0.014865 \tValidation Loss: 0.014622\n",
      "Validation loss decreased (0.014623 --> 0.014622).  Saving model ...\n",
      "Epoch: 160 \tTraining Loss: 0.014865 \tValidation Loss: 0.014633\n",
      "Epoch: 161 \tTraining Loss: 0.014858 \tValidation Loss: 0.014619\n",
      "Validation loss decreased (0.014622 --> 0.014619).  Saving model ...\n",
      "Epoch: 162 \tTraining Loss: 0.014852 \tValidation Loss: 0.014617\n",
      "Validation loss decreased (0.014619 --> 0.014617).  Saving model ...\n",
      "Epoch: 163 \tTraining Loss: 0.014850 \tValidation Loss: 0.014632\n",
      "Epoch: 164 \tTraining Loss: 0.014849 \tValidation Loss: 0.014620\n",
      "Epoch: 165 \tTraining Loss: 0.014858 \tValidation Loss: 0.014622\n",
      "Epoch: 166 \tTraining Loss: 0.014872 \tValidation Loss: 0.014607\n",
      "Validation loss decreased (0.014617 --> 0.014607).  Saving model ...\n",
      "Epoch: 167 \tTraining Loss: 0.014834 \tValidation Loss: 0.014631\n",
      "Epoch: 168 \tTraining Loss: 0.014836 \tValidation Loss: 0.014641\n",
      "Epoch: 169 \tTraining Loss: 0.014839 \tValidation Loss: 0.014625\n",
      "Epoch: 170 \tTraining Loss: 0.014836 \tValidation Loss: 0.014613\n",
      "Epoch: 171 \tTraining Loss: 0.014827 \tValidation Loss: 0.014649\n",
      "Epoch: 172 \tTraining Loss: 0.014831 \tValidation Loss: 0.014601\n",
      "Validation loss decreased (0.014607 --> 0.014601).  Saving model ...\n",
      "Epoch: 173 \tTraining Loss: 0.014817 \tValidation Loss: 0.014596\n",
      "Validation loss decreased (0.014601 --> 0.014596).  Saving model ...\n",
      "Epoch: 174 \tTraining Loss: 0.014795 \tValidation Loss: 0.014623\n",
      "Epoch: 175 \tTraining Loss: 0.014801 \tValidation Loss: 0.014605\n",
      "Epoch: 176 \tTraining Loss: 0.014794 \tValidation Loss: 0.014592\n",
      "Validation loss decreased (0.014596 --> 0.014592).  Saving model ...\n",
      "Epoch: 177 \tTraining Loss: 0.014811 \tValidation Loss: 0.014603\n",
      "Epoch: 178 \tTraining Loss: 0.014798 \tValidation Loss: 0.014601\n",
      "Epoch: 179 \tTraining Loss: 0.014785 \tValidation Loss: 0.014594\n",
      "Epoch: 180 \tTraining Loss: 0.014793 \tValidation Loss: 0.014602\n",
      "Epoch: 181 \tTraining Loss: 0.014788 \tValidation Loss: 0.014597\n",
      "Epoch: 182 \tTraining Loss: 0.014784 \tValidation Loss: 0.014592\n",
      "Validation loss decreased (0.014592 --> 0.014592).  Saving model ...\n",
      "Epoch: 183 \tTraining Loss: 0.014769 \tValidation Loss: 0.014623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 184 \tTraining Loss: 0.014770 \tValidation Loss: 0.014594\n",
      "Epoch: 185 \tTraining Loss: 0.014758 \tValidation Loss: 0.014589\n",
      "Validation loss decreased (0.014592 --> 0.014589).  Saving model ...\n",
      "Epoch: 186 \tTraining Loss: 0.014763 \tValidation Loss: 0.014568\n",
      "Validation loss decreased (0.014589 --> 0.014568).  Saving model ...\n",
      "Epoch: 187 \tTraining Loss: 0.014759 \tValidation Loss: 0.014586\n",
      "Epoch: 188 \tTraining Loss: 0.014755 \tValidation Loss: 0.014606\n",
      "Epoch: 189 \tTraining Loss: 0.014750 \tValidation Loss: 0.014586\n",
      "Epoch: 190 \tTraining Loss: 0.014742 \tValidation Loss: 0.014587\n",
      "Epoch: 191 \tTraining Loss: 0.014739 \tValidation Loss: 0.014586\n",
      "Epoch: 192 \tTraining Loss: 0.014746 \tValidation Loss: 0.014566\n",
      "Validation loss decreased (0.014568 --> 0.014566).  Saving model ...\n",
      "Epoch: 193 \tTraining Loss: 0.014746 \tValidation Loss: 0.014586\n",
      "Epoch: 194 \tTraining Loss: 0.014729 \tValidation Loss: 0.014594\n",
      "Epoch: 195 \tTraining Loss: 0.014733 \tValidation Loss: 0.014590\n",
      "Epoch: 196 \tTraining Loss: 0.014707 \tValidation Loss: 0.014585\n",
      "Epoch: 197 \tTraining Loss: 0.014726 \tValidation Loss: 0.014579\n",
      "Epoch: 198 \tTraining Loss: 0.014723 \tValidation Loss: 0.014581\n",
      "Epoch: 199 \tTraining Loss: 0.014713 \tValidation Loss: 0.014570\n",
      "Epoch: 200 \tTraining Loss: 0.014701 \tValidation Loss: 0.014569\n",
      "Epoch: 201 \tTraining Loss: 0.014703 \tValidation Loss: 0.014563\n",
      "Validation loss decreased (0.014566 --> 0.014563).  Saving model ...\n",
      "Epoch: 202 \tTraining Loss: 0.014715 \tValidation Loss: 0.014563\n",
      "Epoch: 203 \tTraining Loss: 0.014716 \tValidation Loss: 0.014598\n",
      "Epoch: 204 \tTraining Loss: 0.014696 \tValidation Loss: 0.014563\n",
      "Epoch: 205 \tTraining Loss: 0.014708 \tValidation Loss: 0.014579\n",
      "Epoch: 206 \tTraining Loss: 0.014692 \tValidation Loss: 0.014560\n",
      "Validation loss decreased (0.014563 --> 0.014560).  Saving model ...\n",
      "Epoch: 207 \tTraining Loss: 0.014696 \tValidation Loss: 0.014570\n",
      "Epoch: 208 \tTraining Loss: 0.014670 \tValidation Loss: 0.014558\n",
      "Validation loss decreased (0.014560 --> 0.014558).  Saving model ...\n",
      "Epoch: 209 \tTraining Loss: 0.014701 \tValidation Loss: 0.014559\n",
      "Epoch: 210 \tTraining Loss: 0.014675 \tValidation Loss: 0.014556\n",
      "Validation loss decreased (0.014558 --> 0.014556).  Saving model ...\n",
      "Epoch: 211 \tTraining Loss: 0.014677 \tValidation Loss: 0.014553\n",
      "Validation loss decreased (0.014556 --> 0.014553).  Saving model ...\n",
      "Epoch: 212 \tTraining Loss: 0.014675 \tValidation Loss: 0.014566\n",
      "Epoch: 213 \tTraining Loss: 0.014669 \tValidation Loss: 0.014547\n",
      "Validation loss decreased (0.014553 --> 0.014547).  Saving model ...\n",
      "Epoch: 214 \tTraining Loss: 0.014674 \tValidation Loss: 0.014575\n",
      "Epoch: 215 \tTraining Loss: 0.014674 \tValidation Loss: 0.014561\n",
      "Epoch: 216 \tTraining Loss: 0.014660 \tValidation Loss: 0.014546\n",
      "Validation loss decreased (0.014547 --> 0.014546).  Saving model ...\n",
      "Epoch: 217 \tTraining Loss: 0.014650 \tValidation Loss: 0.014552\n",
      "Epoch: 218 \tTraining Loss: 0.014661 \tValidation Loss: 0.014554\n",
      "Epoch: 219 \tTraining Loss: 0.014675 \tValidation Loss: 0.014551\n",
      "Epoch: 220 \tTraining Loss: 0.014645 \tValidation Loss: 0.014548\n",
      "Epoch: 221 \tTraining Loss: 0.014644 \tValidation Loss: 0.014543\n",
      "Validation loss decreased (0.014546 --> 0.014543).  Saving model ...\n",
      "Epoch: 222 \tTraining Loss: 0.014660 \tValidation Loss: 0.014554\n",
      "Epoch: 223 \tTraining Loss: 0.014636 \tValidation Loss: 0.014561\n",
      "Epoch: 224 \tTraining Loss: 0.014645 \tValidation Loss: 0.014544\n",
      "Epoch: 225 \tTraining Loss: 0.014642 \tValidation Loss: 0.014544\n",
      "Epoch: 226 \tTraining Loss: 0.014647 \tValidation Loss: 0.014546\n",
      "Epoch: 227 \tTraining Loss: 0.014644 \tValidation Loss: 0.014565\n",
      "Epoch: 228 \tTraining Loss: 0.014626 \tValidation Loss: 0.014548\n",
      "Epoch: 229 \tTraining Loss: 0.014618 \tValidation Loss: 0.014554\n",
      "Epoch: 230 \tTraining Loss: 0.014619 \tValidation Loss: 0.014535\n",
      "Validation loss decreased (0.014543 --> 0.014535).  Saving model ...\n",
      "Epoch: 231 \tTraining Loss: 0.014620 \tValidation Loss: 0.014557\n",
      "Epoch: 232 \tTraining Loss: 0.014603 \tValidation Loss: 0.014549\n",
      "Epoch: 233 \tTraining Loss: 0.014622 \tValidation Loss: 0.014545\n",
      "Epoch: 234 \tTraining Loss: 0.014614 \tValidation Loss: 0.014543\n",
      "Epoch: 235 \tTraining Loss: 0.014601 \tValidation Loss: 0.014532\n",
      "Validation loss decreased (0.014535 --> 0.014532).  Saving model ...\n",
      "Epoch: 236 \tTraining Loss: 0.014601 \tValidation Loss: 0.014536\n",
      "Epoch: 237 \tTraining Loss: 0.014600 \tValidation Loss: 0.014540\n",
      "Epoch: 238 \tTraining Loss: 0.014596 \tValidation Loss: 0.014536\n",
      "Epoch: 239 \tTraining Loss: 0.014596 \tValidation Loss: 0.014539\n",
      "Epoch: 240 \tTraining Loss: 0.014606 \tValidation Loss: 0.014560\n",
      "Epoch: 241 \tTraining Loss: 0.014595 \tValidation Loss: 0.014548\n",
      "Epoch: 242 \tTraining Loss: 0.014585 \tValidation Loss: 0.014530\n",
      "Validation loss decreased (0.014532 --> 0.014530).  Saving model ...\n",
      "Epoch: 243 \tTraining Loss: 0.014597 \tValidation Loss: 0.014527\n",
      "Validation loss decreased (0.014530 --> 0.014527).  Saving model ...\n",
      "Epoch: 244 \tTraining Loss: 0.014572 \tValidation Loss: 0.014538\n",
      "Epoch: 245 \tTraining Loss: 0.014580 \tValidation Loss: 0.014530\n",
      "Epoch: 246 \tTraining Loss: 0.014578 \tValidation Loss: 0.014541\n",
      "Epoch: 247 \tTraining Loss: 0.014586 \tValidation Loss: 0.014533\n",
      "Epoch: 248 \tTraining Loss: 0.014574 \tValidation Loss: 0.014526\n",
      "Validation loss decreased (0.014527 --> 0.014526).  Saving model ...\n",
      "Epoch: 249 \tTraining Loss: 0.014579 \tValidation Loss: 0.014527\n",
      "Epoch: 250 \tTraining Loss: 0.014573 \tValidation Loss: 0.014522\n",
      "Validation loss decreased (0.014526 --> 0.014522).  Saving model ...\n",
      "Epoch: 251 \tTraining Loss: 0.014563 \tValidation Loss: 0.014526\n",
      "Epoch: 252 \tTraining Loss: 0.014566 \tValidation Loss: 0.014525\n",
      "Epoch: 253 \tTraining Loss: 0.014569 \tValidation Loss: 0.014520\n",
      "Validation loss decreased (0.014522 --> 0.014520).  Saving model ...\n",
      "Epoch: 254 \tTraining Loss: 0.014568 \tValidation Loss: 0.014544\n",
      "Epoch: 255 \tTraining Loss: 0.014558 \tValidation Loss: 0.014539\n",
      "Epoch: 256 \tTraining Loss: 0.014555 \tValidation Loss: 0.014531\n",
      "Epoch: 257 \tTraining Loss: 0.014557 \tValidation Loss: 0.014534\n",
      "Epoch: 258 \tTraining Loss: 0.014553 \tValidation Loss: 0.014523\n",
      "Epoch: 259 \tTraining Loss: 0.014546 \tValidation Loss: 0.014522\n",
      "Epoch: 260 \tTraining Loss: 0.014539 \tValidation Loss: 0.014527\n",
      "Epoch: 261 \tTraining Loss: 0.014563 \tValidation Loss: 0.014517\n",
      "Validation loss decreased (0.014520 --> 0.014517).  Saving model ...\n",
      "Epoch: 262 \tTraining Loss: 0.014551 \tValidation Loss: 0.014521\n",
      "Epoch: 263 \tTraining Loss: 0.014533 \tValidation Loss: 0.014519\n",
      "Epoch: 264 \tTraining Loss: 0.014550 \tValidation Loss: 0.014524\n",
      "Epoch: 265 \tTraining Loss: 0.014537 \tValidation Loss: 0.014514\n",
      "Validation loss decreased (0.014517 --> 0.014514).  Saving model ...\n",
      "Epoch: 266 \tTraining Loss: 0.014548 \tValidation Loss: 0.014520\n",
      "Epoch: 267 \tTraining Loss: 0.014534 \tValidation Loss: 0.014518\n",
      "Epoch: 268 \tTraining Loss: 0.014520 \tValidation Loss: 0.014520\n",
      "Epoch: 269 \tTraining Loss: 0.014531 \tValidation Loss: 0.014515\n",
      "Epoch: 270 \tTraining Loss: 0.014518 \tValidation Loss: 0.014527\n",
      "Epoch: 271 \tTraining Loss: 0.014514 \tValidation Loss: 0.014518\n",
      "Epoch: 272 \tTraining Loss: 0.014520 \tValidation Loss: 0.014513\n",
      "Validation loss decreased (0.014514 --> 0.014513).  Saving model ...\n",
      "Epoch: 273 \tTraining Loss: 0.014518 \tValidation Loss: 0.014508\n",
      "Validation loss decreased (0.014513 --> 0.014508).  Saving model ...\n",
      "Epoch: 274 \tTraining Loss: 0.014515 \tValidation Loss: 0.014518\n",
      "Epoch: 275 \tTraining Loss: 0.014504 \tValidation Loss: 0.014502\n",
      "Validation loss decreased (0.014508 --> 0.014502).  Saving model ...\n",
      "Epoch: 276 \tTraining Loss: 0.014516 \tValidation Loss: 0.014511\n",
      "Epoch: 277 \tTraining Loss: 0.014503 \tValidation Loss: 0.014512\n",
      "Epoch: 278 \tTraining Loss: 0.014506 \tValidation Loss: 0.014509\n",
      "Epoch: 279 \tTraining Loss: 0.014493 \tValidation Loss: 0.014521\n",
      "Epoch: 280 \tTraining Loss: 0.014504 \tValidation Loss: 0.014494\n",
      "Validation loss decreased (0.014502 --> 0.014494).  Saving model ...\n",
      "Epoch: 281 \tTraining Loss: 0.014506 \tValidation Loss: 0.014510\n",
      "Epoch: 282 \tTraining Loss: 0.014485 \tValidation Loss: 0.014502\n",
      "Epoch: 283 \tTraining Loss: 0.014490 \tValidation Loss: 0.014503\n",
      "Epoch: 284 \tTraining Loss: 0.014488 \tValidation Loss: 0.014516\n",
      "Epoch: 285 \tTraining Loss: 0.014469 \tValidation Loss: 0.014514\n",
      "Epoch: 286 \tTraining Loss: 0.014483 \tValidation Loss: 0.014515\n",
      "Epoch: 287 \tTraining Loss: 0.014483 \tValidation Loss: 0.014502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 288 \tTraining Loss: 0.014468 \tValidation Loss: 0.014501\n",
      "Epoch: 289 \tTraining Loss: 0.014487 \tValidation Loss: 0.014502\n",
      "Epoch: 290 \tTraining Loss: 0.014486 \tValidation Loss: 0.014491\n",
      "Validation loss decreased (0.014494 --> 0.014491).  Saving model ...\n",
      "Epoch: 291 \tTraining Loss: 0.014457 \tValidation Loss: 0.014513\n",
      "Epoch: 292 \tTraining Loss: 0.014473 \tValidation Loss: 0.014504\n",
      "Epoch: 293 \tTraining Loss: 0.014479 \tValidation Loss: 0.014501\n",
      "Epoch: 294 \tTraining Loss: 0.014463 \tValidation Loss: 0.014508\n",
      "Epoch: 295 \tTraining Loss: 0.014471 \tValidation Loss: 0.014502\n",
      "Epoch: 296 \tTraining Loss: 0.014458 \tValidation Loss: 0.014502\n",
      "Epoch: 297 \tTraining Loss: 0.014477 \tValidation Loss: 0.014495\n",
      "Epoch: 298 \tTraining Loss: 0.014462 \tValidation Loss: 0.014501\n",
      "Epoch: 299 \tTraining Loss: 0.014472 \tValidation Loss: 0.014509\n",
      "Epoch: 300 \tTraining Loss: 0.014461 \tValidation Loss: 0.014515\n",
      "Epoch: 301 \tTraining Loss: 0.014444 \tValidation Loss: 0.014506\n",
      "Epoch: 302 \tTraining Loss: 0.014458 \tValidation Loss: 0.014496\n",
      "Epoch: 303 \tTraining Loss: 0.014458 \tValidation Loss: 0.014504\n",
      "Epoch: 304 \tTraining Loss: 0.014450 \tValidation Loss: 0.014505\n",
      "Epoch: 305 \tTraining Loss: 0.014450 \tValidation Loss: 0.014494\n",
      "Epoch: 306 \tTraining Loss: 0.014459 \tValidation Loss: 0.014497\n",
      "Epoch: 307 \tTraining Loss: 0.014428 \tValidation Loss: 0.014492\n",
      "Epoch: 308 \tTraining Loss: 0.014451 \tValidation Loss: 0.014499\n",
      "Epoch: 309 \tTraining Loss: 0.014443 \tValidation Loss: 0.014496\n",
      "Epoch: 310 \tTraining Loss: 0.014445 \tValidation Loss: 0.014492\n",
      "Epoch: 311 \tTraining Loss: 0.014436 \tValidation Loss: 0.014491\n",
      "Epoch: 312 \tTraining Loss: 0.014437 \tValidation Loss: 0.014496\n",
      "Epoch: 313 \tTraining Loss: 0.014429 \tValidation Loss: 0.014500\n",
      "Epoch: 314 \tTraining Loss: 0.014451 \tValidation Loss: 0.014490\n",
      "Validation loss decreased (0.014491 --> 0.014490).  Saving model ...\n",
      "Epoch: 315 \tTraining Loss: 0.014420 \tValidation Loss: 0.014485\n",
      "Validation loss decreased (0.014490 --> 0.014485).  Saving model ...\n",
      "Epoch: 316 \tTraining Loss: 0.014436 \tValidation Loss: 0.014489\n",
      "Epoch: 317 \tTraining Loss: 0.014425 \tValidation Loss: 0.014493\n",
      "Epoch: 318 \tTraining Loss: 0.014422 \tValidation Loss: 0.014480\n",
      "Validation loss decreased (0.014485 --> 0.014480).  Saving model ...\n",
      "Epoch: 319 \tTraining Loss: 0.014427 \tValidation Loss: 0.014500\n",
      "Epoch: 320 \tTraining Loss: 0.014419 \tValidation Loss: 0.014491\n",
      "Epoch: 321 \tTraining Loss: 0.014431 \tValidation Loss: 0.014489\n",
      "Epoch: 322 \tTraining Loss: 0.014414 \tValidation Loss: 0.014491\n",
      "Epoch: 323 \tTraining Loss: 0.014410 \tValidation Loss: 0.014483\n",
      "Epoch: 324 \tTraining Loss: 0.014415 \tValidation Loss: 0.014494\n",
      "Epoch: 325 \tTraining Loss: 0.014424 \tValidation Loss: 0.014488\n",
      "Epoch: 326 \tTraining Loss: 0.014402 \tValidation Loss: 0.014484\n",
      "Epoch: 327 \tTraining Loss: 0.014400 \tValidation Loss: 0.014487\n",
      "Epoch: 328 \tTraining Loss: 0.014410 \tValidation Loss: 0.014490\n",
      "Epoch: 329 \tTraining Loss: 0.014408 \tValidation Loss: 0.014484\n",
      "Epoch: 330 \tTraining Loss: 0.014406 \tValidation Loss: 0.014487\n",
      "Epoch: 331 \tTraining Loss: 0.014400 \tValidation Loss: 0.014483\n",
      "Epoch: 332 \tTraining Loss: 0.014409 \tValidation Loss: 0.014487\n",
      "Epoch: 333 \tTraining Loss: 0.014409 \tValidation Loss: 0.014479\n",
      "Validation loss decreased (0.014480 --> 0.014479).  Saving model ...\n",
      "Epoch: 334 \tTraining Loss: 0.014387 \tValidation Loss: 0.014489\n",
      "Epoch: 335 \tTraining Loss: 0.014388 \tValidation Loss: 0.014488\n",
      "Epoch: 336 \tTraining Loss: 0.014388 \tValidation Loss: 0.014494\n",
      "Epoch: 337 \tTraining Loss: 0.014388 \tValidation Loss: 0.014481\n",
      "Epoch: 338 \tTraining Loss: 0.014390 \tValidation Loss: 0.014479\n",
      "Validation loss decreased (0.014479 --> 0.014479).  Saving model ...\n",
      "Epoch: 339 \tTraining Loss: 0.014387 \tValidation Loss: 0.014481\n",
      "Epoch: 340 \tTraining Loss: 0.014383 \tValidation Loss: 0.014479\n",
      "Epoch: 341 \tTraining Loss: 0.014390 \tValidation Loss: 0.014489\n",
      "Epoch: 342 \tTraining Loss: 0.014382 \tValidation Loss: 0.014483\n",
      "Epoch: 343 \tTraining Loss: 0.014375 \tValidation Loss: 0.014480\n",
      "Epoch: 344 \tTraining Loss: 0.014391 \tValidation Loss: 0.014485\n",
      "Epoch: 345 \tTraining Loss: 0.014366 \tValidation Loss: 0.014488\n",
      "Epoch: 346 \tTraining Loss: 0.014373 \tValidation Loss: 0.014476\n",
      "Validation loss decreased (0.014479 --> 0.014476).  Saving model ...\n",
      "Epoch: 347 \tTraining Loss: 0.014384 \tValidation Loss: 0.014478\n",
      "Epoch: 348 \tTraining Loss: 0.014374 \tValidation Loss: 0.014491\n",
      "Epoch: 349 \tTraining Loss: 0.014367 \tValidation Loss: 0.014490\n",
      "Epoch: 350 \tTraining Loss: 0.014372 \tValidation Loss: 0.014483\n",
      "Epoch: 351 \tTraining Loss: 0.014361 \tValidation Loss: 0.014480\n",
      "Epoch: 352 \tTraining Loss: 0.014353 \tValidation Loss: 0.014474\n",
      "Validation loss decreased (0.014476 --> 0.014474).  Saving model ...\n",
      "Epoch: 353 \tTraining Loss: 0.014378 \tValidation Loss: 0.014471\n",
      "Validation loss decreased (0.014474 --> 0.014471).  Saving model ...\n",
      "Epoch: 354 \tTraining Loss: 0.014367 \tValidation Loss: 0.014478\n",
      "Epoch: 355 \tTraining Loss: 0.014370 \tValidation Loss: 0.014473\n",
      "Epoch: 356 \tTraining Loss: 0.014363 \tValidation Loss: 0.014474\n",
      "Epoch: 357 \tTraining Loss: 0.014364 \tValidation Loss: 0.014476\n",
      "Epoch: 358 \tTraining Loss: 0.014366 \tValidation Loss: 0.014481\n",
      "Epoch: 359 \tTraining Loss: 0.014349 \tValidation Loss: 0.014479\n",
      "Epoch: 360 \tTraining Loss: 0.014348 \tValidation Loss: 0.014474\n",
      "Epoch: 361 \tTraining Loss: 0.014346 \tValidation Loss: 0.014474\n",
      "Epoch: 362 \tTraining Loss: 0.014356 \tValidation Loss: 0.014475\n",
      "Epoch: 363 \tTraining Loss: 0.014340 \tValidation Loss: 0.014481\n",
      "Epoch: 364 \tTraining Loss: 0.014362 \tValidation Loss: 0.014476\n",
      "Epoch: 365 \tTraining Loss: 0.014358 \tValidation Loss: 0.014475\n",
      "Epoch: 366 \tTraining Loss: 0.014349 \tValidation Loss: 0.014471\n",
      "Epoch: 367 \tTraining Loss: 0.014334 \tValidation Loss: 0.014475\n",
      "Epoch: 368 \tTraining Loss: 0.014348 \tValidation Loss: 0.014468\n",
      "Validation loss decreased (0.014471 --> 0.014468).  Saving model ...\n",
      "Epoch: 369 \tTraining Loss: 0.014343 \tValidation Loss: 0.014476\n",
      "Epoch: 370 \tTraining Loss: 0.014332 \tValidation Loss: 0.014474\n",
      "Epoch: 371 \tTraining Loss: 0.014329 \tValidation Loss: 0.014469\n",
      "Epoch: 372 \tTraining Loss: 0.014342 \tValidation Loss: 0.014475\n",
      "Epoch: 373 \tTraining Loss: 0.014334 \tValidation Loss: 0.014476\n",
      "Epoch: 374 \tTraining Loss: 0.014333 \tValidation Loss: 0.014472\n",
      "Epoch: 375 \tTraining Loss: 0.014322 \tValidation Loss: 0.014465\n",
      "Validation loss decreased (0.014468 --> 0.014465).  Saving model ...\n",
      "Epoch: 376 \tTraining Loss: 0.014349 \tValidation Loss: 0.014473\n",
      "Epoch: 377 \tTraining Loss: 0.014323 \tValidation Loss: 0.014474\n",
      "Epoch: 378 \tTraining Loss: 0.014338 \tValidation Loss: 0.014473\n",
      "Epoch: 379 \tTraining Loss: 0.014321 \tValidation Loss: 0.014468\n",
      "Epoch: 380 \tTraining Loss: 0.014321 \tValidation Loss: 0.014470\n",
      "Epoch: 381 \tTraining Loss: 0.014313 \tValidation Loss: 0.014471\n",
      "Epoch: 382 \tTraining Loss: 0.014328 \tValidation Loss: 0.014476\n",
      "Epoch: 383 \tTraining Loss: 0.014302 \tValidation Loss: 0.014470\n",
      "Epoch: 384 \tTraining Loss: 0.014320 \tValidation Loss: 0.014470\n",
      "Epoch: 385 \tTraining Loss: 0.014317 \tValidation Loss: 0.014472\n",
      "Epoch: 386 \tTraining Loss: 0.014304 \tValidation Loss: 0.014473\n",
      "Epoch: 387 \tTraining Loss: 0.014306 \tValidation Loss: 0.014470\n",
      "Epoch: 388 \tTraining Loss: 0.014315 \tValidation Loss: 0.014467\n",
      "Epoch: 389 \tTraining Loss: 0.014314 \tValidation Loss: 0.014467\n",
      "Epoch: 390 \tTraining Loss: 0.014310 \tValidation Loss: 0.014469\n",
      "Epoch: 391 \tTraining Loss: 0.014302 \tValidation Loss: 0.014469\n",
      "Epoch: 392 \tTraining Loss: 0.014289 \tValidation Loss: 0.014460\n",
      "Validation loss decreased (0.014465 --> 0.014460).  Saving model ...\n",
      "Epoch: 393 \tTraining Loss: 0.014316 \tValidation Loss: 0.014469\n",
      "Epoch: 394 \tTraining Loss: 0.014287 \tValidation Loss: 0.014466\n",
      "Epoch: 395 \tTraining Loss: 0.014278 \tValidation Loss: 0.014467\n",
      "Epoch: 396 \tTraining Loss: 0.014318 \tValidation Loss: 0.014470\n",
      "Epoch: 397 \tTraining Loss: 0.014278 \tValidation Loss: 0.014467\n",
      "Epoch: 398 \tTraining Loss: 0.014290 \tValidation Loss: 0.014470\n",
      "Epoch: 399 \tTraining Loss: 0.014307 \tValidation Loss: 0.014457\n",
      "Validation loss decreased (0.014460 --> 0.014457).  Saving model ...\n",
      "Epoch: 400 \tTraining Loss: 0.014289 \tValidation Loss: 0.014465\n",
      "Epoch: 401 \tTraining Loss: 0.014289 \tValidation Loss: 0.014465\n",
      "Epoch: 402 \tTraining Loss: 0.014286 \tValidation Loss: 0.014461\n",
      "Epoch: 403 \tTraining Loss: 0.014287 \tValidation Loss: 0.014467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 404 \tTraining Loss: 0.014288 \tValidation Loss: 0.014461\n",
      "Epoch: 405 \tTraining Loss: 0.014279 \tValidation Loss: 0.014464\n",
      "Epoch: 406 \tTraining Loss: 0.014286 \tValidation Loss: 0.014463\n",
      "Epoch: 407 \tTraining Loss: 0.014289 \tValidation Loss: 0.014457\n",
      "Validation loss decreased (0.014457 --> 0.014457).  Saving model ...\n",
      "Epoch: 408 \tTraining Loss: 0.014295 \tValidation Loss: 0.014463\n",
      "Epoch: 409 \tTraining Loss: 0.014272 \tValidation Loss: 0.014467\n",
      "Epoch: 410 \tTraining Loss: 0.014268 \tValidation Loss: 0.014460\n",
      "Epoch: 411 \tTraining Loss: 0.014288 \tValidation Loss: 0.014459\n",
      "Epoch: 412 \tTraining Loss: 0.014277 \tValidation Loss: 0.014462\n",
      "Epoch: 413 \tTraining Loss: 0.014289 \tValidation Loss: 0.014463\n",
      "Epoch: 414 \tTraining Loss: 0.014286 \tValidation Loss: 0.014459\n",
      "Epoch: 415 \tTraining Loss: 0.014291 \tValidation Loss: 0.014466\n",
      "Epoch: 416 \tTraining Loss: 0.014271 \tValidation Loss: 0.014460\n",
      "Epoch: 417 \tTraining Loss: 0.014272 \tValidation Loss: 0.014459\n",
      "Epoch: 418 \tTraining Loss: 0.014268 \tValidation Loss: 0.014464\n",
      "Epoch: 419 \tTraining Loss: 0.014276 \tValidation Loss: 0.014465\n",
      "Epoch: 420 \tTraining Loss: 0.014273 \tValidation Loss: 0.014461\n",
      "Epoch: 421 \tTraining Loss: 0.014267 \tValidation Loss: 0.014455\n",
      "Validation loss decreased (0.014457 --> 0.014455).  Saving model ...\n",
      "Epoch: 422 \tTraining Loss: 0.014268 \tValidation Loss: 0.014461\n",
      "Epoch: 423 \tTraining Loss: 0.014257 \tValidation Loss: 0.014463\n",
      "Epoch: 424 \tTraining Loss: 0.014260 \tValidation Loss: 0.014459\n",
      "Epoch: 425 \tTraining Loss: 0.014258 \tValidation Loss: 0.014461\n",
      "Epoch: 426 \tTraining Loss: 0.014244 \tValidation Loss: 0.014458\n",
      "Epoch: 427 \tTraining Loss: 0.014260 \tValidation Loss: 0.014461\n",
      "Epoch: 428 \tTraining Loss: 0.014246 \tValidation Loss: 0.014453\n",
      "Validation loss decreased (0.014455 --> 0.014453).  Saving model ...\n",
      "Epoch: 429 \tTraining Loss: 0.014242 \tValidation Loss: 0.014459\n",
      "Epoch: 430 \tTraining Loss: 0.014247 \tValidation Loss: 0.014460\n",
      "Epoch: 431 \tTraining Loss: 0.014260 \tValidation Loss: 0.014460\n",
      "Epoch: 432 \tTraining Loss: 0.014240 \tValidation Loss: 0.014458\n",
      "Epoch: 433 \tTraining Loss: 0.014251 \tValidation Loss: 0.014456\n",
      "Epoch: 434 \tTraining Loss: 0.014231 \tValidation Loss: 0.014459\n",
      "Epoch: 435 \tTraining Loss: 0.014255 \tValidation Loss: 0.014459\n",
      "Epoch: 436 \tTraining Loss: 0.014254 \tValidation Loss: 0.014456\n",
      "Epoch: 437 \tTraining Loss: 0.014235 \tValidation Loss: 0.014462\n",
      "Epoch: 438 \tTraining Loss: 0.014252 \tValidation Loss: 0.014457\n",
      "Epoch: 439 \tTraining Loss: 0.014238 \tValidation Loss: 0.014454\n",
      "Epoch: 440 \tTraining Loss: 0.014236 \tValidation Loss: 0.014457\n",
      "Epoch: 441 \tTraining Loss: 0.014234 \tValidation Loss: 0.014453\n",
      "Validation loss decreased (0.014453 --> 0.014453).  Saving model ...\n",
      "Epoch: 442 \tTraining Loss: 0.014229 \tValidation Loss: 0.014456\n",
      "Epoch: 443 \tTraining Loss: 0.014224 \tValidation Loss: 0.014461\n",
      "Epoch: 444 \tTraining Loss: 0.014246 \tValidation Loss: 0.014457\n",
      "Epoch: 445 \tTraining Loss: 0.014236 \tValidation Loss: 0.014456\n",
      "Epoch: 446 \tTraining Loss: 0.014222 \tValidation Loss: 0.014451\n",
      "Validation loss decreased (0.014453 --> 0.014451).  Saving model ...\n",
      "Epoch: 447 \tTraining Loss: 0.014229 \tValidation Loss: 0.014456\n",
      "Epoch: 448 \tTraining Loss: 0.014235 \tValidation Loss: 0.014455\n",
      "Epoch: 449 \tTraining Loss: 0.014213 \tValidation Loss: 0.014460\n",
      "Epoch: 450 \tTraining Loss: 0.014209 \tValidation Loss: 0.014448\n",
      "Validation loss decreased (0.014451 --> 0.014448).  Saving model ...\n",
      "Epoch: 451 \tTraining Loss: 0.014236 \tValidation Loss: 0.014455\n",
      "Epoch: 452 \tTraining Loss: 0.014214 \tValidation Loss: 0.014451\n",
      "Epoch: 453 \tTraining Loss: 0.014212 \tValidation Loss: 0.014454\n",
      "Epoch: 454 \tTraining Loss: 0.014219 \tValidation Loss: 0.014455\n",
      "Epoch: 455 \tTraining Loss: 0.014203 \tValidation Loss: 0.014453\n",
      "Epoch: 456 \tTraining Loss: 0.014209 \tValidation Loss: 0.014462\n",
      "Epoch: 457 \tTraining Loss: 0.014219 \tValidation Loss: 0.014451\n",
      "Epoch: 458 \tTraining Loss: 0.014230 \tValidation Loss: 0.014453\n",
      "Epoch: 459 \tTraining Loss: 0.014221 \tValidation Loss: 0.014456\n",
      "Epoch: 460 \tTraining Loss: 0.014198 \tValidation Loss: 0.014446\n",
      "Validation loss decreased (0.014448 --> 0.014446).  Saving model ...\n",
      "Epoch: 461 \tTraining Loss: 0.014211 \tValidation Loss: 0.014454\n",
      "Epoch: 462 \tTraining Loss: 0.014209 \tValidation Loss: 0.014451\n",
      "Epoch: 463 \tTraining Loss: 0.014218 \tValidation Loss: 0.014453\n",
      "Epoch: 464 \tTraining Loss: 0.014195 \tValidation Loss: 0.014451\n",
      "Epoch: 465 \tTraining Loss: 0.014213 \tValidation Loss: 0.014456\n",
      "Epoch: 466 \tTraining Loss: 0.014196 \tValidation Loss: 0.014455\n",
      "Epoch: 467 \tTraining Loss: 0.014204 \tValidation Loss: 0.014444\n",
      "Validation loss decreased (0.014446 --> 0.014444).  Saving model ...\n",
      "Epoch: 468 \tTraining Loss: 0.014207 \tValidation Loss: 0.014450\n",
      "Epoch: 469 \tTraining Loss: 0.014199 \tValidation Loss: 0.014450\n",
      "Epoch: 470 \tTraining Loss: 0.014186 \tValidation Loss: 0.014456\n",
      "Epoch: 471 \tTraining Loss: 0.014185 \tValidation Loss: 0.014449\n",
      "Epoch: 472 \tTraining Loss: 0.014209 \tValidation Loss: 0.014454\n",
      "Epoch: 473 \tTraining Loss: 0.014196 \tValidation Loss: 0.014448\n",
      "Epoch: 474 \tTraining Loss: 0.014195 \tValidation Loss: 0.014447\n",
      "Epoch: 475 \tTraining Loss: 0.014186 \tValidation Loss: 0.014448\n",
      "Epoch: 476 \tTraining Loss: 0.014190 \tValidation Loss: 0.014446\n",
      "Epoch: 477 \tTraining Loss: 0.014184 \tValidation Loss: 0.014449\n",
      "Epoch: 478 \tTraining Loss: 0.014190 \tValidation Loss: 0.014448\n",
      "Epoch: 479 \tTraining Loss: 0.014194 \tValidation Loss: 0.014444\n",
      "Epoch: 480 \tTraining Loss: 0.014176 \tValidation Loss: 0.014444\n",
      "Validation loss decreased (0.014444 --> 0.014444).  Saving model ...\n",
      "Epoch: 481 \tTraining Loss: 0.014181 \tValidation Loss: 0.014453\n",
      "Epoch: 482 \tTraining Loss: 0.014183 \tValidation Loss: 0.014439\n",
      "Validation loss decreased (0.014444 --> 0.014439).  Saving model ...\n",
      "Epoch: 483 \tTraining Loss: 0.014178 \tValidation Loss: 0.014447\n",
      "Epoch: 484 \tTraining Loss: 0.014172 \tValidation Loss: 0.014446\n",
      "Epoch: 485 \tTraining Loss: 0.014158 \tValidation Loss: 0.014439\n",
      "Validation loss decreased (0.014439 --> 0.014439).  Saving model ...\n",
      "Epoch: 486 \tTraining Loss: 0.014177 \tValidation Loss: 0.014443\n",
      "Epoch: 487 \tTraining Loss: 0.014176 \tValidation Loss: 0.014447\n",
      "Epoch: 488 \tTraining Loss: 0.014156 \tValidation Loss: 0.014444\n",
      "Epoch: 489 \tTraining Loss: 0.014168 \tValidation Loss: 0.014443\n",
      "Epoch: 490 \tTraining Loss: 0.014179 \tValidation Loss: 0.014445\n",
      "Epoch: 491 \tTraining Loss: 0.014181 \tValidation Loss: 0.014444\n",
      "Epoch: 492 \tTraining Loss: 0.014176 \tValidation Loss: 0.014447\n",
      "Epoch: 493 \tTraining Loss: 0.014161 \tValidation Loss: 0.014445\n",
      "Epoch: 494 \tTraining Loss: 0.014166 \tValidation Loss: 0.014446\n",
      "Epoch: 495 \tTraining Loss: 0.014166 \tValidation Loss: 0.014447\n",
      "Epoch: 496 \tTraining Loss: 0.014177 \tValidation Loss: 0.014445\n",
      "Epoch: 497 \tTraining Loss: 0.014148 \tValidation Loss: 0.014440\n",
      "Epoch: 498 \tTraining Loss: 0.014177 \tValidation Loss: 0.014443\n",
      "Epoch: 499 \tTraining Loss: 0.014172 \tValidation Loss: 0.014439\n",
      "Epoch: 500 \tTraining Loss: 0.014161 \tValidation Loss: 0.014443\n",
      "Epoch: 501 \tTraining Loss: 0.014160 \tValidation Loss: 0.014451\n",
      "Epoch: 502 \tTraining Loss: 0.014159 \tValidation Loss: 0.014440\n",
      "Epoch: 503 \tTraining Loss: 0.014152 \tValidation Loss: 0.014445\n",
      "Epoch: 504 \tTraining Loss: 0.014165 \tValidation Loss: 0.014438\n",
      "Validation loss decreased (0.014439 --> 0.014438).  Saving model ...\n",
      "Epoch: 505 \tTraining Loss: 0.014153 \tValidation Loss: 0.014440\n",
      "Epoch: 506 \tTraining Loss: 0.014158 \tValidation Loss: 0.014443\n",
      "Epoch: 507 \tTraining Loss: 0.014148 \tValidation Loss: 0.014442\n",
      "Epoch: 508 \tTraining Loss: 0.014152 \tValidation Loss: 0.014445\n",
      "Epoch: 509 \tTraining Loss: 0.014143 \tValidation Loss: 0.014440\n",
      "Epoch: 510 \tTraining Loss: 0.014146 \tValidation Loss: 0.014447\n",
      "Epoch: 511 \tTraining Loss: 0.014154 \tValidation Loss: 0.014438\n",
      "Epoch: 512 \tTraining Loss: 0.014147 \tValidation Loss: 0.014441\n",
      "Epoch: 513 \tTraining Loss: 0.014153 \tValidation Loss: 0.014444\n",
      "Epoch: 514 \tTraining Loss: 0.014149 \tValidation Loss: 0.014445\n",
      "Epoch: 515 \tTraining Loss: 0.014138 \tValidation Loss: 0.014441\n",
      "Epoch: 516 \tTraining Loss: 0.014155 \tValidation Loss: 0.014438\n",
      "Epoch: 517 \tTraining Loss: 0.014139 \tValidation Loss: 0.014436\n",
      "Validation loss decreased (0.014438 --> 0.014436).  Saving model ...\n",
      "Epoch: 518 \tTraining Loss: 0.014115 \tValidation Loss: 0.014441\n",
      "Epoch: 519 \tTraining Loss: 0.014139 \tValidation Loss: 0.014442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 520 \tTraining Loss: 0.014138 \tValidation Loss: 0.014439\n",
      "Epoch: 521 \tTraining Loss: 0.014145 \tValidation Loss: 0.014442\n",
      "Epoch: 522 \tTraining Loss: 0.014133 \tValidation Loss: 0.014440\n",
      "Epoch: 523 \tTraining Loss: 0.014147 \tValidation Loss: 0.014438\n",
      "Epoch: 524 \tTraining Loss: 0.014126 \tValidation Loss: 0.014439\n",
      "Epoch: 525 \tTraining Loss: 0.014142 \tValidation Loss: 0.014438\n",
      "Epoch: 526 \tTraining Loss: 0.014136 \tValidation Loss: 0.014440\n",
      "Epoch: 527 \tTraining Loss: 0.014124 \tValidation Loss: 0.014439\n",
      "Epoch: 528 \tTraining Loss: 0.014126 \tValidation Loss: 0.014440\n",
      "Epoch: 529 \tTraining Loss: 0.014128 \tValidation Loss: 0.014440\n",
      "Epoch: 530 \tTraining Loss: 0.014119 \tValidation Loss: 0.014438\n",
      "Epoch: 531 \tTraining Loss: 0.014121 \tValidation Loss: 0.014435\n",
      "Validation loss decreased (0.014436 --> 0.014435).  Saving model ...\n",
      "Epoch: 532 \tTraining Loss: 0.014113 \tValidation Loss: 0.014442\n",
      "Epoch: 533 \tTraining Loss: 0.014127 \tValidation Loss: 0.014444\n",
      "Epoch: 534 \tTraining Loss: 0.014107 \tValidation Loss: 0.014436\n",
      "Epoch: 535 \tTraining Loss: 0.014116 \tValidation Loss: 0.014439\n",
      "Epoch: 536 \tTraining Loss: 0.014107 \tValidation Loss: 0.014439\n",
      "Epoch: 537 \tTraining Loss: 0.014106 \tValidation Loss: 0.014437\n",
      "Epoch: 538 \tTraining Loss: 0.014120 \tValidation Loss: 0.014436\n",
      "Epoch: 539 \tTraining Loss: 0.014108 \tValidation Loss: 0.014439\n",
      "Epoch: 540 \tTraining Loss: 0.014116 \tValidation Loss: 0.014442\n",
      "Epoch: 541 \tTraining Loss: 0.014113 \tValidation Loss: 0.014442\n",
      "Epoch: 542 \tTraining Loss: 0.014106 \tValidation Loss: 0.014438\n",
      "Epoch: 543 \tTraining Loss: 0.014110 \tValidation Loss: 0.014437\n",
      "Epoch: 544 \tTraining Loss: 0.014108 \tValidation Loss: 0.014436\n",
      "Epoch: 545 \tTraining Loss: 0.014111 \tValidation Loss: 0.014438\n",
      "Epoch: 546 \tTraining Loss: 0.014110 \tValidation Loss: 0.014437\n",
      "Epoch: 547 \tTraining Loss: 0.014101 \tValidation Loss: 0.014439\n",
      "Epoch: 548 \tTraining Loss: 0.014102 \tValidation Loss: 0.014436\n",
      "Epoch: 549 \tTraining Loss: 0.014104 \tValidation Loss: 0.014434\n",
      "Validation loss decreased (0.014435 --> 0.014434).  Saving model ...\n",
      "Epoch: 550 \tTraining Loss: 0.014102 \tValidation Loss: 0.014441\n",
      "Epoch: 551 \tTraining Loss: 0.014107 \tValidation Loss: 0.014433\n",
      "Validation loss decreased (0.014434 --> 0.014433).  Saving model ...\n",
      "Epoch: 552 \tTraining Loss: 0.014089 \tValidation Loss: 0.014433\n",
      "Validation loss decreased (0.014433 --> 0.014433).  Saving model ...\n",
      "Epoch: 553 \tTraining Loss: 0.014112 \tValidation Loss: 0.014435\n",
      "Epoch: 554 \tTraining Loss: 0.014087 \tValidation Loss: 0.014437\n",
      "Epoch: 555 \tTraining Loss: 0.014087 \tValidation Loss: 0.014433\n",
      "Validation loss decreased (0.014433 --> 0.014433).  Saving model ...\n",
      "Epoch: 556 \tTraining Loss: 0.014088 \tValidation Loss: 0.014439\n",
      "Epoch: 557 \tTraining Loss: 0.014083 \tValidation Loss: 0.014437\n",
      "Epoch: 558 \tTraining Loss: 0.014101 \tValidation Loss: 0.014434\n",
      "Epoch: 559 \tTraining Loss: 0.014080 \tValidation Loss: 0.014437\n",
      "Epoch: 560 \tTraining Loss: 0.014071 \tValidation Loss: 0.014432\n",
      "Validation loss decreased (0.014433 --> 0.014432).  Saving model ...\n",
      "Epoch: 561 \tTraining Loss: 0.014083 \tValidation Loss: 0.014436\n",
      "Epoch: 562 \tTraining Loss: 0.014092 \tValidation Loss: 0.014439\n",
      "Epoch: 563 \tTraining Loss: 0.014084 \tValidation Loss: 0.014435\n",
      "Epoch: 564 \tTraining Loss: 0.014095 \tValidation Loss: 0.014438\n",
      "Epoch: 565 \tTraining Loss: 0.014091 \tValidation Loss: 0.014434\n",
      "Epoch: 566 \tTraining Loss: 0.014076 \tValidation Loss: 0.014435\n",
      "Epoch: 567 \tTraining Loss: 0.014094 \tValidation Loss: 0.014435\n",
      "Epoch: 568 \tTraining Loss: 0.014087 \tValidation Loss: 0.014436\n",
      "Epoch: 569 \tTraining Loss: 0.014090 \tValidation Loss: 0.014437\n",
      "Epoch: 570 \tTraining Loss: 0.014099 \tValidation Loss: 0.014434\n",
      "Epoch: 571 \tTraining Loss: 0.014074 \tValidation Loss: 0.014442\n",
      "Epoch: 572 \tTraining Loss: 0.014070 \tValidation Loss: 0.014439\n",
      "Epoch: 573 \tTraining Loss: 0.014081 \tValidation Loss: 0.014434\n",
      "Epoch: 574 \tTraining Loss: 0.014069 \tValidation Loss: 0.014436\n",
      "Epoch: 575 \tTraining Loss: 0.014076 \tValidation Loss: 0.014432\n",
      "Validation loss decreased (0.014432 --> 0.014432).  Saving model ...\n",
      "Epoch: 576 \tTraining Loss: 0.014081 \tValidation Loss: 0.014433\n",
      "Epoch: 577 \tTraining Loss: 0.014075 \tValidation Loss: 0.014434\n",
      "Epoch: 578 \tTraining Loss: 0.014067 \tValidation Loss: 0.014436\n",
      "Epoch: 579 \tTraining Loss: 0.014069 \tValidation Loss: 0.014436\n",
      "Epoch: 580 \tTraining Loss: 0.014050 \tValidation Loss: 0.014433\n",
      "Epoch: 581 \tTraining Loss: 0.014059 \tValidation Loss: 0.014434\n",
      "Epoch: 582 \tTraining Loss: 0.014059 \tValidation Loss: 0.014434\n",
      "Epoch: 583 \tTraining Loss: 0.014067 \tValidation Loss: 0.014437\n",
      "Epoch: 584 \tTraining Loss: 0.014042 \tValidation Loss: 0.014433\n",
      "Epoch: 585 \tTraining Loss: 0.014067 \tValidation Loss: 0.014430\n",
      "Validation loss decreased (0.014432 --> 0.014430).  Saving model ...\n",
      "Epoch: 586 \tTraining Loss: 0.014046 \tValidation Loss: 0.014433\n",
      "Epoch: 587 \tTraining Loss: 0.014059 \tValidation Loss: 0.014434\n",
      "Epoch: 588 \tTraining Loss: 0.014070 \tValidation Loss: 0.014437\n",
      "Epoch: 589 \tTraining Loss: 0.014051 \tValidation Loss: 0.014435\n",
      "Epoch: 590 \tTraining Loss: 0.014062 \tValidation Loss: 0.014435\n",
      "Epoch: 591 \tTraining Loss: 0.014056 \tValidation Loss: 0.014434\n",
      "Epoch: 592 \tTraining Loss: 0.014054 \tValidation Loss: 0.014433\n",
      "Epoch: 593 \tTraining Loss: 0.014051 \tValidation Loss: 0.014437\n",
      "Epoch: 594 \tTraining Loss: 0.014045 \tValidation Loss: 0.014431\n",
      "Epoch: 595 \tTraining Loss: 0.014046 \tValidation Loss: 0.014429\n",
      "Validation loss decreased (0.014430 --> 0.014429).  Saving model ...\n",
      "Epoch: 596 \tTraining Loss: 0.014064 \tValidation Loss: 0.014428\n",
      "Validation loss decreased (0.014429 --> 0.014428).  Saving model ...\n",
      "Epoch: 597 \tTraining Loss: 0.014071 \tValidation Loss: 0.014437\n",
      "Epoch: 598 \tTraining Loss: 0.014053 \tValidation Loss: 0.014432\n",
      "Epoch: 599 \tTraining Loss: 0.014054 \tValidation Loss: 0.014428\n",
      "Validation loss decreased (0.014428 --> 0.014428).  Saving model ...\n",
      "Epoch: 600 \tTraining Loss: 0.014047 \tValidation Loss: 0.014430\n",
      "Epoch: 601 \tTraining Loss: 0.014043 \tValidation Loss: 0.014429\n",
      "Epoch: 602 \tTraining Loss: 0.014053 \tValidation Loss: 0.014428\n",
      "Epoch: 603 \tTraining Loss: 0.014020 \tValidation Loss: 0.014432\n",
      "Epoch: 604 \tTraining Loss: 0.014042 \tValidation Loss: 0.014429\n",
      "Epoch: 605 \tTraining Loss: 0.014028 \tValidation Loss: 0.014429\n",
      "Epoch: 606 \tTraining Loss: 0.014036 \tValidation Loss: 0.014439\n",
      "Epoch: 607 \tTraining Loss: 0.014035 \tValidation Loss: 0.014433\n",
      "Epoch: 608 \tTraining Loss: 0.014040 \tValidation Loss: 0.014430\n",
      "Epoch: 609 \tTraining Loss: 0.014025 \tValidation Loss: 0.014428\n",
      "Epoch: 610 \tTraining Loss: 0.014029 \tValidation Loss: 0.014432\n",
      "Epoch: 611 \tTraining Loss: 0.014032 \tValidation Loss: 0.014431\n",
      "Epoch: 612 \tTraining Loss: 0.014036 \tValidation Loss: 0.014425\n",
      "Validation loss decreased (0.014428 --> 0.014425).  Saving model ...\n",
      "Epoch: 613 \tTraining Loss: 0.014010 \tValidation Loss: 0.014431\n",
      "Epoch: 614 \tTraining Loss: 0.014019 \tValidation Loss: 0.014429\n",
      "Epoch: 615 \tTraining Loss: 0.014030 \tValidation Loss: 0.014429\n",
      "Epoch: 616 \tTraining Loss: 0.014032 \tValidation Loss: 0.014439\n",
      "Epoch: 617 \tTraining Loss: 0.014010 \tValidation Loss: 0.014430\n",
      "Epoch: 618 \tTraining Loss: 0.014010 \tValidation Loss: 0.014424\n",
      "Validation loss decreased (0.014425 --> 0.014424).  Saving model ...\n",
      "Epoch: 619 \tTraining Loss: 0.014028 \tValidation Loss: 0.014433\n",
      "Epoch: 620 \tTraining Loss: 0.014011 \tValidation Loss: 0.014427\n",
      "Epoch: 621 \tTraining Loss: 0.014013 \tValidation Loss: 0.014431\n",
      "Epoch: 622 \tTraining Loss: 0.014014 \tValidation Loss: 0.014424\n",
      "Epoch: 623 \tTraining Loss: 0.014027 \tValidation Loss: 0.014424\n",
      "Validation loss decreased (0.014424 --> 0.014424).  Saving model ...\n",
      "Epoch: 624 \tTraining Loss: 0.014019 \tValidation Loss: 0.014431\n",
      "Epoch: 625 \tTraining Loss: 0.014019 \tValidation Loss: 0.014426\n",
      "Epoch: 626 \tTraining Loss: 0.014015 \tValidation Loss: 0.014429\n",
      "Epoch: 627 \tTraining Loss: 0.014010 \tValidation Loss: 0.014426\n",
      "Epoch: 628 \tTraining Loss: 0.014024 \tValidation Loss: 0.014424\n",
      "Validation loss decreased (0.014424 --> 0.014424).  Saving model ...\n",
      "Epoch: 629 \tTraining Loss: 0.014022 \tValidation Loss: 0.014431\n",
      "Epoch: 630 \tTraining Loss: 0.014013 \tValidation Loss: 0.014427\n",
      "Epoch: 631 \tTraining Loss: 0.013999 \tValidation Loss: 0.014429\n",
      "Epoch: 632 \tTraining Loss: 0.014016 \tValidation Loss: 0.014428\n",
      "Epoch: 633 \tTraining Loss: 0.014008 \tValidation Loss: 0.014433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 634 \tTraining Loss: 0.014002 \tValidation Loss: 0.014423\n",
      "Validation loss decreased (0.014424 --> 0.014423).  Saving model ...\n",
      "Epoch: 635 \tTraining Loss: 0.014013 \tValidation Loss: 0.014429\n",
      "Epoch: 636 \tTraining Loss: 0.013990 \tValidation Loss: 0.014422\n",
      "Validation loss decreased (0.014423 --> 0.014422).  Saving model ...\n",
      "Epoch: 637 \tTraining Loss: 0.013988 \tValidation Loss: 0.014428\n",
      "Epoch: 638 \tTraining Loss: 0.014013 \tValidation Loss: 0.014424\n",
      "Epoch: 639 \tTraining Loss: 0.013989 \tValidation Loss: 0.014427\n",
      "Epoch: 640 \tTraining Loss: 0.013993 \tValidation Loss: 0.014429\n",
      "Epoch: 641 \tTraining Loss: 0.013984 \tValidation Loss: 0.014429\n",
      "Epoch: 642 \tTraining Loss: 0.014005 \tValidation Loss: 0.014428\n",
      "Epoch: 643 \tTraining Loss: 0.014007 \tValidation Loss: 0.014428\n",
      "Epoch: 644 \tTraining Loss: 0.013999 \tValidation Loss: 0.014429\n",
      "Epoch: 645 \tTraining Loss: 0.013964 \tValidation Loss: 0.014423\n",
      "Epoch: 646 \tTraining Loss: 0.013994 \tValidation Loss: 0.014428\n",
      "Epoch: 647 \tTraining Loss: 0.013993 \tValidation Loss: 0.014426\n",
      "Epoch: 648 \tTraining Loss: 0.014000 \tValidation Loss: 0.014421\n",
      "Validation loss decreased (0.014422 --> 0.014421).  Saving model ...\n",
      "Epoch: 649 \tTraining Loss: 0.013995 \tValidation Loss: 0.014434\n",
      "Epoch: 650 \tTraining Loss: 0.013984 \tValidation Loss: 0.014429\n",
      "Epoch: 651 \tTraining Loss: 0.013988 \tValidation Loss: 0.014430\n",
      "Epoch: 652 \tTraining Loss: 0.013987 \tValidation Loss: 0.014420\n",
      "Validation loss decreased (0.014421 --> 0.014420).  Saving model ...\n",
      "Epoch: 653 \tTraining Loss: 0.013992 \tValidation Loss: 0.014424\n",
      "Epoch: 654 \tTraining Loss: 0.013967 \tValidation Loss: 0.014428\n",
      "Epoch: 655 \tTraining Loss: 0.013972 \tValidation Loss: 0.014427\n",
      "Epoch: 656 \tTraining Loss: 0.013967 \tValidation Loss: 0.014424\n",
      "Epoch: 657 \tTraining Loss: 0.013986 \tValidation Loss: 0.014430\n",
      "Epoch: 658 \tTraining Loss: 0.013976 \tValidation Loss: 0.014428\n",
      "Epoch: 659 \tTraining Loss: 0.013976 \tValidation Loss: 0.014424\n",
      "Epoch: 660 \tTraining Loss: 0.013968 \tValidation Loss: 0.014426\n",
      "Epoch: 661 \tTraining Loss: 0.013971 \tValidation Loss: 0.014418\n",
      "Validation loss decreased (0.014420 --> 0.014418).  Saving model ...\n",
      "Epoch: 662 \tTraining Loss: 0.013981 \tValidation Loss: 0.014428\n",
      "Epoch: 663 \tTraining Loss: 0.013965 \tValidation Loss: 0.014423\n",
      "Epoch: 664 \tTraining Loss: 0.013961 \tValidation Loss: 0.014422\n",
      "Epoch: 665 \tTraining Loss: 0.013977 \tValidation Loss: 0.014422\n",
      "Epoch: 666 \tTraining Loss: 0.013951 \tValidation Loss: 0.014426\n",
      "Epoch: 667 \tTraining Loss: 0.013969 \tValidation Loss: 0.014426\n",
      "Epoch: 668 \tTraining Loss: 0.013977 \tValidation Loss: 0.014427\n",
      "Epoch: 669 \tTraining Loss: 0.013966 \tValidation Loss: 0.014425\n",
      "Epoch: 670 \tTraining Loss: 0.013949 \tValidation Loss: 0.014421\n",
      "Epoch: 671 \tTraining Loss: 0.013964 \tValidation Loss: 0.014430\n",
      "Epoch: 672 \tTraining Loss: 0.013969 \tValidation Loss: 0.014429\n",
      "Epoch: 673 \tTraining Loss: 0.013945 \tValidation Loss: 0.014427\n",
      "Epoch: 674 \tTraining Loss: 0.013966 \tValidation Loss: 0.014434\n",
      "Epoch: 675 \tTraining Loss: 0.013953 \tValidation Loss: 0.014430\n",
      "Epoch: 676 \tTraining Loss: 0.013960 \tValidation Loss: 0.014429\n",
      "Epoch: 677 \tTraining Loss: 0.013957 \tValidation Loss: 0.014424\n",
      "Epoch: 678 \tTraining Loss: 0.013948 \tValidation Loss: 0.014424\n",
      "Epoch: 679 \tTraining Loss: 0.013946 \tValidation Loss: 0.014425\n",
      "Epoch: 680 \tTraining Loss: 0.013954 \tValidation Loss: 0.014425\n",
      "Epoch: 681 \tTraining Loss: 0.013956 \tValidation Loss: 0.014427\n",
      "Epoch: 682 \tTraining Loss: 0.013947 \tValidation Loss: 0.014420\n",
      "Epoch: 683 \tTraining Loss: 0.013943 \tValidation Loss: 0.014421\n",
      "Epoch: 684 \tTraining Loss: 0.013937 \tValidation Loss: 0.014421\n",
      "Epoch: 685 \tTraining Loss: 0.013953 \tValidation Loss: 0.014418\n",
      "Validation loss decreased (0.014418 --> 0.014418).  Saving model ...\n",
      "Epoch: 686 \tTraining Loss: 0.013959 \tValidation Loss: 0.014426\n",
      "Epoch: 687 \tTraining Loss: 0.013940 \tValidation Loss: 0.014428\n",
      "Epoch: 688 \tTraining Loss: 0.013939 \tValidation Loss: 0.014423\n",
      "Epoch: 689 \tTraining Loss: 0.013951 \tValidation Loss: 0.014421\n",
      "Epoch: 690 \tTraining Loss: 0.013939 \tValidation Loss: 0.014421\n",
      "Epoch: 691 \tTraining Loss: 0.013946 \tValidation Loss: 0.014423\n",
      "Epoch: 692 \tTraining Loss: 0.013939 \tValidation Loss: 0.014424\n",
      "Epoch: 693 \tTraining Loss: 0.013942 \tValidation Loss: 0.014419\n",
      "Epoch: 694 \tTraining Loss: 0.013941 \tValidation Loss: 0.014423\n",
      "Epoch: 695 \tTraining Loss: 0.013947 \tValidation Loss: 0.014428\n",
      "Epoch: 696 \tTraining Loss: 0.013935 \tValidation Loss: 0.014428\n",
      "Epoch: 697 \tTraining Loss: 0.013920 \tValidation Loss: 0.014417\n",
      "Validation loss decreased (0.014418 --> 0.014417).  Saving model ...\n",
      "Epoch: 698 \tTraining Loss: 0.013935 \tValidation Loss: 0.014423\n",
      "Epoch: 699 \tTraining Loss: 0.013919 \tValidation Loss: 0.014420\n",
      "Epoch: 700 \tTraining Loss: 0.013932 \tValidation Loss: 0.014424\n",
      "Epoch: 701 \tTraining Loss: 0.013923 \tValidation Loss: 0.014421\n",
      "Epoch: 702 \tTraining Loss: 0.013926 \tValidation Loss: 0.014421\n",
      "Epoch: 703 \tTraining Loss: 0.013932 \tValidation Loss: 0.014429\n",
      "Epoch: 704 \tTraining Loss: 0.013911 \tValidation Loss: 0.014421\n",
      "Epoch: 705 \tTraining Loss: 0.013932 \tValidation Loss: 0.014427\n",
      "Epoch: 706 \tTraining Loss: 0.013941 \tValidation Loss: 0.014425\n",
      "Epoch: 707 \tTraining Loss: 0.013930 \tValidation Loss: 0.014424\n",
      "Epoch: 708 \tTraining Loss: 0.013922 \tValidation Loss: 0.014423\n",
      "Epoch: 709 \tTraining Loss: 0.013916 \tValidation Loss: 0.014419\n",
      "Epoch: 710 \tTraining Loss: 0.013920 \tValidation Loss: 0.014420\n",
      "Epoch: 711 \tTraining Loss: 0.013904 \tValidation Loss: 0.014420\n",
      "Epoch: 712 \tTraining Loss: 0.013913 \tValidation Loss: 0.014421\n",
      "Epoch: 713 \tTraining Loss: 0.013914 \tValidation Loss: 0.014423\n",
      "Epoch: 714 \tTraining Loss: 0.013907 \tValidation Loss: 0.014424\n",
      "Epoch: 715 \tTraining Loss: 0.013905 \tValidation Loss: 0.014423\n",
      "Epoch: 716 \tTraining Loss: 0.013917 \tValidation Loss: 0.014420\n",
      "Epoch: 717 \tTraining Loss: 0.013916 \tValidation Loss: 0.014429\n",
      "Epoch: 718 \tTraining Loss: 0.013885 \tValidation Loss: 0.014423\n",
      "Epoch: 719 \tTraining Loss: 0.013902 \tValidation Loss: 0.014423\n",
      "Epoch: 720 \tTraining Loss: 0.013919 \tValidation Loss: 0.014421\n",
      "Epoch: 721 \tTraining Loss: 0.013923 \tValidation Loss: 0.014421\n",
      "Epoch: 722 \tTraining Loss: 0.013915 \tValidation Loss: 0.014424\n",
      "Epoch: 723 \tTraining Loss: 0.013902 \tValidation Loss: 0.014423\n",
      "Epoch: 724 \tTraining Loss: 0.013888 \tValidation Loss: 0.014426\n",
      "Epoch: 725 \tTraining Loss: 0.013886 \tValidation Loss: 0.014424\n",
      "Epoch: 726 \tTraining Loss: 0.013902 \tValidation Loss: 0.014421\n",
      "Epoch: 727 \tTraining Loss: 0.013892 \tValidation Loss: 0.014423\n",
      "Epoch: 728 \tTraining Loss: 0.013892 \tValidation Loss: 0.014420\n",
      "Epoch: 729 \tTraining Loss: 0.013910 \tValidation Loss: 0.014417\n",
      "Validation loss decreased (0.014417 --> 0.014417).  Saving model ...\n",
      "Epoch: 730 \tTraining Loss: 0.013897 \tValidation Loss: 0.014419\n",
      "Epoch: 731 \tTraining Loss: 0.013894 \tValidation Loss: 0.014425\n",
      "Epoch: 732 \tTraining Loss: 0.013887 \tValidation Loss: 0.014419\n",
      "Epoch: 733 \tTraining Loss: 0.013896 \tValidation Loss: 0.014425\n",
      "Epoch: 734 \tTraining Loss: 0.013916 \tValidation Loss: 0.014416\n",
      "Validation loss decreased (0.014417 --> 0.014416).  Saving model ...\n",
      "Epoch: 735 \tTraining Loss: 0.013889 \tValidation Loss: 0.014425\n",
      "Epoch: 736 \tTraining Loss: 0.013895 \tValidation Loss: 0.014423\n",
      "Epoch: 737 \tTraining Loss: 0.013890 \tValidation Loss: 0.014419\n",
      "Epoch: 738 \tTraining Loss: 0.013893 \tValidation Loss: 0.014418\n",
      "Epoch: 739 \tTraining Loss: 0.013887 \tValidation Loss: 0.014421\n",
      "Epoch: 740 \tTraining Loss: 0.013883 \tValidation Loss: 0.014415\n",
      "Validation loss decreased (0.014416 --> 0.014415).  Saving model ...\n",
      "Epoch: 741 \tTraining Loss: 0.013894 \tValidation Loss: 0.014421\n",
      "Epoch: 742 \tTraining Loss: 0.013887 \tValidation Loss: 0.014418\n",
      "Epoch: 743 \tTraining Loss: 0.013874 \tValidation Loss: 0.014419\n",
      "Epoch: 744 \tTraining Loss: 0.013891 \tValidation Loss: 0.014424\n",
      "Epoch: 745 \tTraining Loss: 0.013886 \tValidation Loss: 0.014417\n",
      "Epoch: 746 \tTraining Loss: 0.013874 \tValidation Loss: 0.014416\n",
      "Epoch: 747 \tTraining Loss: 0.013866 \tValidation Loss: 0.014422\n",
      "Epoch: 748 \tTraining Loss: 0.013872 \tValidation Loss: 0.014414\n",
      "Validation loss decreased (0.014415 --> 0.014414).  Saving model ...\n",
      "Epoch: 749 \tTraining Loss: 0.013881 \tValidation Loss: 0.014419\n",
      "Epoch: 750 \tTraining Loss: 0.013871 \tValidation Loss: 0.014417\n",
      "Epoch: 751 \tTraining Loss: 0.013887 \tValidation Loss: 0.014421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 752 \tTraining Loss: 0.013876 \tValidation Loss: 0.014420\n",
      "Epoch: 753 \tTraining Loss: 0.013856 \tValidation Loss: 0.014427\n",
      "Epoch: 754 \tTraining Loss: 0.013874 \tValidation Loss: 0.014416\n",
      "Epoch: 755 \tTraining Loss: 0.013858 \tValidation Loss: 0.014428\n",
      "Epoch: 756 \tTraining Loss: 0.013861 \tValidation Loss: 0.014417\n",
      "Epoch: 757 \tTraining Loss: 0.013854 \tValidation Loss: 0.014422\n",
      "Epoch: 758 \tTraining Loss: 0.013865 \tValidation Loss: 0.014423\n",
      "Epoch: 759 \tTraining Loss: 0.013855 \tValidation Loss: 0.014420\n",
      "Epoch: 760 \tTraining Loss: 0.013877 \tValidation Loss: 0.014421\n",
      "Epoch: 761 \tTraining Loss: 0.013880 \tValidation Loss: 0.014418\n",
      "Epoch: 762 \tTraining Loss: 0.013865 \tValidation Loss: 0.014421\n",
      "Epoch: 763 \tTraining Loss: 0.013860 \tValidation Loss: 0.014419\n",
      "Epoch: 764 \tTraining Loss: 0.013860 \tValidation Loss: 0.014425\n",
      "Epoch: 765 \tTraining Loss: 0.013848 \tValidation Loss: 0.014422\n",
      "Epoch: 766 \tTraining Loss: 0.013855 \tValidation Loss: 0.014417\n",
      "Epoch: 767 \tTraining Loss: 0.013876 \tValidation Loss: 0.014419\n",
      "Epoch: 768 \tTraining Loss: 0.013859 \tValidation Loss: 0.014424\n",
      "Epoch: 769 \tTraining Loss: 0.013839 \tValidation Loss: 0.014417\n",
      "Epoch: 770 \tTraining Loss: 0.013857 \tValidation Loss: 0.014421\n",
      "Epoch: 771 \tTraining Loss: 0.013852 \tValidation Loss: 0.014419\n",
      "Epoch: 772 \tTraining Loss: 0.013853 \tValidation Loss: 0.014422\n",
      "Epoch: 773 \tTraining Loss: 0.013840 \tValidation Loss: 0.014418\n",
      "Epoch: 774 \tTraining Loss: 0.013842 \tValidation Loss: 0.014415\n",
      "Epoch: 775 \tTraining Loss: 0.013854 \tValidation Loss: 0.014420\n",
      "Epoch: 776 \tTraining Loss: 0.013849 \tValidation Loss: 0.014424\n",
      "Epoch: 777 \tTraining Loss: 0.013853 \tValidation Loss: 0.014424\n",
      "Epoch: 778 \tTraining Loss: 0.013851 \tValidation Loss: 0.014420\n",
      "Epoch: 779 \tTraining Loss: 0.013824 \tValidation Loss: 0.014418\n",
      "Epoch: 780 \tTraining Loss: 0.013836 \tValidation Loss: 0.014424\n",
      "Epoch: 781 \tTraining Loss: 0.013830 \tValidation Loss: 0.014423\n",
      "Epoch: 782 \tTraining Loss: 0.013840 \tValidation Loss: 0.014423\n",
      "Epoch: 783 \tTraining Loss: 0.013864 \tValidation Loss: 0.014421\n",
      "Epoch: 784 \tTraining Loss: 0.013823 \tValidation Loss: 0.014419\n",
      "Epoch: 785 \tTraining Loss: 0.013851 \tValidation Loss: 0.014418\n",
      "Epoch: 786 \tTraining Loss: 0.013836 \tValidation Loss: 0.014422\n",
      "Epoch: 787 \tTraining Loss: 0.013837 \tValidation Loss: 0.014418\n",
      "Epoch: 788 \tTraining Loss: 0.013806 \tValidation Loss: 0.014417\n",
      "Epoch: 789 \tTraining Loss: 0.013833 \tValidation Loss: 0.014418\n",
      "Epoch: 790 \tTraining Loss: 0.013841 \tValidation Loss: 0.014424\n",
      "Epoch: 791 \tTraining Loss: 0.013833 \tValidation Loss: 0.014425\n",
      "Epoch: 792 \tTraining Loss: 0.013827 \tValidation Loss: 0.014418\n",
      "Epoch: 793 \tTraining Loss: 0.013833 \tValidation Loss: 0.014418\n",
      "Epoch: 794 \tTraining Loss: 0.013833 \tValidation Loss: 0.014424\n",
      "Epoch: 795 \tTraining Loss: 0.013810 \tValidation Loss: 0.014425\n",
      "Epoch: 796 \tTraining Loss: 0.013828 \tValidation Loss: 0.014422\n",
      "Epoch: 797 \tTraining Loss: 0.013832 \tValidation Loss: 0.014421\n",
      "Epoch: 798 \tTraining Loss: 0.013819 \tValidation Loss: 0.014420\n",
      "Epoch: 799 \tTraining Loss: 0.013828 \tValidation Loss: 0.014423\n",
      "Epoch: 800 \tTraining Loss: 0.013824 \tValidation Loss: 0.014418\n",
      "Epoch: 801 \tTraining Loss: 0.013812 \tValidation Loss: 0.014420\n",
      "Epoch: 802 \tTraining Loss: 0.013825 \tValidation Loss: 0.014419\n",
      "Epoch: 803 \tTraining Loss: 0.013805 \tValidation Loss: 0.014422\n",
      "Epoch: 804 \tTraining Loss: 0.013826 \tValidation Loss: 0.014418\n",
      "Epoch: 805 \tTraining Loss: 0.013812 \tValidation Loss: 0.014418\n",
      "Epoch: 806 \tTraining Loss: 0.013805 \tValidation Loss: 0.014421\n",
      "Epoch: 807 \tTraining Loss: 0.013814 \tValidation Loss: 0.014419\n",
      "Epoch: 808 \tTraining Loss: 0.013811 \tValidation Loss: 0.014418\n",
      "Epoch: 809 \tTraining Loss: 0.013806 \tValidation Loss: 0.014421\n",
      "Epoch: 810 \tTraining Loss: 0.013814 \tValidation Loss: 0.014416\n",
      "Epoch: 811 \tTraining Loss: 0.013817 \tValidation Loss: 0.014422\n",
      "Epoch: 812 \tTraining Loss: 0.013802 \tValidation Loss: 0.014419\n",
      "Epoch: 813 \tTraining Loss: 0.013809 \tValidation Loss: 0.014418\n",
      "Epoch: 814 \tTraining Loss: 0.013809 \tValidation Loss: 0.014418\n",
      "Epoch: 815 \tTraining Loss: 0.013798 \tValidation Loss: 0.014417\n",
      "Epoch: 816 \tTraining Loss: 0.013792 \tValidation Loss: 0.014421\n",
      "Epoch: 817 \tTraining Loss: 0.013795 \tValidation Loss: 0.014416\n",
      "Epoch: 818 \tTraining Loss: 0.013785 \tValidation Loss: 0.014420\n",
      "Epoch: 819 \tTraining Loss: 0.013789 \tValidation Loss: 0.014415\n",
      "Epoch: 820 \tTraining Loss: 0.013793 \tValidation Loss: 0.014420\n",
      "Epoch: 821 \tTraining Loss: 0.013782 \tValidation Loss: 0.014417\n",
      "Epoch: 822 \tTraining Loss: 0.013790 \tValidation Loss: 0.014419\n",
      "Epoch: 823 \tTraining Loss: 0.013790 \tValidation Loss: 0.014423\n",
      "Epoch: 824 \tTraining Loss: 0.013791 \tValidation Loss: 0.014419\n",
      "Epoch: 825 \tTraining Loss: 0.013797 \tValidation Loss: 0.014421\n",
      "Epoch: 826 \tTraining Loss: 0.013805 \tValidation Loss: 0.014418\n",
      "Epoch: 827 \tTraining Loss: 0.013789 \tValidation Loss: 0.014420\n",
      "Epoch: 828 \tTraining Loss: 0.013782 \tValidation Loss: 0.014426\n",
      "Epoch: 829 \tTraining Loss: 0.013788 \tValidation Loss: 0.014418\n",
      "Epoch: 830 \tTraining Loss: 0.013796 \tValidation Loss: 0.014419\n",
      "Epoch: 831 \tTraining Loss: 0.013790 \tValidation Loss: 0.014417\n",
      "Epoch: 832 \tTraining Loss: 0.013780 \tValidation Loss: 0.014426\n",
      "Epoch: 833 \tTraining Loss: 0.013793 \tValidation Loss: 0.014415\n",
      "Epoch: 834 \tTraining Loss: 0.013783 \tValidation Loss: 0.014418\n",
      "Epoch: 835 \tTraining Loss: 0.013783 \tValidation Loss: 0.014428\n",
      "Epoch: 836 \tTraining Loss: 0.013783 \tValidation Loss: 0.014423\n",
      "Epoch: 837 \tTraining Loss: 0.013787 \tValidation Loss: 0.014421\n",
      "Epoch: 838 \tTraining Loss: 0.013782 \tValidation Loss: 0.014423\n",
      "Epoch: 839 \tTraining Loss: 0.013776 \tValidation Loss: 0.014423\n",
      "Epoch: 840 \tTraining Loss: 0.013776 \tValidation Loss: 0.014422\n",
      "Epoch: 841 \tTraining Loss: 0.013768 \tValidation Loss: 0.014425\n",
      "Epoch: 842 \tTraining Loss: 0.013765 \tValidation Loss: 0.014417\n",
      "Epoch: 843 \tTraining Loss: 0.013765 \tValidation Loss: 0.014420\n",
      "Epoch: 844 \tTraining Loss: 0.013775 \tValidation Loss: 0.014420\n",
      "Epoch: 845 \tTraining Loss: 0.013752 \tValidation Loss: 0.014419\n",
      "Epoch: 846 \tTraining Loss: 0.013761 \tValidation Loss: 0.014420\n",
      "Epoch: 847 \tTraining Loss: 0.013757 \tValidation Loss: 0.014420\n",
      "Epoch: 848 \tTraining Loss: 0.013772 \tValidation Loss: 0.014419\n",
      "Epoch: 849 \tTraining Loss: 0.013779 \tValidation Loss: 0.014418\n",
      "Epoch: 850 \tTraining Loss: 0.013761 \tValidation Loss: 0.014420\n",
      "Epoch: 851 \tTraining Loss: 0.013764 \tValidation Loss: 0.014417\n",
      "Epoch: 852 \tTraining Loss: 0.013759 \tValidation Loss: 0.014417\n",
      "Epoch: 853 \tTraining Loss: 0.013764 \tValidation Loss: 0.014417\n",
      "Epoch: 854 \tTraining Loss: 0.013761 \tValidation Loss: 0.014422\n",
      "Epoch: 855 \tTraining Loss: 0.013772 \tValidation Loss: 0.014420\n",
      "Epoch: 856 \tTraining Loss: 0.013751 \tValidation Loss: 0.014423\n",
      "Epoch: 857 \tTraining Loss: 0.013764 \tValidation Loss: 0.014418\n",
      "Epoch: 858 \tTraining Loss: 0.013749 \tValidation Loss: 0.014419\n",
      "Epoch: 859 \tTraining Loss: 0.013750 \tValidation Loss: 0.014422\n",
      "Epoch: 860 \tTraining Loss: 0.013746 \tValidation Loss: 0.014420\n",
      "Epoch: 861 \tTraining Loss: 0.013761 \tValidation Loss: 0.014413\n",
      "Validation loss decreased (0.014414 --> 0.014413).  Saving model ...\n",
      "Epoch: 862 \tTraining Loss: 0.013763 \tValidation Loss: 0.014413\n",
      "Epoch: 863 \tTraining Loss: 0.013743 \tValidation Loss: 0.014416\n",
      "Epoch: 864 \tTraining Loss: 0.013745 \tValidation Loss: 0.014426\n",
      "Epoch: 865 \tTraining Loss: 0.013743 \tValidation Loss: 0.014420\n",
      "Epoch: 866 \tTraining Loss: 0.013734 \tValidation Loss: 0.014422\n",
      "Epoch: 867 \tTraining Loss: 0.013739 \tValidation Loss: 0.014421\n",
      "Epoch: 868 \tTraining Loss: 0.013770 \tValidation Loss: 0.014416\n",
      "Epoch: 869 \tTraining Loss: 0.013769 \tValidation Loss: 0.014418\n",
      "Epoch: 870 \tTraining Loss: 0.013754 \tValidation Loss: 0.014421\n",
      "Epoch: 871 \tTraining Loss: 0.013736 \tValidation Loss: 0.014418\n",
      "Epoch: 872 \tTraining Loss: 0.013722 \tValidation Loss: 0.014421\n",
      "Epoch: 873 \tTraining Loss: 0.013733 \tValidation Loss: 0.014417\n",
      "Epoch: 874 \tTraining Loss: 0.013745 \tValidation Loss: 0.014415\n",
      "Epoch: 875 \tTraining Loss: 0.013744 \tValidation Loss: 0.014415\n",
      "Epoch: 876 \tTraining Loss: 0.013738 \tValidation Loss: 0.014415\n",
      "Epoch: 877 \tTraining Loss: 0.013757 \tValidation Loss: 0.014422\n",
      "Epoch: 878 \tTraining Loss: 0.013735 \tValidation Loss: 0.014420\n",
      "Epoch: 879 \tTraining Loss: 0.013737 \tValidation Loss: 0.014417\n",
      "Epoch: 880 \tTraining Loss: 0.013741 \tValidation Loss: 0.014415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 881 \tTraining Loss: 0.013747 \tValidation Loss: 0.014416\n",
      "Epoch: 882 \tTraining Loss: 0.013730 \tValidation Loss: 0.014418\n",
      "Epoch: 883 \tTraining Loss: 0.013740 \tValidation Loss: 0.014415\n",
      "Epoch: 884 \tTraining Loss: 0.013709 \tValidation Loss: 0.014416\n",
      "Epoch: 885 \tTraining Loss: 0.013730 \tValidation Loss: 0.014417\n",
      "Epoch: 886 \tTraining Loss: 0.013737 \tValidation Loss: 0.014419\n",
      "Epoch: 887 \tTraining Loss: 0.013714 \tValidation Loss: 0.014418\n",
      "Epoch: 888 \tTraining Loss: 0.013724 \tValidation Loss: 0.014417\n",
      "Epoch: 889 \tTraining Loss: 0.013726 \tValidation Loss: 0.014418\n",
      "Epoch: 890 \tTraining Loss: 0.013726 \tValidation Loss: 0.014417\n",
      "Epoch: 891 \tTraining Loss: 0.013726 \tValidation Loss: 0.014415\n",
      "Epoch: 892 \tTraining Loss: 0.013726 \tValidation Loss: 0.014414\n",
      "Epoch: 893 \tTraining Loss: 0.013728 \tValidation Loss: 0.014414\n",
      "Epoch: 894 \tTraining Loss: 0.013736 \tValidation Loss: 0.014414\n",
      "Epoch: 895 \tTraining Loss: 0.013694 \tValidation Loss: 0.014413\n",
      "Epoch: 896 \tTraining Loss: 0.013721 \tValidation Loss: 0.014415\n",
      "Epoch: 897 \tTraining Loss: 0.013715 \tValidation Loss: 0.014416\n",
      "Epoch: 898 \tTraining Loss: 0.013720 \tValidation Loss: 0.014417\n",
      "Epoch: 899 \tTraining Loss: 0.013716 \tValidation Loss: 0.014412\n",
      "Validation loss decreased (0.014413 --> 0.014412).  Saving model ...\n",
      "Epoch: 900 \tTraining Loss: 0.013707 \tValidation Loss: 0.014414\n",
      "Epoch: 901 \tTraining Loss: 0.013715 \tValidation Loss: 0.014411\n",
      "Validation loss decreased (0.014412 --> 0.014411).  Saving model ...\n",
      "Epoch: 902 \tTraining Loss: 0.013717 \tValidation Loss: 0.014414\n",
      "Epoch: 903 \tTraining Loss: 0.013714 \tValidation Loss: 0.014417\n",
      "Epoch: 904 \tTraining Loss: 0.013721 \tValidation Loss: 0.014413\n",
      "Epoch: 905 \tTraining Loss: 0.013720 \tValidation Loss: 0.014419\n",
      "Epoch: 906 \tTraining Loss: 0.013708 \tValidation Loss: 0.014416\n",
      "Epoch: 907 \tTraining Loss: 0.013699 \tValidation Loss: 0.014419\n",
      "Epoch: 908 \tTraining Loss: 0.013692 \tValidation Loss: 0.014415\n",
      "Epoch: 909 \tTraining Loss: 0.013703 \tValidation Loss: 0.014415\n",
      "Epoch: 910 \tTraining Loss: 0.013699 \tValidation Loss: 0.014416\n",
      "Epoch: 911 \tTraining Loss: 0.013704 \tValidation Loss: 0.014419\n",
      "Epoch: 912 \tTraining Loss: 0.013697 \tValidation Loss: 0.014411\n",
      "Epoch: 913 \tTraining Loss: 0.013699 \tValidation Loss: 0.014418\n",
      "Epoch: 914 \tTraining Loss: 0.013695 \tValidation Loss: 0.014418\n",
      "Epoch: 915 \tTraining Loss: 0.013682 \tValidation Loss: 0.014418\n",
      "Epoch: 916 \tTraining Loss: 0.013683 \tValidation Loss: 0.014420\n",
      "Epoch: 917 \tTraining Loss: 0.013700 \tValidation Loss: 0.014413\n",
      "Epoch: 918 \tTraining Loss: 0.013700 \tValidation Loss: 0.014414\n",
      "Epoch: 919 \tTraining Loss: 0.013670 \tValidation Loss: 0.014415\n",
      "Epoch: 920 \tTraining Loss: 0.013692 \tValidation Loss: 0.014410\n",
      "Validation loss decreased (0.014411 --> 0.014410).  Saving model ...\n",
      "Epoch: 921 \tTraining Loss: 0.013687 \tValidation Loss: 0.014415\n",
      "Epoch: 922 \tTraining Loss: 0.013677 \tValidation Loss: 0.014416\n",
      "Epoch: 923 \tTraining Loss: 0.013695 \tValidation Loss: 0.014416\n",
      "Epoch: 924 \tTraining Loss: 0.013672 \tValidation Loss: 0.014415\n",
      "Epoch: 925 \tTraining Loss: 0.013679 \tValidation Loss: 0.014419\n",
      "Epoch: 926 \tTraining Loss: 0.013681 \tValidation Loss: 0.014417\n",
      "Epoch: 927 \tTraining Loss: 0.013681 \tValidation Loss: 0.014417\n",
      "Epoch: 928 \tTraining Loss: 0.013690 \tValidation Loss: 0.014414\n",
      "Epoch: 929 \tTraining Loss: 0.013661 \tValidation Loss: 0.014418\n",
      "Epoch: 930 \tTraining Loss: 0.013671 \tValidation Loss: 0.014416\n",
      "Epoch: 931 \tTraining Loss: 0.013682 \tValidation Loss: 0.014417\n",
      "Epoch: 932 \tTraining Loss: 0.013672 \tValidation Loss: 0.014421\n",
      "Epoch: 933 \tTraining Loss: 0.013675 \tValidation Loss: 0.014420\n",
      "Epoch: 934 \tTraining Loss: 0.013662 \tValidation Loss: 0.014419\n",
      "Epoch: 935 \tTraining Loss: 0.013687 \tValidation Loss: 0.014426\n",
      "Epoch: 936 \tTraining Loss: 0.013656 \tValidation Loss: 0.014426\n",
      "Epoch: 937 \tTraining Loss: 0.013676 \tValidation Loss: 0.014416\n",
      "Epoch: 938 \tTraining Loss: 0.013676 \tValidation Loss: 0.014421\n",
      "Epoch: 939 \tTraining Loss: 0.013668 \tValidation Loss: 0.014421\n",
      "Epoch: 940 \tTraining Loss: 0.013679 \tValidation Loss: 0.014423\n",
      "Epoch: 941 \tTraining Loss: 0.013674 \tValidation Loss: 0.014418\n",
      "Epoch: 942 \tTraining Loss: 0.013664 \tValidation Loss: 0.014414\n",
      "Epoch: 943 \tTraining Loss: 0.013667 \tValidation Loss: 0.014419\n",
      "Epoch: 944 \tTraining Loss: 0.013659 \tValidation Loss: 0.014420\n",
      "Epoch: 945 \tTraining Loss: 0.013675 \tValidation Loss: 0.014419\n",
      "Epoch: 946 \tTraining Loss: 0.013662 \tValidation Loss: 0.014417\n",
      "Epoch: 947 \tTraining Loss: 0.013649 \tValidation Loss: 0.014417\n",
      "Epoch: 948 \tTraining Loss: 0.013654 \tValidation Loss: 0.014419\n",
      "Epoch: 949 \tTraining Loss: 0.013660 \tValidation Loss: 0.014413\n",
      "Epoch: 950 \tTraining Loss: 0.013642 \tValidation Loss: 0.014417\n",
      "Epoch: 951 \tTraining Loss: 0.013626 \tValidation Loss: 0.014415\n",
      "Epoch: 952 \tTraining Loss: 0.013660 \tValidation Loss: 0.014415\n",
      "Epoch: 953 \tTraining Loss: 0.013653 \tValidation Loss: 0.014416\n",
      "Epoch: 954 \tTraining Loss: 0.013656 \tValidation Loss: 0.014420\n",
      "Epoch: 955 \tTraining Loss: 0.013655 \tValidation Loss: 0.014419\n",
      "Epoch: 956 \tTraining Loss: 0.013664 \tValidation Loss: 0.014418\n",
      "Epoch: 957 \tTraining Loss: 0.013654 \tValidation Loss: 0.014415\n",
      "Epoch: 958 \tTraining Loss: 0.013652 \tValidation Loss: 0.014413\n",
      "Epoch: 959 \tTraining Loss: 0.013654 \tValidation Loss: 0.014422\n",
      "Epoch: 960 \tTraining Loss: 0.013641 \tValidation Loss: 0.014421\n",
      "Epoch: 961 \tTraining Loss: 0.013633 \tValidation Loss: 0.014423\n",
      "Epoch: 962 \tTraining Loss: 0.013645 \tValidation Loss: 0.014422\n",
      "Epoch: 963 \tTraining Loss: 0.013652 \tValidation Loss: 0.014424\n",
      "Epoch: 964 \tTraining Loss: 0.013647 \tValidation Loss: 0.014418\n",
      "Epoch: 965 \tTraining Loss: 0.013641 \tValidation Loss: 0.014414\n",
      "Epoch: 966 \tTraining Loss: 0.013633 \tValidation Loss: 0.014418\n",
      "Epoch: 967 \tTraining Loss: 0.013673 \tValidation Loss: 0.014420\n",
      "Epoch: 968 \tTraining Loss: 0.013633 \tValidation Loss: 0.014417\n",
      "Epoch: 969 \tTraining Loss: 0.013648 \tValidation Loss: 0.014420\n",
      "Epoch: 970 \tTraining Loss: 0.013634 \tValidation Loss: 0.014416\n",
      "Epoch: 971 \tTraining Loss: 0.013641 \tValidation Loss: 0.014416\n",
      "Epoch: 972 \tTraining Loss: 0.013625 \tValidation Loss: 0.014415\n",
      "Epoch: 973 \tTraining Loss: 0.013635 \tValidation Loss: 0.014418\n",
      "Epoch: 974 \tTraining Loss: 0.013620 \tValidation Loss: 0.014417\n",
      "Epoch: 975 \tTraining Loss: 0.013634 \tValidation Loss: 0.014419\n",
      "Epoch: 976 \tTraining Loss: 0.013622 \tValidation Loss: 0.014418\n",
      "Epoch: 977 \tTraining Loss: 0.013643 \tValidation Loss: 0.014419\n",
      "Epoch: 978 \tTraining Loss: 0.013624 \tValidation Loss: 0.014417\n",
      "Epoch: 979 \tTraining Loss: 0.013617 \tValidation Loss: 0.014412\n",
      "Epoch: 980 \tTraining Loss: 0.013598 \tValidation Loss: 0.014416\n",
      "Epoch: 981 \tTraining Loss: 0.013619 \tValidation Loss: 0.014421\n",
      "Epoch: 982 \tTraining Loss: 0.013621 \tValidation Loss: 0.014419\n",
      "Epoch: 983 \tTraining Loss: 0.013609 \tValidation Loss: 0.014421\n",
      "Epoch: 984 \tTraining Loss: 0.013613 \tValidation Loss: 0.014422\n",
      "Epoch: 985 \tTraining Loss: 0.013618 \tValidation Loss: 0.014419\n",
      "Epoch: 986 \tTraining Loss: 0.013629 \tValidation Loss: 0.014415\n",
      "Epoch: 987 \tTraining Loss: 0.013625 \tValidation Loss: 0.014418\n",
      "Epoch: 988 \tTraining Loss: 0.013612 \tValidation Loss: 0.014415\n",
      "Epoch: 989 \tTraining Loss: 0.013614 \tValidation Loss: 0.014424\n",
      "Epoch: 990 \tTraining Loss: 0.013610 \tValidation Loss: 0.014420\n",
      "Epoch: 991 \tTraining Loss: 0.013611 \tValidation Loss: 0.014419\n",
      "Epoch: 992 \tTraining Loss: 0.013612 \tValidation Loss: 0.014422\n",
      "Epoch: 993 \tTraining Loss: 0.013635 \tValidation Loss: 0.014417\n",
      "Epoch: 994 \tTraining Loss: 0.013615 \tValidation Loss: 0.014424\n",
      "Epoch: 995 \tTraining Loss: 0.013617 \tValidation Loss: 0.014421\n",
      "Epoch: 996 \tTraining Loss: 0.013598 \tValidation Loss: 0.014416\n",
      "Epoch: 997 \tTraining Loss: 0.013597 \tValidation Loss: 0.014424\n",
      "Epoch: 998 \tTraining Loss: 0.013615 \tValidation Loss: 0.014420\n",
      "Epoch: 999 \tTraining Loss: 0.013612 \tValidation Loss: 0.014415\n",
      "Epoch: 1000 \tTraining Loss: 0.013593 \tValidation Loss: 0.014421\n",
      "Epoch: 1001 \tTraining Loss: 0.013599 \tValidation Loss: 0.014419\n",
      "Epoch: 1002 \tTraining Loss: 0.013608 \tValidation Loss: 0.014424\n",
      "Epoch: 1003 \tTraining Loss: 0.013605 \tValidation Loss: 0.014416\n",
      "Epoch: 1004 \tTraining Loss: 0.013619 \tValidation Loss: 0.014420\n",
      "Epoch: 1005 \tTraining Loss: 0.013602 \tValidation Loss: 0.014414\n",
      "Epoch: 1006 \tTraining Loss: 0.013585 \tValidation Loss: 0.014421\n",
      "Epoch: 1007 \tTraining Loss: 0.013598 \tValidation Loss: 0.014413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1008 \tTraining Loss: 0.013596 \tValidation Loss: 0.014419\n",
      "Epoch: 1009 \tTraining Loss: 0.013595 \tValidation Loss: 0.014415\n",
      "Epoch: 1010 \tTraining Loss: 0.013600 \tValidation Loss: 0.014417\n",
      "Epoch: 1011 \tTraining Loss: 0.013586 \tValidation Loss: 0.014422\n",
      "Epoch: 1012 \tTraining Loss: 0.013574 \tValidation Loss: 0.014419\n",
      "Epoch: 1013 \tTraining Loss: 0.013572 \tValidation Loss: 0.014413\n",
      "Epoch: 1014 \tTraining Loss: 0.013581 \tValidation Loss: 0.014419\n",
      "Epoch: 1015 \tTraining Loss: 0.013597 \tValidation Loss: 0.014418\n",
      "Epoch: 1016 \tTraining Loss: 0.013580 \tValidation Loss: 0.014420\n",
      "Epoch: 1017 \tTraining Loss: 0.013578 \tValidation Loss: 0.014422\n",
      "Epoch: 1018 \tTraining Loss: 0.013589 \tValidation Loss: 0.014422\n",
      "Epoch: 1019 \tTraining Loss: 0.013573 \tValidation Loss: 0.014423\n",
      "Epoch: 1020 \tTraining Loss: 0.013591 \tValidation Loss: 0.014414\n",
      "Epoch: 1021 \tTraining Loss: 0.013579 \tValidation Loss: 0.014415\n",
      "Epoch: 1022 \tTraining Loss: 0.013593 \tValidation Loss: 0.014414\n",
      "Epoch: 1023 \tTraining Loss: 0.013571 \tValidation Loss: 0.014427\n",
      "Epoch: 1024 \tTraining Loss: 0.013601 \tValidation Loss: 0.014421\n",
      "Epoch: 1025 \tTraining Loss: 0.013577 \tValidation Loss: 0.014422\n",
      "Epoch: 1026 \tTraining Loss: 0.013571 \tValidation Loss: 0.014414\n",
      "Epoch: 1027 \tTraining Loss: 0.013574 \tValidation Loss: 0.014424\n",
      "Epoch: 1028 \tTraining Loss: 0.013560 \tValidation Loss: 0.014424\n",
      "Epoch: 1029 \tTraining Loss: 0.013566 \tValidation Loss: 0.014418\n",
      "Epoch: 1030 \tTraining Loss: 0.013575 \tValidation Loss: 0.014420\n",
      "Epoch: 1031 \tTraining Loss: 0.013577 \tValidation Loss: 0.014412\n",
      "Epoch: 1032 \tTraining Loss: 0.013572 \tValidation Loss: 0.014417\n",
      "Epoch: 1033 \tTraining Loss: 0.013579 \tValidation Loss: 0.014415\n",
      "Epoch: 1034 \tTraining Loss: 0.013569 \tValidation Loss: 0.014421\n",
      "Epoch: 1035 \tTraining Loss: 0.013573 \tValidation Loss: 0.014428\n",
      "Epoch: 1036 \tTraining Loss: 0.013583 \tValidation Loss: 0.014418\n",
      "Epoch: 1037 \tTraining Loss: 0.013567 \tValidation Loss: 0.014422\n",
      "Epoch: 1038 \tTraining Loss: 0.013563 \tValidation Loss: 0.014427\n",
      "Epoch: 1039 \tTraining Loss: 0.013572 \tValidation Loss: 0.014417\n",
      "Epoch: 1040 \tTraining Loss: 0.013552 \tValidation Loss: 0.014421\n",
      "Epoch: 1041 \tTraining Loss: 0.013563 \tValidation Loss: 0.014418\n",
      "Epoch: 1042 \tTraining Loss: 0.013580 \tValidation Loss: 0.014419\n",
      "Epoch: 1043 \tTraining Loss: 0.013567 \tValidation Loss: 0.014421\n",
      "Epoch: 1044 \tTraining Loss: 0.013550 \tValidation Loss: 0.014423\n",
      "Epoch: 1045 \tTraining Loss: 0.013560 \tValidation Loss: 0.014421\n",
      "Epoch: 1046 \tTraining Loss: 0.013549 \tValidation Loss: 0.014421\n",
      "Epoch: 1047 \tTraining Loss: 0.013557 \tValidation Loss: 0.014422\n",
      "Epoch: 1048 \tTraining Loss: 0.013561 \tValidation Loss: 0.014425\n",
      "Epoch: 1049 \tTraining Loss: 0.013545 \tValidation Loss: 0.014418\n",
      "Epoch: 1050 \tTraining Loss: 0.013571 \tValidation Loss: 0.014418\n",
      "Epoch: 1051 \tTraining Loss: 0.013562 \tValidation Loss: 0.014421\n",
      "Epoch: 1052 \tTraining Loss: 0.013552 \tValidation Loss: 0.014419\n",
      "Epoch: 1053 \tTraining Loss: 0.013546 \tValidation Loss: 0.014425\n",
      "Epoch: 1054 \tTraining Loss: 0.013545 \tValidation Loss: 0.014426\n",
      "Epoch: 1055 \tTraining Loss: 0.013545 \tValidation Loss: 0.014417\n",
      "Epoch: 1056 \tTraining Loss: 0.013561 \tValidation Loss: 0.014419\n",
      "Epoch: 1057 \tTraining Loss: 0.013543 \tValidation Loss: 0.014424\n",
      "Epoch: 1058 \tTraining Loss: 0.013539 \tValidation Loss: 0.014422\n",
      "Epoch: 1059 \tTraining Loss: 0.013546 \tValidation Loss: 0.014423\n",
      "Epoch: 1060 \tTraining Loss: 0.013552 \tValidation Loss: 0.014423\n",
      "Epoch: 1061 \tTraining Loss: 0.013551 \tValidation Loss: 0.014420\n",
      "Epoch: 1062 \tTraining Loss: 0.013542 \tValidation Loss: 0.014418\n",
      "Epoch: 1063 \tTraining Loss: 0.013534 \tValidation Loss: 0.014420\n",
      "Epoch: 1064 \tTraining Loss: 0.013524 \tValidation Loss: 0.014423\n",
      "Epoch: 1065 \tTraining Loss: 0.013533 \tValidation Loss: 0.014423\n",
      "Epoch: 1066 \tTraining Loss: 0.013550 \tValidation Loss: 0.014418\n",
      "Epoch: 1067 \tTraining Loss: 0.013545 \tValidation Loss: 0.014417\n",
      "Epoch: 1068 \tTraining Loss: 0.013527 \tValidation Loss: 0.014419\n",
      "Epoch: 1069 \tTraining Loss: 0.013545 \tValidation Loss: 0.014418\n",
      "Epoch: 1070 \tTraining Loss: 0.013543 \tValidation Loss: 0.014424\n",
      "Epoch: 1071 \tTraining Loss: 0.013531 \tValidation Loss: 0.014426\n",
      "Epoch: 1072 \tTraining Loss: 0.013534 \tValidation Loss: 0.014419\n",
      "Epoch: 1073 \tTraining Loss: 0.013535 \tValidation Loss: 0.014419\n",
      "Epoch: 1074 \tTraining Loss: 0.013547 \tValidation Loss: 0.014420\n",
      "Epoch: 1075 \tTraining Loss: 0.013526 \tValidation Loss: 0.014420\n",
      "Epoch: 1076 \tTraining Loss: 0.013553 \tValidation Loss: 0.014418\n",
      "Epoch: 1077 \tTraining Loss: 0.013525 \tValidation Loss: 0.014418\n",
      "Epoch: 1078 \tTraining Loss: 0.013515 \tValidation Loss: 0.014414\n",
      "Epoch: 1079 \tTraining Loss: 0.013543 \tValidation Loss: 0.014418\n",
      "Epoch: 1080 \tTraining Loss: 0.013510 \tValidation Loss: 0.014416\n",
      "Epoch: 1081 \tTraining Loss: 0.013510 \tValidation Loss: 0.014414\n",
      "Epoch: 1082 \tTraining Loss: 0.013517 \tValidation Loss: 0.014417\n",
      "Epoch: 1083 \tTraining Loss: 0.013525 \tValidation Loss: 0.014418\n",
      "Epoch: 1084 \tTraining Loss: 0.013527 \tValidation Loss: 0.014421\n",
      "Epoch: 1085 \tTraining Loss: 0.013510 \tValidation Loss: 0.014422\n",
      "Epoch: 1086 \tTraining Loss: 0.013507 \tValidation Loss: 0.014417\n",
      "Epoch: 1087 \tTraining Loss: 0.013516 \tValidation Loss: 0.014414\n",
      "Epoch: 1088 \tTraining Loss: 0.013495 \tValidation Loss: 0.014416\n",
      "Epoch: 1089 \tTraining Loss: 0.013517 \tValidation Loss: 0.014417\n",
      "Epoch: 1090 \tTraining Loss: 0.013504 \tValidation Loss: 0.014418\n",
      "Epoch: 1091 \tTraining Loss: 0.013510 \tValidation Loss: 0.014416\n",
      "Epoch: 1092 \tTraining Loss: 0.013510 \tValidation Loss: 0.014428\n",
      "Epoch: 1093 \tTraining Loss: 0.013517 \tValidation Loss: 0.014414\n",
      "Epoch: 1094 \tTraining Loss: 0.013515 \tValidation Loss: 0.014412\n",
      "Epoch: 1095 \tTraining Loss: 0.013518 \tValidation Loss: 0.014413\n",
      "Epoch: 1096 \tTraining Loss: 0.013520 \tValidation Loss: 0.014421\n",
      "Epoch: 1097 \tTraining Loss: 0.013499 \tValidation Loss: 0.014421\n",
      "Epoch: 1098 \tTraining Loss: 0.013491 \tValidation Loss: 0.014424\n",
      "Epoch: 1099 \tTraining Loss: 0.013507 \tValidation Loss: 0.014417\n",
      "Epoch: 1100 \tTraining Loss: 0.013504 \tValidation Loss: 0.014425\n",
      "Epoch: 1101 \tTraining Loss: 0.013520 \tValidation Loss: 0.014428\n",
      "Epoch: 1102 \tTraining Loss: 0.013514 \tValidation Loss: 0.014422\n",
      "Epoch: 1103 \tTraining Loss: 0.013495 \tValidation Loss: 0.014424\n",
      "Epoch: 1104 \tTraining Loss: 0.013513 \tValidation Loss: 0.014421\n",
      "Epoch: 1105 \tTraining Loss: 0.013500 \tValidation Loss: 0.014417\n",
      "Epoch: 1106 \tTraining Loss: 0.013493 \tValidation Loss: 0.014421\n",
      "Epoch: 1107 \tTraining Loss: 0.013490 \tValidation Loss: 0.014423\n",
      "Epoch: 1108 \tTraining Loss: 0.013483 \tValidation Loss: 0.014429\n",
      "Epoch: 1109 \tTraining Loss: 0.013504 \tValidation Loss: 0.014421\n",
      "Epoch: 1110 \tTraining Loss: 0.013483 \tValidation Loss: 0.014424\n",
      "Epoch: 1111 \tTraining Loss: 0.013496 \tValidation Loss: 0.014417\n",
      "Epoch: 1112 \tTraining Loss: 0.013494 \tValidation Loss: 0.014420\n",
      "Epoch: 1113 \tTraining Loss: 0.013477 \tValidation Loss: 0.014417\n",
      "Epoch: 1114 \tTraining Loss: 0.013509 \tValidation Loss: 0.014425\n",
      "Epoch: 1115 \tTraining Loss: 0.013500 \tValidation Loss: 0.014417\n",
      "Epoch: 1116 \tTraining Loss: 0.013483 \tValidation Loss: 0.014420\n",
      "Epoch: 1117 \tTraining Loss: 0.013488 \tValidation Loss: 0.014415\n",
      "Epoch: 1118 \tTraining Loss: 0.013479 \tValidation Loss: 0.014421\n",
      "Epoch: 1119 \tTraining Loss: 0.013488 \tValidation Loss: 0.014427\n",
      "Epoch: 1120 \tTraining Loss: 0.013497 \tValidation Loss: 0.014425\n",
      "Epoch: 1121 \tTraining Loss: 0.013480 \tValidation Loss: 0.014421\n",
      "Epoch: 1122 \tTraining Loss: 0.013479 \tValidation Loss: 0.014418\n",
      "Epoch: 1123 \tTraining Loss: 0.013466 \tValidation Loss: 0.014421\n",
      "Epoch: 1124 \tTraining Loss: 0.013481 \tValidation Loss: 0.014418\n",
      "Epoch: 1125 \tTraining Loss: 0.013475 \tValidation Loss: 0.014424\n",
      "Epoch: 1126 \tTraining Loss: 0.013473 \tValidation Loss: 0.014419\n",
      "Epoch: 1127 \tTraining Loss: 0.013469 \tValidation Loss: 0.014427\n",
      "Epoch: 1128 \tTraining Loss: 0.013456 \tValidation Loss: 0.014418\n",
      "Epoch: 1129 \tTraining Loss: 0.013462 \tValidation Loss: 0.014417\n",
      "Epoch: 1130 \tTraining Loss: 0.013476 \tValidation Loss: 0.014420\n",
      "Epoch: 1131 \tTraining Loss: 0.013463 \tValidation Loss: 0.014427\n",
      "Epoch: 1132 \tTraining Loss: 0.013463 \tValidation Loss: 0.014428\n",
      "Epoch: 1133 \tTraining Loss: 0.013462 \tValidation Loss: 0.014424\n",
      "Epoch: 1134 \tTraining Loss: 0.013484 \tValidation Loss: 0.014421\n",
      "Epoch: 1135 \tTraining Loss: 0.013466 \tValidation Loss: 0.014424\n",
      "Epoch: 1136 \tTraining Loss: 0.013481 \tValidation Loss: 0.014421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1137 \tTraining Loss: 0.013466 \tValidation Loss: 0.014419\n",
      "Epoch: 1138 \tTraining Loss: 0.013464 \tValidation Loss: 0.014427\n",
      "Epoch: 1139 \tTraining Loss: 0.013466 \tValidation Loss: 0.014425\n",
      "Epoch: 1140 \tTraining Loss: 0.013460 \tValidation Loss: 0.014427\n",
      "Epoch: 1141 \tTraining Loss: 0.013457 \tValidation Loss: 0.014421\n",
      "Epoch: 1142 \tTraining Loss: 0.013459 \tValidation Loss: 0.014422\n",
      "Epoch: 1143 \tTraining Loss: 0.013476 \tValidation Loss: 0.014425\n",
      "Epoch: 1144 \tTraining Loss: 0.013439 \tValidation Loss: 0.014423\n",
      "Epoch: 1145 \tTraining Loss: 0.013458 \tValidation Loss: 0.014428\n",
      "Epoch: 1146 \tTraining Loss: 0.013440 \tValidation Loss: 0.014418\n",
      "Epoch: 1147 \tTraining Loss: 0.013464 \tValidation Loss: 0.014420\n",
      "Epoch: 1148 \tTraining Loss: 0.013462 \tValidation Loss: 0.014416\n",
      "Epoch: 1149 \tTraining Loss: 0.013451 \tValidation Loss: 0.014428\n",
      "Epoch: 1150 \tTraining Loss: 0.013470 \tValidation Loss: 0.014423\n",
      "Epoch: 1151 \tTraining Loss: 0.013428 \tValidation Loss: 0.014421\n",
      "Epoch: 1152 \tTraining Loss: 0.013448 \tValidation Loss: 0.014419\n",
      "Epoch: 1153 \tTraining Loss: 0.013474 \tValidation Loss: 0.014423\n",
      "Epoch: 1154 \tTraining Loss: 0.013451 \tValidation Loss: 0.014425\n",
      "Epoch: 1155 \tTraining Loss: 0.013441 \tValidation Loss: 0.014413\n",
      "Epoch: 1156 \tTraining Loss: 0.013449 \tValidation Loss: 0.014422\n",
      "Epoch: 1157 \tTraining Loss: 0.013453 \tValidation Loss: 0.014425\n",
      "Epoch: 1158 \tTraining Loss: 0.013425 \tValidation Loss: 0.014425\n",
      "Epoch: 1159 \tTraining Loss: 0.013454 \tValidation Loss: 0.014420\n",
      "Epoch: 1160 \tTraining Loss: 0.013445 \tValidation Loss: 0.014426\n",
      "Epoch: 1161 \tTraining Loss: 0.013446 \tValidation Loss: 0.014423\n",
      "Epoch: 1162 \tTraining Loss: 0.013436 \tValidation Loss: 0.014416\n",
      "Epoch: 1163 \tTraining Loss: 0.013428 \tValidation Loss: 0.014423\n",
      "Epoch: 1164 \tTraining Loss: 0.013437 \tValidation Loss: 0.014428\n",
      "Epoch: 1165 \tTraining Loss: 0.013437 \tValidation Loss: 0.014419\n",
      "Epoch: 1166 \tTraining Loss: 0.013437 \tValidation Loss: 0.014422\n",
      "Epoch: 1167 \tTraining Loss: 0.013437 \tValidation Loss: 0.014425\n",
      "Epoch: 1168 \tTraining Loss: 0.013426 \tValidation Loss: 0.014418\n",
      "Epoch: 1169 \tTraining Loss: 0.013437 \tValidation Loss: 0.014422\n",
      "Epoch: 1170 \tTraining Loss: 0.013426 \tValidation Loss: 0.014421\n",
      "Epoch: 1171 \tTraining Loss: 0.013417 \tValidation Loss: 0.014422\n",
      "Epoch: 1172 \tTraining Loss: 0.013427 \tValidation Loss: 0.014420\n",
      "Epoch: 1173 \tTraining Loss: 0.013431 \tValidation Loss: 0.014419\n",
      "Epoch: 1174 \tTraining Loss: 0.013415 \tValidation Loss: 0.014423\n",
      "Epoch: 1175 \tTraining Loss: 0.013417 \tValidation Loss: 0.014426\n",
      "Epoch: 1176 \tTraining Loss: 0.013425 \tValidation Loss: 0.014428\n",
      "Epoch: 1177 \tTraining Loss: 0.013414 \tValidation Loss: 0.014426\n",
      "Epoch: 1178 \tTraining Loss: 0.013419 \tValidation Loss: 0.014427\n",
      "Epoch: 1179 \tTraining Loss: 0.013423 \tValidation Loss: 0.014423\n",
      "Epoch: 1180 \tTraining Loss: 0.013424 \tValidation Loss: 0.014424\n",
      "Epoch: 1181 \tTraining Loss: 0.013431 \tValidation Loss: 0.014428\n",
      "Epoch: 1182 \tTraining Loss: 0.013416 \tValidation Loss: 0.014421\n",
      "Epoch: 1183 \tTraining Loss: 0.013435 \tValidation Loss: 0.014429\n",
      "Epoch: 1184 \tTraining Loss: 0.013426 \tValidation Loss: 0.014423\n",
      "Epoch: 1185 \tTraining Loss: 0.013413 \tValidation Loss: 0.014424\n",
      "Epoch: 1186 \tTraining Loss: 0.013424 \tValidation Loss: 0.014433\n",
      "Epoch: 1187 \tTraining Loss: 0.013425 \tValidation Loss: 0.014430\n",
      "Epoch: 1188 \tTraining Loss: 0.013402 \tValidation Loss: 0.014433\n",
      "Epoch: 1189 \tTraining Loss: 0.013419 \tValidation Loss: 0.014425\n",
      "Epoch: 1190 \tTraining Loss: 0.013416 \tValidation Loss: 0.014431\n",
      "Epoch: 1191 \tTraining Loss: 0.013418 \tValidation Loss: 0.014427\n",
      "Epoch: 1192 \tTraining Loss: 0.013394 \tValidation Loss: 0.014425\n",
      "Epoch: 1193 \tTraining Loss: 0.013416 \tValidation Loss: 0.014426\n",
      "Epoch: 1194 \tTraining Loss: 0.013417 \tValidation Loss: 0.014424\n",
      "Epoch: 1195 \tTraining Loss: 0.013442 \tValidation Loss: 0.014420\n",
      "Epoch: 1196 \tTraining Loss: 0.013393 \tValidation Loss: 0.014427\n",
      "Epoch: 1197 \tTraining Loss: 0.013432 \tValidation Loss: 0.014420\n",
      "Epoch: 1198 \tTraining Loss: 0.013389 \tValidation Loss: 0.014425\n",
      "Epoch: 1199 \tTraining Loss: 0.013426 \tValidation Loss: 0.014423\n",
      "Epoch: 1200 \tTraining Loss: 0.013413 \tValidation Loss: 0.014424\n",
      "Epoch: 1201 \tTraining Loss: 0.013403 \tValidation Loss: 0.014426\n",
      "Epoch: 1202 \tTraining Loss: 0.013415 \tValidation Loss: 0.014427\n",
      "Epoch: 1203 \tTraining Loss: 0.013401 \tValidation Loss: 0.014423\n",
      "Epoch: 1204 \tTraining Loss: 0.013399 \tValidation Loss: 0.014435\n",
      "Epoch: 1205 \tTraining Loss: 0.013392 \tValidation Loss: 0.014418\n",
      "Epoch: 1206 \tTraining Loss: 0.013405 \tValidation Loss: 0.014424\n",
      "Epoch: 1207 \tTraining Loss: 0.013411 \tValidation Loss: 0.014433\n",
      "Epoch: 1208 \tTraining Loss: 0.013389 \tValidation Loss: 0.014422\n",
      "Epoch: 1209 \tTraining Loss: 0.013415 \tValidation Loss: 0.014428\n",
      "Epoch: 1210 \tTraining Loss: 0.013404 \tValidation Loss: 0.014430\n",
      "Epoch: 1211 \tTraining Loss: 0.013403 \tValidation Loss: 0.014429\n",
      "Epoch: 1212 \tTraining Loss: 0.013373 \tValidation Loss: 0.014427\n",
      "Epoch: 1213 \tTraining Loss: 0.013387 \tValidation Loss: 0.014428\n",
      "Epoch: 1214 \tTraining Loss: 0.013395 \tValidation Loss: 0.014423\n",
      "Epoch: 1215 \tTraining Loss: 0.013402 \tValidation Loss: 0.014431\n",
      "Epoch: 1216 \tTraining Loss: 0.013382 \tValidation Loss: 0.014427\n",
      "Epoch: 1217 \tTraining Loss: 0.013385 \tValidation Loss: 0.014423\n",
      "Epoch: 1218 \tTraining Loss: 0.013371 \tValidation Loss: 0.014432\n",
      "Epoch: 1219 \tTraining Loss: 0.013402 \tValidation Loss: 0.014428\n",
      "Epoch: 1220 \tTraining Loss: 0.013394 \tValidation Loss: 0.014427\n",
      "Epoch: 1221 \tTraining Loss: 0.013392 \tValidation Loss: 0.014430\n",
      "Epoch: 1222 \tTraining Loss: 0.013387 \tValidation Loss: 0.014421\n",
      "Epoch: 1223 \tTraining Loss: 0.013385 \tValidation Loss: 0.014421\n",
      "Epoch: 1224 \tTraining Loss: 0.013379 \tValidation Loss: 0.014421\n",
      "Epoch: 1225 \tTraining Loss: 0.013402 \tValidation Loss: 0.014423\n",
      "Epoch: 1226 \tTraining Loss: 0.013365 \tValidation Loss: 0.014425\n",
      "Epoch: 1227 \tTraining Loss: 0.013392 \tValidation Loss: 0.014427\n",
      "Epoch: 1228 \tTraining Loss: 0.013402 \tValidation Loss: 0.014426\n",
      "Epoch: 1229 \tTraining Loss: 0.013380 \tValidation Loss: 0.014432\n",
      "Epoch: 1230 \tTraining Loss: 0.013366 \tValidation Loss: 0.014421\n",
      "Epoch: 1231 \tTraining Loss: 0.013386 \tValidation Loss: 0.014427\n",
      "Epoch: 1232 \tTraining Loss: 0.013375 \tValidation Loss: 0.014419\n",
      "Epoch: 1233 \tTraining Loss: 0.013396 \tValidation Loss: 0.014425\n",
      "Epoch: 1234 \tTraining Loss: 0.013372 \tValidation Loss: 0.014420\n",
      "Epoch: 1235 \tTraining Loss: 0.013393 \tValidation Loss: 0.014427\n",
      "Epoch: 1236 \tTraining Loss: 0.013354 \tValidation Loss: 0.014426\n",
      "Epoch: 1237 \tTraining Loss: 0.013365 \tValidation Loss: 0.014429\n",
      "Epoch: 1238 \tTraining Loss: 0.013378 \tValidation Loss: 0.014427\n",
      "Epoch: 1239 \tTraining Loss: 0.013347 \tValidation Loss: 0.014429\n",
      "Epoch: 1240 \tTraining Loss: 0.013379 \tValidation Loss: 0.014421\n",
      "Epoch: 1241 \tTraining Loss: 0.013371 \tValidation Loss: 0.014432\n",
      "Epoch: 1242 \tTraining Loss: 0.013361 \tValidation Loss: 0.014424\n",
      "Epoch: 1243 \tTraining Loss: 0.013373 \tValidation Loss: 0.014426\n",
      "Epoch: 1244 \tTraining Loss: 0.013368 \tValidation Loss: 0.014429\n",
      "Epoch: 1245 \tTraining Loss: 0.013367 \tValidation Loss: 0.014421\n",
      "Epoch: 1246 \tTraining Loss: 0.013368 \tValidation Loss: 0.014428\n",
      "Epoch: 1247 \tTraining Loss: 0.013365 \tValidation Loss: 0.014428\n",
      "Epoch: 1248 \tTraining Loss: 0.013357 \tValidation Loss: 0.014429\n",
      "Epoch: 1249 \tTraining Loss: 0.013356 \tValidation Loss: 0.014422\n",
      "Epoch: 1250 \tTraining Loss: 0.013366 \tValidation Loss: 0.014424\n",
      "Epoch: 1251 \tTraining Loss: 0.013356 \tValidation Loss: 0.014431\n",
      "Epoch: 1252 \tTraining Loss: 0.013373 \tValidation Loss: 0.014432\n",
      "Epoch: 1253 \tTraining Loss: 0.013353 \tValidation Loss: 0.014420\n",
      "Epoch: 1254 \tTraining Loss: 0.013371 \tValidation Loss: 0.014430\n",
      "Epoch: 1255 \tTraining Loss: 0.013378 \tValidation Loss: 0.014431\n",
      "Epoch: 1256 \tTraining Loss: 0.013353 \tValidation Loss: 0.014428\n",
      "Epoch: 1257 \tTraining Loss: 0.013343 \tValidation Loss: 0.014430\n",
      "Epoch: 1258 \tTraining Loss: 0.013361 \tValidation Loss: 0.014423\n",
      "Epoch: 1259 \tTraining Loss: 0.013336 \tValidation Loss: 0.014427\n",
      "Epoch: 1260 \tTraining Loss: 0.013353 \tValidation Loss: 0.014425\n",
      "Epoch: 1261 \tTraining Loss: 0.013339 \tValidation Loss: 0.014430\n",
      "Epoch: 1262 \tTraining Loss: 0.013359 \tValidation Loss: 0.014425\n",
      "Epoch: 1263 \tTraining Loss: 0.013342 \tValidation Loss: 0.014429\n",
      "Epoch: 1264 \tTraining Loss: 0.013345 \tValidation Loss: 0.014431\n",
      "Epoch: 1265 \tTraining Loss: 0.013329 \tValidation Loss: 0.014430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1266 \tTraining Loss: 0.013343 \tValidation Loss: 0.014431\n",
      "Epoch: 1267 \tTraining Loss: 0.013355 \tValidation Loss: 0.014433\n",
      "Epoch: 1268 \tTraining Loss: 0.013338 \tValidation Loss: 0.014425\n",
      "Epoch: 1269 \tTraining Loss: 0.013336 \tValidation Loss: 0.014426\n",
      "Epoch: 1270 \tTraining Loss: 0.013328 \tValidation Loss: 0.014431\n",
      "Epoch: 1271 \tTraining Loss: 0.013341 \tValidation Loss: 0.014429\n",
      "Epoch: 1272 \tTraining Loss: 0.013336 \tValidation Loss: 0.014426\n",
      "Epoch: 1273 \tTraining Loss: 0.013324 \tValidation Loss: 0.014432\n",
      "Epoch: 1274 \tTraining Loss: 0.013326 \tValidation Loss: 0.014425\n",
      "Epoch: 1275 \tTraining Loss: 0.013329 \tValidation Loss: 0.014428\n",
      "Epoch: 1276 \tTraining Loss: 0.013326 \tValidation Loss: 0.014432\n",
      "Epoch: 1277 \tTraining Loss: 0.013343 \tValidation Loss: 0.014428\n",
      "Epoch: 1278 \tTraining Loss: 0.013331 \tValidation Loss: 0.014441\n",
      "Epoch: 1279 \tTraining Loss: 0.013334 \tValidation Loss: 0.014430\n",
      "Epoch: 1280 \tTraining Loss: 0.013339 \tValidation Loss: 0.014423\n",
      "Epoch: 1281 \tTraining Loss: 0.013288 \tValidation Loss: 0.014428\n",
      "Epoch: 1282 \tTraining Loss: 0.013316 \tValidation Loss: 0.014433\n",
      "Epoch: 1283 \tTraining Loss: 0.013342 \tValidation Loss: 0.014427\n",
      "Epoch: 1284 \tTraining Loss: 0.013313 \tValidation Loss: 0.014432\n",
      "Epoch: 1285 \tTraining Loss: 0.013340 \tValidation Loss: 0.014429\n",
      "Epoch: 1286 \tTraining Loss: 0.013329 \tValidation Loss: 0.014430\n",
      "Epoch: 1287 \tTraining Loss: 0.013323 \tValidation Loss: 0.014429\n",
      "Epoch: 1288 \tTraining Loss: 0.013318 \tValidation Loss: 0.014428\n",
      "Epoch: 1289 \tTraining Loss: 0.013306 \tValidation Loss: 0.014428\n",
      "Epoch: 1290 \tTraining Loss: 0.013333 \tValidation Loss: 0.014435\n",
      "Epoch: 1291 \tTraining Loss: 0.013295 \tValidation Loss: 0.014434\n",
      "Epoch: 1292 \tTraining Loss: 0.013305 \tValidation Loss: 0.014434\n",
      "Epoch: 1293 \tTraining Loss: 0.013315 \tValidation Loss: 0.014426\n",
      "Epoch: 1294 \tTraining Loss: 0.013306 \tValidation Loss: 0.014437\n",
      "Epoch: 1295 \tTraining Loss: 0.013312 \tValidation Loss: 0.014433\n",
      "Epoch: 1296 \tTraining Loss: 0.013313 \tValidation Loss: 0.014433\n",
      "Epoch: 1297 \tTraining Loss: 0.013310 \tValidation Loss: 0.014439\n",
      "Epoch: 1298 \tTraining Loss: 0.013299 \tValidation Loss: 0.014431\n",
      "Epoch: 1299 \tTraining Loss: 0.013300 \tValidation Loss: 0.014436\n",
      "Epoch: 1300 \tTraining Loss: 0.013294 \tValidation Loss: 0.014433\n",
      "Epoch: 1301 \tTraining Loss: 0.013313 \tValidation Loss: 0.014427\n",
      "Epoch: 1302 \tTraining Loss: 0.013306 \tValidation Loss: 0.014429\n",
      "Epoch: 1303 \tTraining Loss: 0.013303 \tValidation Loss: 0.014428\n",
      "Epoch: 1304 \tTraining Loss: 0.013297 \tValidation Loss: 0.014441\n",
      "Epoch: 1305 \tTraining Loss: 0.013306 \tValidation Loss: 0.014430\n",
      "Epoch: 1306 \tTraining Loss: 0.013269 \tValidation Loss: 0.014435\n",
      "Epoch: 1307 \tTraining Loss: 0.013294 \tValidation Loss: 0.014431\n",
      "Epoch: 1308 \tTraining Loss: 0.013295 \tValidation Loss: 0.014430\n",
      "Epoch: 1309 \tTraining Loss: 0.013301 \tValidation Loss: 0.014432\n",
      "Epoch: 1310 \tTraining Loss: 0.013303 \tValidation Loss: 0.014431\n",
      "Epoch: 1311 \tTraining Loss: 0.013301 \tValidation Loss: 0.014435\n",
      "Epoch: 1312 \tTraining Loss: 0.013276 \tValidation Loss: 0.014434\n",
      "Epoch: 1313 \tTraining Loss: 0.013291 \tValidation Loss: 0.014436\n",
      "Epoch: 1314 \tTraining Loss: 0.013269 \tValidation Loss: 0.014434\n",
      "Epoch: 1315 \tTraining Loss: 0.013299 \tValidation Loss: 0.014439\n",
      "Epoch: 1316 \tTraining Loss: 0.013280 \tValidation Loss: 0.014436\n",
      "Epoch: 1317 \tTraining Loss: 0.013281 \tValidation Loss: 0.014437\n",
      "Epoch: 1318 \tTraining Loss: 0.013299 \tValidation Loss: 0.014442\n",
      "Epoch: 1319 \tTraining Loss: 0.013281 \tValidation Loss: 0.014442\n",
      "Epoch: 1320 \tTraining Loss: 0.013280 \tValidation Loss: 0.014435\n",
      "Epoch: 1321 \tTraining Loss: 0.013274 \tValidation Loss: 0.014441\n",
      "Epoch: 1322 \tTraining Loss: 0.013282 \tValidation Loss: 0.014436\n",
      "Epoch: 1323 \tTraining Loss: 0.013283 \tValidation Loss: 0.014433\n",
      "Epoch: 1324 \tTraining Loss: 0.013278 \tValidation Loss: 0.014434\n",
      "Epoch: 1325 \tTraining Loss: 0.013274 \tValidation Loss: 0.014434\n",
      "Epoch: 1326 \tTraining Loss: 0.013280 \tValidation Loss: 0.014438\n",
      "Epoch: 1327 \tTraining Loss: 0.013263 \tValidation Loss: 0.014430\n",
      "Epoch: 1328 \tTraining Loss: 0.013302 \tValidation Loss: 0.014436\n",
      "Epoch: 1329 \tTraining Loss: 0.013300 \tValidation Loss: 0.014431\n",
      "Epoch: 1330 \tTraining Loss: 0.013289 \tValidation Loss: 0.014436\n",
      "Epoch: 1331 \tTraining Loss: 0.013276 \tValidation Loss: 0.014438\n",
      "Epoch: 1332 \tTraining Loss: 0.013289 \tValidation Loss: 0.014430\n",
      "Epoch: 1333 \tTraining Loss: 0.013285 \tValidation Loss: 0.014431\n",
      "Epoch: 1334 \tTraining Loss: 0.013274 \tValidation Loss: 0.014444\n",
      "Epoch: 1335 \tTraining Loss: 0.013295 \tValidation Loss: 0.014431\n",
      "Epoch: 1336 \tTraining Loss: 0.013273 \tValidation Loss: 0.014436\n",
      "Epoch: 1337 \tTraining Loss: 0.013271 \tValidation Loss: 0.014433\n",
      "Epoch: 1338 \tTraining Loss: 0.013268 \tValidation Loss: 0.014431\n",
      "Epoch: 1339 \tTraining Loss: 0.013275 \tValidation Loss: 0.014435\n",
      "Epoch: 1340 \tTraining Loss: 0.013280 \tValidation Loss: 0.014434\n",
      "Epoch: 1341 \tTraining Loss: 0.013260 \tValidation Loss: 0.014432\n",
      "Epoch: 1342 \tTraining Loss: 0.013252 \tValidation Loss: 0.014437\n",
      "Epoch: 1343 \tTraining Loss: 0.013280 \tValidation Loss: 0.014429\n",
      "Epoch: 1344 \tTraining Loss: 0.013246 \tValidation Loss: 0.014429\n",
      "Epoch: 1345 \tTraining Loss: 0.013275 \tValidation Loss: 0.014438\n",
      "Epoch: 1346 \tTraining Loss: 0.013257 \tValidation Loss: 0.014430\n",
      "Epoch: 1347 \tTraining Loss: 0.013265 \tValidation Loss: 0.014435\n",
      "Epoch: 1348 \tTraining Loss: 0.013258 \tValidation Loss: 0.014434\n",
      "Epoch: 1349 \tTraining Loss: 0.013268 \tValidation Loss: 0.014440\n",
      "Epoch: 1350 \tTraining Loss: 0.013266 \tValidation Loss: 0.014439\n",
      "Epoch: 1351 \tTraining Loss: 0.013260 \tValidation Loss: 0.014430\n",
      "Epoch: 1352 \tTraining Loss: 0.013256 \tValidation Loss: 0.014438\n",
      "Epoch: 1353 \tTraining Loss: 0.013248 \tValidation Loss: 0.014440\n",
      "Epoch: 1354 \tTraining Loss: 0.013266 \tValidation Loss: 0.014437\n",
      "Epoch: 1355 \tTraining Loss: 0.013230 \tValidation Loss: 0.014438\n",
      "Epoch: 1356 \tTraining Loss: 0.013252 \tValidation Loss: 0.014441\n",
      "Epoch: 1357 \tTraining Loss: 0.013242 \tValidation Loss: 0.014445\n",
      "Epoch: 1358 \tTraining Loss: 0.013237 \tValidation Loss: 0.014436\n",
      "Epoch: 1359 \tTraining Loss: 0.013260 \tValidation Loss: 0.014442\n",
      "Epoch: 1360 \tTraining Loss: 0.013262 \tValidation Loss: 0.014434\n",
      "Epoch: 1361 \tTraining Loss: 0.013243 \tValidation Loss: 0.014432\n",
      "Epoch: 1362 \tTraining Loss: 0.013237 \tValidation Loss: 0.014438\n",
      "Epoch: 1363 \tTraining Loss: 0.013253 \tValidation Loss: 0.014442\n",
      "Epoch: 1364 \tTraining Loss: 0.013238 \tValidation Loss: 0.014434\n",
      "Epoch: 1365 \tTraining Loss: 0.013236 \tValidation Loss: 0.014434\n",
      "Epoch: 1366 \tTraining Loss: 0.013238 \tValidation Loss: 0.014439\n",
      "Epoch: 1367 \tTraining Loss: 0.013240 \tValidation Loss: 0.014443\n",
      "Epoch: 1368 \tTraining Loss: 0.013235 \tValidation Loss: 0.014443\n",
      "Epoch: 1369 \tTraining Loss: 0.013230 \tValidation Loss: 0.014439\n",
      "Epoch: 1370 \tTraining Loss: 0.013229 \tValidation Loss: 0.014446\n",
      "Epoch: 1371 \tTraining Loss: 0.013224 \tValidation Loss: 0.014434\n",
      "Epoch: 1372 \tTraining Loss: 0.013245 \tValidation Loss: 0.014442\n",
      "Epoch: 1373 \tTraining Loss: 0.013235 \tValidation Loss: 0.014438\n",
      "Epoch: 1374 \tTraining Loss: 0.013242 \tValidation Loss: 0.014440\n",
      "Epoch: 1375 \tTraining Loss: 0.013232 \tValidation Loss: 0.014440\n",
      "Epoch: 1376 \tTraining Loss: 0.013250 \tValidation Loss: 0.014438\n",
      "Epoch: 1377 \tTraining Loss: 0.013246 \tValidation Loss: 0.014442\n",
      "Epoch: 1378 \tTraining Loss: 0.013239 \tValidation Loss: 0.014435\n",
      "Epoch: 1379 \tTraining Loss: 0.013240 \tValidation Loss: 0.014436\n",
      "Epoch: 1380 \tTraining Loss: 0.013234 \tValidation Loss: 0.014440\n",
      "Epoch: 1381 \tTraining Loss: 0.013219 \tValidation Loss: 0.014439\n",
      "Epoch: 1382 \tTraining Loss: 0.013210 \tValidation Loss: 0.014446\n",
      "Epoch: 1383 \tTraining Loss: 0.013222 \tValidation Loss: 0.014439\n",
      "Epoch: 1384 \tTraining Loss: 0.013227 \tValidation Loss: 0.014441\n",
      "Epoch: 1385 \tTraining Loss: 0.013224 \tValidation Loss: 0.014438\n",
      "Epoch: 1386 \tTraining Loss: 0.013225 \tValidation Loss: 0.014442\n",
      "Epoch: 1387 \tTraining Loss: 0.013221 \tValidation Loss: 0.014435\n",
      "Epoch: 1388 \tTraining Loss: 0.013220 \tValidation Loss: 0.014438\n",
      "Epoch: 1389 \tTraining Loss: 0.013229 \tValidation Loss: 0.014435\n",
      "Epoch: 1390 \tTraining Loss: 0.013213 \tValidation Loss: 0.014434\n",
      "Epoch: 1391 \tTraining Loss: 0.013240 \tValidation Loss: 0.014436\n",
      "Epoch: 1392 \tTraining Loss: 0.013217 \tValidation Loss: 0.014434\n",
      "Epoch: 1393 \tTraining Loss: 0.013221 \tValidation Loss: 0.014427\n",
      "Epoch: 1394 \tTraining Loss: 0.013219 \tValidation Loss: 0.014443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1395 \tTraining Loss: 0.013230 \tValidation Loss: 0.014447\n",
      "Epoch: 1396 \tTraining Loss: 0.013230 \tValidation Loss: 0.014437\n",
      "Epoch: 1397 \tTraining Loss: 0.013198 \tValidation Loss: 0.014439\n",
      "Epoch: 1398 \tTraining Loss: 0.013197 \tValidation Loss: 0.014437\n",
      "Epoch: 1399 \tTraining Loss: 0.013216 \tValidation Loss: 0.014442\n",
      "Epoch: 1400 \tTraining Loss: 0.013179 \tValidation Loss: 0.014444\n",
      "Epoch: 1401 \tTraining Loss: 0.013217 \tValidation Loss: 0.014445\n",
      "Epoch: 1402 \tTraining Loss: 0.013221 \tValidation Loss: 0.014441\n",
      "Epoch: 1403 \tTraining Loss: 0.013239 \tValidation Loss: 0.014443\n",
      "Epoch: 1404 \tTraining Loss: 0.013214 \tValidation Loss: 0.014441\n",
      "Epoch: 1405 \tTraining Loss: 0.013212 \tValidation Loss: 0.014438\n",
      "Epoch: 1406 \tTraining Loss: 0.013209 \tValidation Loss: 0.014440\n",
      "Epoch: 1407 \tTraining Loss: 0.013197 \tValidation Loss: 0.014440\n",
      "Epoch: 1408 \tTraining Loss: 0.013208 \tValidation Loss: 0.014445\n",
      "Epoch: 1409 \tTraining Loss: 0.013194 \tValidation Loss: 0.014444\n",
      "Epoch: 1410 \tTraining Loss: 0.013209 \tValidation Loss: 0.014443\n",
      "Epoch: 1411 \tTraining Loss: 0.013199 \tValidation Loss: 0.014444\n",
      "Epoch: 1412 \tTraining Loss: 0.013197 \tValidation Loss: 0.014435\n",
      "Epoch: 1413 \tTraining Loss: 0.013201 \tValidation Loss: 0.014440\n",
      "Epoch: 1414 \tTraining Loss: 0.013191 \tValidation Loss: 0.014449\n",
      "Epoch: 1415 \tTraining Loss: 0.013186 \tValidation Loss: 0.014439\n",
      "Epoch: 1416 \tTraining Loss: 0.013206 \tValidation Loss: 0.014442\n",
      "Epoch: 1417 \tTraining Loss: 0.013186 \tValidation Loss: 0.014449\n",
      "Epoch: 1418 \tTraining Loss: 0.013192 \tValidation Loss: 0.014443\n",
      "Epoch: 1419 \tTraining Loss: 0.013203 \tValidation Loss: 0.014433\n",
      "Epoch: 1420 \tTraining Loss: 0.013183 \tValidation Loss: 0.014439\n",
      "Epoch: 1421 \tTraining Loss: 0.013172 \tValidation Loss: 0.014446\n",
      "Epoch: 1422 \tTraining Loss: 0.013204 \tValidation Loss: 0.014446\n",
      "Epoch: 1423 \tTraining Loss: 0.013178 \tValidation Loss: 0.014436\n",
      "Epoch: 1424 \tTraining Loss: 0.013204 \tValidation Loss: 0.014446\n",
      "Epoch: 1425 \tTraining Loss: 0.013190 \tValidation Loss: 0.014437\n",
      "Epoch: 1426 \tTraining Loss: 0.013170 \tValidation Loss: 0.014440\n",
      "Epoch: 1427 \tTraining Loss: 0.013200 \tValidation Loss: 0.014440\n",
      "Epoch: 1428 \tTraining Loss: 0.013170 \tValidation Loss: 0.014446\n",
      "Epoch: 1429 \tTraining Loss: 0.013180 \tValidation Loss: 0.014438\n",
      "Epoch: 1430 \tTraining Loss: 0.013178 \tValidation Loss: 0.014444\n",
      "Epoch: 1431 \tTraining Loss: 0.013171 \tValidation Loss: 0.014442\n",
      "Epoch: 1432 \tTraining Loss: 0.013176 \tValidation Loss: 0.014438\n",
      "Epoch: 1433 \tTraining Loss: 0.013206 \tValidation Loss: 0.014441\n",
      "Epoch: 1434 \tTraining Loss: 0.013167 \tValidation Loss: 0.014446\n",
      "Epoch: 1435 \tTraining Loss: 0.013206 \tValidation Loss: 0.014451\n",
      "Epoch: 1436 \tTraining Loss: 0.013189 \tValidation Loss: 0.014445\n",
      "Epoch: 1437 \tTraining Loss: 0.013177 \tValidation Loss: 0.014444\n",
      "Epoch: 1438 \tTraining Loss: 0.013186 \tValidation Loss: 0.014446\n",
      "Epoch: 1439 \tTraining Loss: 0.013190 \tValidation Loss: 0.014446\n",
      "Epoch: 1440 \tTraining Loss: 0.013174 \tValidation Loss: 0.014444\n",
      "Epoch: 1441 \tTraining Loss: 0.013183 \tValidation Loss: 0.014447\n",
      "Epoch: 1442 \tTraining Loss: 0.013193 \tValidation Loss: 0.014450\n",
      "Epoch: 1443 \tTraining Loss: 0.013161 \tValidation Loss: 0.014449\n",
      "Epoch: 1444 \tTraining Loss: 0.013183 \tValidation Loss: 0.014442\n",
      "Epoch: 1445 \tTraining Loss: 0.013187 \tValidation Loss: 0.014443\n",
      "Epoch: 1446 \tTraining Loss: 0.013165 \tValidation Loss: 0.014447\n",
      "Epoch: 1447 \tTraining Loss: 0.013170 \tValidation Loss: 0.014453\n",
      "Epoch: 1448 \tTraining Loss: 0.013175 \tValidation Loss: 0.014448\n",
      "Epoch: 1449 \tTraining Loss: 0.013160 \tValidation Loss: 0.014460\n",
      "Epoch: 1450 \tTraining Loss: 0.013153 \tValidation Loss: 0.014445\n",
      "Epoch: 1451 \tTraining Loss: 0.013186 \tValidation Loss: 0.014439\n",
      "Epoch: 1452 \tTraining Loss: 0.013162 \tValidation Loss: 0.014457\n",
      "Epoch: 1453 \tTraining Loss: 0.013161 \tValidation Loss: 0.014441\n",
      "Epoch: 1454 \tTraining Loss: 0.013161 \tValidation Loss: 0.014440\n",
      "Epoch: 1455 \tTraining Loss: 0.013170 \tValidation Loss: 0.014443\n",
      "Epoch: 1456 \tTraining Loss: 0.013166 \tValidation Loss: 0.014454\n",
      "Epoch: 1457 \tTraining Loss: 0.013147 \tValidation Loss: 0.014447\n",
      "Epoch: 1458 \tTraining Loss: 0.013165 \tValidation Loss: 0.014453\n",
      "Epoch: 1459 \tTraining Loss: 0.013166 \tValidation Loss: 0.014446\n",
      "Epoch: 1460 \tTraining Loss: 0.013145 \tValidation Loss: 0.014449\n",
      "Epoch: 1461 \tTraining Loss: 0.013162 \tValidation Loss: 0.014452\n",
      "Epoch: 1462 \tTraining Loss: 0.013180 \tValidation Loss: 0.014450\n",
      "Epoch: 1463 \tTraining Loss: 0.013158 \tValidation Loss: 0.014442\n",
      "Epoch: 1464 \tTraining Loss: 0.013129 \tValidation Loss: 0.014446\n",
      "Epoch: 1465 \tTraining Loss: 0.013158 \tValidation Loss: 0.014453\n",
      "Epoch: 1466 \tTraining Loss: 0.013177 \tValidation Loss: 0.014450\n",
      "Epoch: 1467 \tTraining Loss: 0.013149 \tValidation Loss: 0.014448\n",
      "Epoch: 1468 \tTraining Loss: 0.013158 \tValidation Loss: 0.014452\n",
      "Epoch: 1469 \tTraining Loss: 0.013141 \tValidation Loss: 0.014441\n",
      "Epoch: 1470 \tTraining Loss: 0.013134 \tValidation Loss: 0.014451\n",
      "Epoch: 1471 \tTraining Loss: 0.013151 \tValidation Loss: 0.014453\n",
      "Epoch: 1472 \tTraining Loss: 0.013150 \tValidation Loss: 0.014452\n",
      "Epoch: 1473 \tTraining Loss: 0.013129 \tValidation Loss: 0.014448\n",
      "Epoch: 1474 \tTraining Loss: 0.013131 \tValidation Loss: 0.014457\n",
      "Epoch: 1475 \tTraining Loss: 0.013156 \tValidation Loss: 0.014446\n",
      "Epoch: 1476 \tTraining Loss: 0.013133 \tValidation Loss: 0.014444\n",
      "Epoch: 1477 \tTraining Loss: 0.013176 \tValidation Loss: 0.014446\n",
      "Epoch: 1478 \tTraining Loss: 0.013150 \tValidation Loss: 0.014449\n",
      "Epoch: 1479 \tTraining Loss: 0.013144 \tValidation Loss: 0.014450\n",
      "Epoch: 1480 \tTraining Loss: 0.013152 \tValidation Loss: 0.014459\n",
      "Epoch: 1481 \tTraining Loss: 0.013136 \tValidation Loss: 0.014454\n",
      "Epoch: 1482 \tTraining Loss: 0.013155 \tValidation Loss: 0.014446\n",
      "Epoch: 1483 \tTraining Loss: 0.013136 \tValidation Loss: 0.014460\n",
      "Epoch: 1484 \tTraining Loss: 0.013137 \tValidation Loss: 0.014447\n",
      "Epoch: 1485 \tTraining Loss: 0.013134 \tValidation Loss: 0.014457\n",
      "Epoch: 1486 \tTraining Loss: 0.013137 \tValidation Loss: 0.014449\n",
      "Epoch: 1487 \tTraining Loss: 0.013148 \tValidation Loss: 0.014456\n",
      "Epoch: 1488 \tTraining Loss: 0.013123 \tValidation Loss: 0.014454\n",
      "Epoch: 1489 \tTraining Loss: 0.013131 \tValidation Loss: 0.014452\n",
      "Epoch: 1490 \tTraining Loss: 0.013108 \tValidation Loss: 0.014459\n",
      "Epoch: 1491 \tTraining Loss: 0.013123 \tValidation Loss: 0.014450\n",
      "Epoch: 1492 \tTraining Loss: 0.013133 \tValidation Loss: 0.014452\n",
      "Epoch: 1493 \tTraining Loss: 0.013135 \tValidation Loss: 0.014449\n",
      "Epoch: 1494 \tTraining Loss: 0.013118 \tValidation Loss: 0.014459\n",
      "Epoch: 1495 \tTraining Loss: 0.013130 \tValidation Loss: 0.014460\n",
      "Epoch: 1496 \tTraining Loss: 0.013135 \tValidation Loss: 0.014448\n",
      "Epoch: 1497 \tTraining Loss: 0.013135 \tValidation Loss: 0.014446\n",
      "Epoch: 1498 \tTraining Loss: 0.013120 \tValidation Loss: 0.014454\n",
      "Epoch: 1499 \tTraining Loss: 0.013133 \tValidation Loss: 0.014464\n",
      "Epoch: 1500 \tTraining Loss: 0.013107 \tValidation Loss: 0.014460\n",
      "Epoch: 1501 \tTraining Loss: 0.013120 \tValidation Loss: 0.014460\n",
      "Epoch: 1502 \tTraining Loss: 0.013102 \tValidation Loss: 0.014452\n",
      "Epoch: 1503 \tTraining Loss: 0.013098 \tValidation Loss: 0.014452\n",
      "Epoch: 1504 \tTraining Loss: 0.013121 \tValidation Loss: 0.014451\n",
      "Epoch: 1505 \tTraining Loss: 0.013119 \tValidation Loss: 0.014452\n",
      "Epoch: 1506 \tTraining Loss: 0.013135 \tValidation Loss: 0.014455\n",
      "Epoch: 1507 \tTraining Loss: 0.013123 \tValidation Loss: 0.014458\n",
      "Epoch: 1508 \tTraining Loss: 0.013125 \tValidation Loss: 0.014460\n",
      "Epoch: 1509 \tTraining Loss: 0.013132 \tValidation Loss: 0.014460\n",
      "Epoch: 1510 \tTraining Loss: 0.013132 \tValidation Loss: 0.014455\n",
      "Epoch: 1511 \tTraining Loss: 0.013108 \tValidation Loss: 0.014450\n",
      "Epoch: 1512 \tTraining Loss: 0.013113 \tValidation Loss: 0.014451\n",
      "Epoch: 1513 \tTraining Loss: 0.013114 \tValidation Loss: 0.014455\n",
      "Epoch: 1514 \tTraining Loss: 0.013118 \tValidation Loss: 0.014462\n",
      "Epoch: 1515 \tTraining Loss: 0.013102 \tValidation Loss: 0.014447\n",
      "Epoch: 1516 \tTraining Loss: 0.013122 \tValidation Loss: 0.014455\n",
      "Epoch: 1517 \tTraining Loss: 0.013130 \tValidation Loss: 0.014454\n",
      "Epoch: 1518 \tTraining Loss: 0.013103 \tValidation Loss: 0.014451\n",
      "Epoch: 1519 \tTraining Loss: 0.013114 \tValidation Loss: 0.014457\n",
      "Epoch: 1520 \tTraining Loss: 0.013113 \tValidation Loss: 0.014451\n",
      "Epoch: 1521 \tTraining Loss: 0.013112 \tValidation Loss: 0.014463\n",
      "Epoch: 1522 \tTraining Loss: 0.013109 \tValidation Loss: 0.014454\n",
      "Epoch: 1523 \tTraining Loss: 0.013114 \tValidation Loss: 0.014453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1524 \tTraining Loss: 0.013103 \tValidation Loss: 0.014452\n",
      "Epoch: 1525 \tTraining Loss: 0.013107 \tValidation Loss: 0.014457\n",
      "Epoch: 1526 \tTraining Loss: 0.013096 \tValidation Loss: 0.014448\n",
      "Epoch: 1527 \tTraining Loss: 0.013113 \tValidation Loss: 0.014450\n",
      "Epoch: 1528 \tTraining Loss: 0.013095 \tValidation Loss: 0.014454\n",
      "Epoch: 1529 \tTraining Loss: 0.013114 \tValidation Loss: 0.014449\n",
      "Epoch: 1530 \tTraining Loss: 0.013109 \tValidation Loss: 0.014445\n",
      "Epoch: 1531 \tTraining Loss: 0.013116 \tValidation Loss: 0.014450\n",
      "Epoch: 1532 \tTraining Loss: 0.013092 \tValidation Loss: 0.014460\n",
      "Epoch: 1533 \tTraining Loss: 0.013067 \tValidation Loss: 0.014450\n",
      "Epoch: 1534 \tTraining Loss: 0.013081 \tValidation Loss: 0.014455\n",
      "Epoch: 1535 \tTraining Loss: 0.013101 \tValidation Loss: 0.014461\n",
      "Epoch: 1536 \tTraining Loss: 0.013109 \tValidation Loss: 0.014451\n",
      "Epoch: 1537 \tTraining Loss: 0.013109 \tValidation Loss: 0.014461\n",
      "Epoch: 1538 \tTraining Loss: 0.013089 \tValidation Loss: 0.014458\n",
      "Epoch: 1539 \tTraining Loss: 0.013063 \tValidation Loss: 0.014460\n",
      "Epoch: 1540 \tTraining Loss: 0.013104 \tValidation Loss: 0.014452\n",
      "Epoch: 1541 \tTraining Loss: 0.013096 \tValidation Loss: 0.014456\n",
      "Epoch: 1542 \tTraining Loss: 0.013105 \tValidation Loss: 0.014453\n",
      "Epoch: 1543 \tTraining Loss: 0.013098 \tValidation Loss: 0.014457\n",
      "Epoch: 1544 \tTraining Loss: 0.013099 \tValidation Loss: 0.014458\n",
      "Epoch: 1545 \tTraining Loss: 0.013069 \tValidation Loss: 0.014459\n",
      "Epoch: 1546 \tTraining Loss: 0.013095 \tValidation Loss: 0.014455\n",
      "Epoch: 1547 \tTraining Loss: 0.013060 \tValidation Loss: 0.014460\n",
      "Epoch: 1548 \tTraining Loss: 0.013103 \tValidation Loss: 0.014461\n",
      "Epoch: 1549 \tTraining Loss: 0.013088 \tValidation Loss: 0.014472\n",
      "Epoch: 1550 \tTraining Loss: 0.013089 \tValidation Loss: 0.014469\n",
      "Epoch: 1551 \tTraining Loss: 0.013072 \tValidation Loss: 0.014463\n",
      "Epoch: 1552 \tTraining Loss: 0.013077 \tValidation Loss: 0.014459\n",
      "Epoch: 1553 \tTraining Loss: 0.013076 \tValidation Loss: 0.014456\n",
      "Epoch: 1554 \tTraining Loss: 0.013084 \tValidation Loss: 0.014460\n",
      "Epoch: 1555 \tTraining Loss: 0.013078 \tValidation Loss: 0.014460\n",
      "Epoch: 1556 \tTraining Loss: 0.013068 \tValidation Loss: 0.014469\n",
      "Epoch: 1557 \tTraining Loss: 0.013062 \tValidation Loss: 0.014461\n",
      "Epoch: 1558 \tTraining Loss: 0.013086 \tValidation Loss: 0.014469\n",
      "Epoch: 1559 \tTraining Loss: 0.013072 \tValidation Loss: 0.014468\n",
      "Epoch: 1560 \tTraining Loss: 0.013068 \tValidation Loss: 0.014452\n",
      "Epoch: 1561 \tTraining Loss: 0.013071 \tValidation Loss: 0.014459\n",
      "Epoch: 1562 \tTraining Loss: 0.013065 \tValidation Loss: 0.014455\n",
      "Epoch: 1563 \tTraining Loss: 0.013058 \tValidation Loss: 0.014461\n",
      "Epoch: 1564 \tTraining Loss: 0.013037 \tValidation Loss: 0.014463\n",
      "Epoch: 1565 \tTraining Loss: 0.013067 \tValidation Loss: 0.014462\n",
      "Epoch: 1566 \tTraining Loss: 0.013071 \tValidation Loss: 0.014456\n",
      "Epoch: 1567 \tTraining Loss: 0.013082 \tValidation Loss: 0.014457\n",
      "Epoch: 1568 \tTraining Loss: 0.013057 \tValidation Loss: 0.014459\n",
      "Epoch: 1569 \tTraining Loss: 0.013062 \tValidation Loss: 0.014462\n",
      "Epoch: 1570 \tTraining Loss: 0.013082 \tValidation Loss: 0.014458\n",
      "Epoch: 1571 \tTraining Loss: 0.013065 \tValidation Loss: 0.014460\n",
      "Epoch: 1572 \tTraining Loss: 0.013059 \tValidation Loss: 0.014462\n",
      "Epoch: 1573 \tTraining Loss: 0.013062 \tValidation Loss: 0.014463\n",
      "Epoch: 1574 \tTraining Loss: 0.013046 \tValidation Loss: 0.014462\n",
      "Epoch: 1575 \tTraining Loss: 0.013076 \tValidation Loss: 0.014457\n",
      "Epoch: 1576 \tTraining Loss: 0.013071 \tValidation Loss: 0.014460\n",
      "Epoch: 1577 \tTraining Loss: 0.013055 \tValidation Loss: 0.014467\n",
      "Epoch: 1578 \tTraining Loss: 0.013084 \tValidation Loss: 0.014453\n",
      "Epoch: 1579 \tTraining Loss: 0.013051 \tValidation Loss: 0.014453\n",
      "Epoch: 1580 \tTraining Loss: 0.013035 \tValidation Loss: 0.014462\n",
      "Epoch: 1581 \tTraining Loss: 0.013047 \tValidation Loss: 0.014467\n",
      "Epoch: 1582 \tTraining Loss: 0.013062 \tValidation Loss: 0.014464\n",
      "Epoch: 1583 \tTraining Loss: 0.013058 \tValidation Loss: 0.014479\n",
      "Epoch: 1584 \tTraining Loss: 0.013053 \tValidation Loss: 0.014462\n",
      "Epoch: 1585 \tTraining Loss: 0.013047 \tValidation Loss: 0.014457\n",
      "Epoch: 1586 \tTraining Loss: 0.013044 \tValidation Loss: 0.014463\n",
      "Epoch: 1587 \tTraining Loss: 0.013068 \tValidation Loss: 0.014461\n",
      "Epoch: 1588 \tTraining Loss: 0.013066 \tValidation Loss: 0.014464\n",
      "Epoch: 1589 \tTraining Loss: 0.013053 \tValidation Loss: 0.014469\n",
      "Epoch: 1590 \tTraining Loss: 0.013049 \tValidation Loss: 0.014472\n",
      "Epoch: 1591 \tTraining Loss: 0.013042 \tValidation Loss: 0.014460\n",
      "Epoch: 1592 \tTraining Loss: 0.013066 \tValidation Loss: 0.014475\n",
      "Epoch: 1593 \tTraining Loss: 0.013049 \tValidation Loss: 0.014465\n",
      "Epoch: 1594 \tTraining Loss: 0.013036 \tValidation Loss: 0.014466\n",
      "Epoch: 1595 \tTraining Loss: 0.013040 \tValidation Loss: 0.014470\n",
      "Epoch: 1596 \tTraining Loss: 0.013030 \tValidation Loss: 0.014464\n",
      "Epoch: 1597 \tTraining Loss: 0.013038 \tValidation Loss: 0.014459\n",
      "Epoch: 1598 \tTraining Loss: 0.013036 \tValidation Loss: 0.014470\n",
      "Epoch: 1599 \tTraining Loss: 0.013047 \tValidation Loss: 0.014461\n",
      "Epoch: 1600 \tTraining Loss: 0.013041 \tValidation Loss: 0.014458\n",
      "Epoch: 1601 \tTraining Loss: 0.013052 \tValidation Loss: 0.014462\n",
      "Epoch: 1602 \tTraining Loss: 0.013034 \tValidation Loss: 0.014468\n",
      "Epoch: 1603 \tTraining Loss: 0.013031 \tValidation Loss: 0.014466\n",
      "Epoch: 1604 \tTraining Loss: 0.013043 \tValidation Loss: 0.014471\n",
      "Epoch: 1605 \tTraining Loss: 0.013035 \tValidation Loss: 0.014472\n",
      "Epoch: 1606 \tTraining Loss: 0.013042 \tValidation Loss: 0.014474\n",
      "Epoch: 1607 \tTraining Loss: 0.013033 \tValidation Loss: 0.014463\n",
      "Epoch: 1608 \tTraining Loss: 0.013033 \tValidation Loss: 0.014474\n",
      "Epoch: 1609 \tTraining Loss: 0.013050 \tValidation Loss: 0.014464\n",
      "Epoch: 1610 \tTraining Loss: 0.013038 \tValidation Loss: 0.014472\n",
      "Epoch: 1611 \tTraining Loss: 0.013019 \tValidation Loss: 0.014462\n",
      "Epoch: 1612 \tTraining Loss: 0.013040 \tValidation Loss: 0.014459\n",
      "Epoch: 1613 \tTraining Loss: 0.013033 \tValidation Loss: 0.014463\n",
      "Epoch: 1614 \tTraining Loss: 0.013033 \tValidation Loss: 0.014474\n",
      "Epoch: 1615 \tTraining Loss: 0.013018 \tValidation Loss: 0.014473\n",
      "Epoch: 1616 \tTraining Loss: 0.013002 \tValidation Loss: 0.014467\n",
      "Epoch: 1617 \tTraining Loss: 0.013018 \tValidation Loss: 0.014471\n",
      "Epoch: 1618 \tTraining Loss: 0.013007 \tValidation Loss: 0.014472\n",
      "Epoch: 1619 \tTraining Loss: 0.013022 \tValidation Loss: 0.014471\n",
      "Epoch: 1620 \tTraining Loss: 0.013021 \tValidation Loss: 0.014469\n",
      "Epoch: 1621 \tTraining Loss: 0.013006 \tValidation Loss: 0.014465\n",
      "Epoch: 1622 \tTraining Loss: 0.013005 \tValidation Loss: 0.014464\n",
      "Epoch: 1623 \tTraining Loss: 0.013014 \tValidation Loss: 0.014467\n",
      "Epoch: 1624 \tTraining Loss: 0.013019 \tValidation Loss: 0.014470\n",
      "Epoch: 1625 \tTraining Loss: 0.013019 \tValidation Loss: 0.014476\n",
      "Epoch: 1626 \tTraining Loss: 0.013019 \tValidation Loss: 0.014470\n",
      "Epoch: 1627 \tTraining Loss: 0.013017 \tValidation Loss: 0.014481\n",
      "Epoch: 1628 \tTraining Loss: 0.013001 \tValidation Loss: 0.014467\n",
      "Epoch: 1629 \tTraining Loss: 0.013020 \tValidation Loss: 0.014472\n",
      "Epoch: 1630 \tTraining Loss: 0.013024 \tValidation Loss: 0.014479\n",
      "Epoch: 1631 \tTraining Loss: 0.012999 \tValidation Loss: 0.014477\n",
      "Epoch: 1632 \tTraining Loss: 0.013005 \tValidation Loss: 0.014477\n",
      "Epoch: 1633 \tTraining Loss: 0.012990 \tValidation Loss: 0.014472\n",
      "Epoch: 1634 \tTraining Loss: 0.013020 \tValidation Loss: 0.014475\n",
      "Epoch: 1635 \tTraining Loss: 0.013043 \tValidation Loss: 0.014478\n",
      "Epoch: 1636 \tTraining Loss: 0.013001 \tValidation Loss: 0.014474\n",
      "Epoch: 1637 \tTraining Loss: 0.013001 \tValidation Loss: 0.014479\n",
      "Epoch: 1638 \tTraining Loss: 0.013006 \tValidation Loss: 0.014481\n",
      "Epoch: 1639 \tTraining Loss: 0.013008 \tValidation Loss: 0.014476\n",
      "Epoch: 1640 \tTraining Loss: 0.012992 \tValidation Loss: 0.014476\n",
      "Epoch: 1641 \tTraining Loss: 0.013014 \tValidation Loss: 0.014477\n",
      "Epoch: 1642 \tTraining Loss: 0.012986 \tValidation Loss: 0.014472\n",
      "Epoch: 1643 \tTraining Loss: 0.012986 \tValidation Loss: 0.014474\n",
      "Epoch: 1644 \tTraining Loss: 0.013011 \tValidation Loss: 0.014463\n",
      "Epoch: 1645 \tTraining Loss: 0.013008 \tValidation Loss: 0.014473\n",
      "Epoch: 1646 \tTraining Loss: 0.012997 \tValidation Loss: 0.014479\n",
      "Epoch: 1647 \tTraining Loss: 0.013008 \tValidation Loss: 0.014477\n",
      "Epoch: 1648 \tTraining Loss: 0.012989 \tValidation Loss: 0.014472\n",
      "Epoch: 1649 \tTraining Loss: 0.012987 \tValidation Loss: 0.014471\n",
      "Epoch: 1650 \tTraining Loss: 0.013017 \tValidation Loss: 0.014466\n",
      "Epoch: 1651 \tTraining Loss: 0.012988 \tValidation Loss: 0.014470\n",
      "Epoch: 1652 \tTraining Loss: 0.012990 \tValidation Loss: 0.014476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1653 \tTraining Loss: 0.013004 \tValidation Loss: 0.014485\n",
      "Epoch: 1654 \tTraining Loss: 0.013017 \tValidation Loss: 0.014474\n",
      "Epoch: 1655 \tTraining Loss: 0.013001 \tValidation Loss: 0.014473\n",
      "Epoch: 1656 \tTraining Loss: 0.012974 \tValidation Loss: 0.014476\n",
      "Epoch: 1657 \tTraining Loss: 0.012971 \tValidation Loss: 0.014478\n",
      "Epoch: 1658 \tTraining Loss: 0.013004 \tValidation Loss: 0.014480\n",
      "Epoch: 1659 \tTraining Loss: 0.012993 \tValidation Loss: 0.014466\n",
      "Epoch: 1660 \tTraining Loss: 0.012979 \tValidation Loss: 0.014475\n",
      "Epoch: 1661 \tTraining Loss: 0.012980 \tValidation Loss: 0.014479\n",
      "Epoch: 1662 \tTraining Loss: 0.012984 \tValidation Loss: 0.014482\n",
      "Epoch: 1663 \tTraining Loss: 0.013011 \tValidation Loss: 0.014468\n",
      "Epoch: 1664 \tTraining Loss: 0.012986 \tValidation Loss: 0.014473\n",
      "Epoch: 1665 \tTraining Loss: 0.012980 \tValidation Loss: 0.014475\n",
      "Epoch: 1666 \tTraining Loss: 0.012947 \tValidation Loss: 0.014474\n",
      "Epoch: 1667 \tTraining Loss: 0.012990 \tValidation Loss: 0.014479\n",
      "Epoch: 1668 \tTraining Loss: 0.013001 \tValidation Loss: 0.014479\n",
      "Epoch: 1669 \tTraining Loss: 0.012989 \tValidation Loss: 0.014475\n",
      "Epoch: 1670 \tTraining Loss: 0.012988 \tValidation Loss: 0.014478\n",
      "Epoch: 1671 \tTraining Loss: 0.012956 \tValidation Loss: 0.014482\n",
      "Epoch: 1672 \tTraining Loss: 0.012991 \tValidation Loss: 0.014480\n",
      "Epoch: 1673 \tTraining Loss: 0.012994 \tValidation Loss: 0.014480\n",
      "Epoch: 1674 \tTraining Loss: 0.012985 \tValidation Loss: 0.014476\n",
      "Epoch: 1675 \tTraining Loss: 0.012976 \tValidation Loss: 0.014475\n",
      "Epoch: 1676 \tTraining Loss: 0.012972 \tValidation Loss: 0.014475\n",
      "Epoch: 1677 \tTraining Loss: 0.012975 \tValidation Loss: 0.014481\n",
      "Epoch: 1678 \tTraining Loss: 0.012976 \tValidation Loss: 0.014473\n",
      "Epoch: 1679 \tTraining Loss: 0.012992 \tValidation Loss: 0.014479\n",
      "Epoch: 1680 \tTraining Loss: 0.012974 \tValidation Loss: 0.014478\n",
      "Epoch: 1681 \tTraining Loss: 0.012954 \tValidation Loss: 0.014480\n",
      "Epoch: 1682 \tTraining Loss: 0.012955 \tValidation Loss: 0.014483\n",
      "Epoch: 1683 \tTraining Loss: 0.012961 \tValidation Loss: 0.014483\n",
      "Epoch: 1684 \tTraining Loss: 0.012952 \tValidation Loss: 0.014487\n",
      "Epoch: 1685 \tTraining Loss: 0.012969 \tValidation Loss: 0.014487\n",
      "Epoch: 1686 \tTraining Loss: 0.012964 \tValidation Loss: 0.014481\n",
      "Epoch: 1687 \tTraining Loss: 0.012963 \tValidation Loss: 0.014483\n",
      "Epoch: 1688 \tTraining Loss: 0.012970 \tValidation Loss: 0.014484\n",
      "Epoch: 1689 \tTraining Loss: 0.012961 \tValidation Loss: 0.014484\n",
      "Epoch: 1690 \tTraining Loss: 0.012953 \tValidation Loss: 0.014485\n",
      "Epoch: 1691 \tTraining Loss: 0.012977 \tValidation Loss: 0.014480\n",
      "Epoch: 1692 \tTraining Loss: 0.012980 \tValidation Loss: 0.014485\n",
      "Epoch: 1693 \tTraining Loss: 0.012978 \tValidation Loss: 0.014479\n",
      "Epoch: 1694 \tTraining Loss: 0.012977 \tValidation Loss: 0.014487\n",
      "Epoch: 1695 \tTraining Loss: 0.012951 \tValidation Loss: 0.014477\n",
      "Epoch: 1696 \tTraining Loss: 0.012986 \tValidation Loss: 0.014483\n",
      "Epoch: 1697 \tTraining Loss: 0.012966 \tValidation Loss: 0.014475\n",
      "Epoch: 1698 \tTraining Loss: 0.012941 \tValidation Loss: 0.014474\n",
      "Epoch: 1699 \tTraining Loss: 0.012957 \tValidation Loss: 0.014487\n",
      "Epoch: 1700 \tTraining Loss: 0.012949 \tValidation Loss: 0.014481\n",
      "Epoch: 1701 \tTraining Loss: 0.012931 \tValidation Loss: 0.014479\n",
      "Epoch: 1702 \tTraining Loss: 0.012966 \tValidation Loss: 0.014478\n",
      "Epoch: 1703 \tTraining Loss: 0.012953 \tValidation Loss: 0.014486\n",
      "Epoch: 1704 \tTraining Loss: 0.012955 \tValidation Loss: 0.014481\n",
      "Epoch: 1705 \tTraining Loss: 0.012941 \tValidation Loss: 0.014471\n",
      "Epoch: 1706 \tTraining Loss: 0.012949 \tValidation Loss: 0.014480\n",
      "Epoch: 1707 \tTraining Loss: 0.012937 \tValidation Loss: 0.014484\n",
      "Epoch: 1708 \tTraining Loss: 0.012952 \tValidation Loss: 0.014492\n",
      "Epoch: 1709 \tTraining Loss: 0.012941 \tValidation Loss: 0.014482\n",
      "Epoch: 1710 \tTraining Loss: 0.012965 \tValidation Loss: 0.014490\n",
      "Epoch: 1711 \tTraining Loss: 0.012936 \tValidation Loss: 0.014490\n",
      "Epoch: 1712 \tTraining Loss: 0.012937 \tValidation Loss: 0.014488\n",
      "Epoch: 1713 \tTraining Loss: 0.012918 \tValidation Loss: 0.014488\n",
      "Epoch: 1714 \tTraining Loss: 0.012940 \tValidation Loss: 0.014477\n",
      "Epoch: 1715 \tTraining Loss: 0.012937 \tValidation Loss: 0.014482\n",
      "Epoch: 1716 \tTraining Loss: 0.012950 \tValidation Loss: 0.014480\n",
      "Epoch: 1717 \tTraining Loss: 0.012955 \tValidation Loss: 0.014480\n",
      "Epoch: 1718 \tTraining Loss: 0.012962 \tValidation Loss: 0.014484\n",
      "Epoch: 1719 \tTraining Loss: 0.012946 \tValidation Loss: 0.014486\n",
      "Epoch: 1720 \tTraining Loss: 0.012953 \tValidation Loss: 0.014476\n",
      "Epoch: 1721 \tTraining Loss: 0.012928 \tValidation Loss: 0.014481\n",
      "Epoch: 1722 \tTraining Loss: 0.012967 \tValidation Loss: 0.014484\n",
      "Epoch: 1723 \tTraining Loss: 0.012942 \tValidation Loss: 0.014488\n",
      "Epoch: 1724 \tTraining Loss: 0.012903 \tValidation Loss: 0.014488\n",
      "Epoch: 1725 \tTraining Loss: 0.012947 \tValidation Loss: 0.014483\n",
      "Epoch: 1726 \tTraining Loss: 0.012935 \tValidation Loss: 0.014486\n",
      "Epoch: 1727 \tTraining Loss: 0.012942 \tValidation Loss: 0.014489\n",
      "Epoch: 1728 \tTraining Loss: 0.012941 \tValidation Loss: 0.014480\n",
      "Epoch: 1729 \tTraining Loss: 0.012946 \tValidation Loss: 0.014490\n",
      "Epoch: 1730 \tTraining Loss: 0.012921 \tValidation Loss: 0.014485\n",
      "Epoch: 1731 \tTraining Loss: 0.012931 \tValidation Loss: 0.014489\n",
      "Epoch: 1732 \tTraining Loss: 0.012943 \tValidation Loss: 0.014484\n",
      "Epoch: 1733 \tTraining Loss: 0.012926 \tValidation Loss: 0.014493\n",
      "Epoch: 1734 \tTraining Loss: 0.012924 \tValidation Loss: 0.014488\n",
      "Epoch: 1735 \tTraining Loss: 0.012936 \tValidation Loss: 0.014480\n",
      "Epoch: 1736 \tTraining Loss: 0.012936 \tValidation Loss: 0.014498\n",
      "Epoch: 1737 \tTraining Loss: 0.012910 \tValidation Loss: 0.014488\n",
      "Epoch: 1738 \tTraining Loss: 0.012924 \tValidation Loss: 0.014492\n",
      "Epoch: 1739 \tTraining Loss: 0.012937 \tValidation Loss: 0.014485\n",
      "Epoch: 1740 \tTraining Loss: 0.012928 \tValidation Loss: 0.014490\n",
      "Epoch: 1741 \tTraining Loss: 0.012940 \tValidation Loss: 0.014492\n",
      "Epoch: 1742 \tTraining Loss: 0.012925 \tValidation Loss: 0.014487\n",
      "Epoch: 1743 \tTraining Loss: 0.012938 \tValidation Loss: 0.014499\n",
      "Epoch: 1744 \tTraining Loss: 0.012919 \tValidation Loss: 0.014501\n",
      "Epoch: 1745 \tTraining Loss: 0.012921 \tValidation Loss: 0.014492\n",
      "Epoch: 1746 \tTraining Loss: 0.012919 \tValidation Loss: 0.014494\n",
      "Epoch: 1747 \tTraining Loss: 0.012916 \tValidation Loss: 0.014491\n",
      "Epoch: 1748 \tTraining Loss: 0.012908 \tValidation Loss: 0.014490\n",
      "Epoch: 1749 \tTraining Loss: 0.012932 \tValidation Loss: 0.014492\n",
      "Epoch: 1750 \tTraining Loss: 0.012940 \tValidation Loss: 0.014483\n",
      "Epoch: 1751 \tTraining Loss: 0.012917 \tValidation Loss: 0.014489\n",
      "Epoch: 1752 \tTraining Loss: 0.012900 \tValidation Loss: 0.014491\n",
      "Epoch: 1753 \tTraining Loss: 0.012913 \tValidation Loss: 0.014495\n",
      "Epoch: 1754 \tTraining Loss: 0.012904 \tValidation Loss: 0.014501\n",
      "Epoch: 1755 \tTraining Loss: 0.012913 \tValidation Loss: 0.014490\n",
      "Epoch: 1756 \tTraining Loss: 0.012923 \tValidation Loss: 0.014494\n",
      "Epoch: 1757 \tTraining Loss: 0.012935 \tValidation Loss: 0.014490\n",
      "Epoch: 1758 \tTraining Loss: 0.012908 \tValidation Loss: 0.014500\n",
      "Epoch: 1759 \tTraining Loss: 0.012903 \tValidation Loss: 0.014482\n",
      "Epoch: 1760 \tTraining Loss: 0.012912 \tValidation Loss: 0.014491\n",
      "Epoch: 1761 \tTraining Loss: 0.012914 \tValidation Loss: 0.014491\n",
      "Epoch: 1762 \tTraining Loss: 0.012931 \tValidation Loss: 0.014493\n",
      "Epoch: 1763 \tTraining Loss: 0.012919 \tValidation Loss: 0.014490\n",
      "Epoch: 1764 \tTraining Loss: 0.012904 \tValidation Loss: 0.014485\n",
      "Epoch: 1765 \tTraining Loss: 0.012927 \tValidation Loss: 0.014485\n",
      "Epoch: 1766 \tTraining Loss: 0.012926 \tValidation Loss: 0.014499\n",
      "Epoch: 1767 \tTraining Loss: 0.012908 \tValidation Loss: 0.014496\n",
      "Epoch: 1768 \tTraining Loss: 0.012894 \tValidation Loss: 0.014495\n",
      "Epoch: 1769 \tTraining Loss: 0.012901 \tValidation Loss: 0.014492\n",
      "Epoch: 1770 \tTraining Loss: 0.012886 \tValidation Loss: 0.014491\n",
      "Epoch: 1771 \tTraining Loss: 0.012873 \tValidation Loss: 0.014500\n",
      "Epoch: 1772 \tTraining Loss: 0.012899 \tValidation Loss: 0.014497\n",
      "Epoch: 1773 \tTraining Loss: 0.012910 \tValidation Loss: 0.014496\n",
      "Epoch: 1774 \tTraining Loss: 0.012902 \tValidation Loss: 0.014496\n",
      "Epoch: 1775 \tTraining Loss: 0.012887 \tValidation Loss: 0.014492\n",
      "Epoch: 1776 \tTraining Loss: 0.012889 \tValidation Loss: 0.014488\n",
      "Epoch: 1777 \tTraining Loss: 0.012896 \tValidation Loss: 0.014488\n",
      "Epoch: 1778 \tTraining Loss: 0.012878 \tValidation Loss: 0.014492\n",
      "Epoch: 1779 \tTraining Loss: 0.012892 \tValidation Loss: 0.014500\n",
      "Epoch: 1780 \tTraining Loss: 0.012894 \tValidation Loss: 0.014488\n",
      "Epoch: 1781 \tTraining Loss: 0.012906 \tValidation Loss: 0.014484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1782 \tTraining Loss: 0.012874 \tValidation Loss: 0.014498\n",
      "Epoch: 1783 \tTraining Loss: 0.012882 \tValidation Loss: 0.014498\n",
      "Epoch: 1784 \tTraining Loss: 0.012898 \tValidation Loss: 0.014495\n",
      "Epoch: 1785 \tTraining Loss: 0.012906 \tValidation Loss: 0.014499\n",
      "Epoch: 1786 \tTraining Loss: 0.012879 \tValidation Loss: 0.014494\n",
      "Epoch: 1787 \tTraining Loss: 0.012903 \tValidation Loss: 0.014494\n",
      "Epoch: 1788 \tTraining Loss: 0.012894 \tValidation Loss: 0.014493\n",
      "Epoch: 1789 \tTraining Loss: 0.012881 \tValidation Loss: 0.014505\n",
      "Epoch: 1790 \tTraining Loss: 0.012878 \tValidation Loss: 0.014489\n",
      "Epoch: 1791 \tTraining Loss: 0.012901 \tValidation Loss: 0.014495\n",
      "Epoch: 1792 \tTraining Loss: 0.012873 \tValidation Loss: 0.014496\n",
      "Epoch: 1793 \tTraining Loss: 0.012898 \tValidation Loss: 0.014498\n",
      "Epoch: 1794 \tTraining Loss: 0.012892 \tValidation Loss: 0.014505\n",
      "Epoch: 1795 \tTraining Loss: 0.012902 \tValidation Loss: 0.014499\n",
      "Epoch: 1796 \tTraining Loss: 0.012864 \tValidation Loss: 0.014493\n",
      "Epoch: 1797 \tTraining Loss: 0.012886 \tValidation Loss: 0.014506\n",
      "Epoch: 1798 \tTraining Loss: 0.012885 \tValidation Loss: 0.014498\n",
      "Epoch: 1799 \tTraining Loss: 0.012899 \tValidation Loss: 0.014500\n",
      "Epoch: 1800 \tTraining Loss: 0.012857 \tValidation Loss: 0.014499\n",
      "Epoch: 1801 \tTraining Loss: 0.012875 \tValidation Loss: 0.014496\n",
      "Epoch: 1802 \tTraining Loss: 0.012880 \tValidation Loss: 0.014506\n",
      "Epoch: 1803 \tTraining Loss: 0.012879 \tValidation Loss: 0.014494\n",
      "Epoch: 1804 \tTraining Loss: 0.012884 \tValidation Loss: 0.014496\n",
      "Epoch: 1805 \tTraining Loss: 0.012863 \tValidation Loss: 0.014495\n",
      "Epoch: 1806 \tTraining Loss: 0.012876 \tValidation Loss: 0.014494\n",
      "Epoch: 1807 \tTraining Loss: 0.012863 \tValidation Loss: 0.014501\n",
      "Epoch: 1808 \tTraining Loss: 0.012851 \tValidation Loss: 0.014499\n",
      "Epoch: 1809 \tTraining Loss: 0.012872 \tValidation Loss: 0.014509\n",
      "Epoch: 1810 \tTraining Loss: 0.012870 \tValidation Loss: 0.014503\n",
      "Epoch: 1811 \tTraining Loss: 0.012874 \tValidation Loss: 0.014500\n",
      "Epoch: 1812 \tTraining Loss: 0.012884 \tValidation Loss: 0.014504\n",
      "Epoch: 1813 \tTraining Loss: 0.012849 \tValidation Loss: 0.014506\n",
      "Epoch: 1814 \tTraining Loss: 0.012866 \tValidation Loss: 0.014508\n",
      "Epoch: 1815 \tTraining Loss: 0.012852 \tValidation Loss: 0.014501\n",
      "Epoch: 1816 \tTraining Loss: 0.012885 \tValidation Loss: 0.014513\n",
      "Epoch: 1817 \tTraining Loss: 0.012861 \tValidation Loss: 0.014500\n",
      "Epoch: 1818 \tTraining Loss: 0.012888 \tValidation Loss: 0.014498\n",
      "Epoch: 1819 \tTraining Loss: 0.012875 \tValidation Loss: 0.014518\n",
      "Epoch: 1820 \tTraining Loss: 0.012871 \tValidation Loss: 0.014506\n",
      "Epoch: 1821 \tTraining Loss: 0.012869 \tValidation Loss: 0.014511\n",
      "Epoch: 1822 \tTraining Loss: 0.012858 \tValidation Loss: 0.014503\n",
      "Epoch: 1823 \tTraining Loss: 0.012868 \tValidation Loss: 0.014503\n",
      "Epoch: 1824 \tTraining Loss: 0.012868 \tValidation Loss: 0.014500\n",
      "Epoch: 1825 \tTraining Loss: 0.012870 \tValidation Loss: 0.014510\n",
      "Epoch: 1826 \tTraining Loss: 0.012845 \tValidation Loss: 0.014506\n",
      "Epoch: 1827 \tTraining Loss: 0.012852 \tValidation Loss: 0.014501\n",
      "Epoch: 1828 \tTraining Loss: 0.012848 \tValidation Loss: 0.014507\n",
      "Epoch: 1829 \tTraining Loss: 0.012845 \tValidation Loss: 0.014522\n",
      "Epoch: 1830 \tTraining Loss: 0.012841 \tValidation Loss: 0.014504\n",
      "Epoch: 1831 \tTraining Loss: 0.012835 \tValidation Loss: 0.014503\n",
      "Epoch: 1832 \tTraining Loss: 0.012865 \tValidation Loss: 0.014505\n",
      "Epoch: 1833 \tTraining Loss: 0.012836 \tValidation Loss: 0.014504\n",
      "Epoch: 1834 \tTraining Loss: 0.012849 \tValidation Loss: 0.014504\n",
      "Epoch: 1835 \tTraining Loss: 0.012850 \tValidation Loss: 0.014503\n",
      "Epoch: 1836 \tTraining Loss: 0.012865 \tValidation Loss: 0.014500\n",
      "Epoch: 1837 \tTraining Loss: 0.012852 \tValidation Loss: 0.014507\n",
      "Epoch: 1838 \tTraining Loss: 0.012849 \tValidation Loss: 0.014505\n",
      "Epoch: 1839 \tTraining Loss: 0.012865 \tValidation Loss: 0.014501\n",
      "Epoch: 1840 \tTraining Loss: 0.012822 \tValidation Loss: 0.014496\n",
      "Epoch: 1841 \tTraining Loss: 0.012837 \tValidation Loss: 0.014516\n",
      "Epoch: 1842 \tTraining Loss: 0.012854 \tValidation Loss: 0.014508\n",
      "Epoch: 1843 \tTraining Loss: 0.012831 \tValidation Loss: 0.014509\n",
      "Epoch: 1844 \tTraining Loss: 0.012831 \tValidation Loss: 0.014508\n",
      "Epoch: 1845 \tTraining Loss: 0.012846 \tValidation Loss: 0.014516\n",
      "Epoch: 1846 \tTraining Loss: 0.012846 \tValidation Loss: 0.014509\n",
      "Epoch: 1847 \tTraining Loss: 0.012845 \tValidation Loss: 0.014506\n",
      "Epoch: 1848 \tTraining Loss: 0.012845 \tValidation Loss: 0.014506\n",
      "Epoch: 1849 \tTraining Loss: 0.012856 \tValidation Loss: 0.014509\n",
      "Epoch: 1850 \tTraining Loss: 0.012838 \tValidation Loss: 0.014505\n",
      "Epoch: 1851 \tTraining Loss: 0.012837 \tValidation Loss: 0.014507\n",
      "Epoch: 1852 \tTraining Loss: 0.012837 \tValidation Loss: 0.014509\n",
      "Epoch: 1853 \tTraining Loss: 0.012839 \tValidation Loss: 0.014508\n",
      "Epoch: 1854 \tTraining Loss: 0.012831 \tValidation Loss: 0.014511\n",
      "Epoch: 1855 \tTraining Loss: 0.012825 \tValidation Loss: 0.014503\n",
      "Epoch: 1856 \tTraining Loss: 0.012837 \tValidation Loss: 0.014497\n",
      "Epoch: 1857 \tTraining Loss: 0.012854 \tValidation Loss: 0.014514\n",
      "Epoch: 1858 \tTraining Loss: 0.012847 \tValidation Loss: 0.014511\n",
      "Epoch: 1859 \tTraining Loss: 0.012832 \tValidation Loss: 0.014500\n",
      "Epoch: 1860 \tTraining Loss: 0.012830 \tValidation Loss: 0.014512\n",
      "Epoch: 1861 \tTraining Loss: 0.012840 \tValidation Loss: 0.014514\n",
      "Epoch: 1862 \tTraining Loss: 0.012821 \tValidation Loss: 0.014513\n",
      "Epoch: 1863 \tTraining Loss: 0.012819 \tValidation Loss: 0.014509\n",
      "Epoch: 1864 \tTraining Loss: 0.012816 \tValidation Loss: 0.014511\n",
      "Epoch: 1865 \tTraining Loss: 0.012824 \tValidation Loss: 0.014505\n",
      "Epoch: 1866 \tTraining Loss: 0.012825 \tValidation Loss: 0.014521\n",
      "Epoch: 1867 \tTraining Loss: 0.012845 \tValidation Loss: 0.014513\n",
      "Epoch: 1868 \tTraining Loss: 0.012830 \tValidation Loss: 0.014503\n",
      "Epoch: 1869 \tTraining Loss: 0.012809 \tValidation Loss: 0.014513\n",
      "Epoch: 1870 \tTraining Loss: 0.012824 \tValidation Loss: 0.014511\n",
      "Epoch: 1871 \tTraining Loss: 0.012799 \tValidation Loss: 0.014504\n",
      "Epoch: 1872 \tTraining Loss: 0.012836 \tValidation Loss: 0.014510\n",
      "Epoch: 1873 \tTraining Loss: 0.012831 \tValidation Loss: 0.014517\n",
      "Epoch: 1874 \tTraining Loss: 0.012823 \tValidation Loss: 0.014507\n",
      "Epoch: 1875 \tTraining Loss: 0.012813 \tValidation Loss: 0.014512\n",
      "Epoch: 1876 \tTraining Loss: 0.012816 \tValidation Loss: 0.014512\n",
      "Epoch: 1877 \tTraining Loss: 0.012818 \tValidation Loss: 0.014518\n",
      "Epoch: 1878 \tTraining Loss: 0.012809 \tValidation Loss: 0.014521\n",
      "Epoch: 1879 \tTraining Loss: 0.012833 \tValidation Loss: 0.014515\n",
      "Epoch: 1880 \tTraining Loss: 0.012815 \tValidation Loss: 0.014522\n",
      "Epoch: 1881 \tTraining Loss: 0.012820 \tValidation Loss: 0.014520\n",
      "Epoch: 1882 \tTraining Loss: 0.012821 \tValidation Loss: 0.014514\n",
      "Epoch: 1883 \tTraining Loss: 0.012820 \tValidation Loss: 0.014513\n",
      "Epoch: 1884 \tTraining Loss: 0.012807 \tValidation Loss: 0.014508\n",
      "Epoch: 1885 \tTraining Loss: 0.012801 \tValidation Loss: 0.014515\n",
      "Epoch: 1886 \tTraining Loss: 0.012830 \tValidation Loss: 0.014531\n",
      "Epoch: 1887 \tTraining Loss: 0.012826 \tValidation Loss: 0.014507\n",
      "Epoch: 1888 \tTraining Loss: 0.012834 \tValidation Loss: 0.014512\n",
      "Epoch: 1889 \tTraining Loss: 0.012806 \tValidation Loss: 0.014519\n",
      "Epoch: 1890 \tTraining Loss: 0.012802 \tValidation Loss: 0.014514\n",
      "Epoch: 1891 \tTraining Loss: 0.012820 \tValidation Loss: 0.014514\n",
      "Epoch: 1892 \tTraining Loss: 0.012802 \tValidation Loss: 0.014508\n",
      "Epoch: 1893 \tTraining Loss: 0.012795 \tValidation Loss: 0.014516\n",
      "Epoch: 1894 \tTraining Loss: 0.012823 \tValidation Loss: 0.014521\n",
      "Epoch: 1895 \tTraining Loss: 0.012816 \tValidation Loss: 0.014524\n",
      "Epoch: 1896 \tTraining Loss: 0.012789 \tValidation Loss: 0.014505\n",
      "Epoch: 1897 \tTraining Loss: 0.012793 \tValidation Loss: 0.014521\n",
      "Epoch: 1898 \tTraining Loss: 0.012802 \tValidation Loss: 0.014521\n",
      "Epoch: 1899 \tTraining Loss: 0.012788 \tValidation Loss: 0.014523\n",
      "Epoch: 1900 \tTraining Loss: 0.012825 \tValidation Loss: 0.014513\n",
      "Epoch: 1901 \tTraining Loss: 0.012818 \tValidation Loss: 0.014521\n",
      "Epoch: 1902 \tTraining Loss: 0.012799 \tValidation Loss: 0.014522\n",
      "Epoch: 1903 \tTraining Loss: 0.012797 \tValidation Loss: 0.014515\n",
      "Epoch: 1904 \tTraining Loss: 0.012809 \tValidation Loss: 0.014516\n",
      "Epoch: 1905 \tTraining Loss: 0.012787 \tValidation Loss: 0.014531\n",
      "Epoch: 1906 \tTraining Loss: 0.012814 \tValidation Loss: 0.014527\n",
      "Epoch: 1907 \tTraining Loss: 0.012777 \tValidation Loss: 0.014519\n",
      "Epoch: 1908 \tTraining Loss: 0.012790 \tValidation Loss: 0.014518\n",
      "Epoch: 1909 \tTraining Loss: 0.012799 \tValidation Loss: 0.014527\n",
      "Epoch: 1910 \tTraining Loss: 0.012788 \tValidation Loss: 0.014522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1911 \tTraining Loss: 0.012800 \tValidation Loss: 0.014509\n",
      "Epoch: 1912 \tTraining Loss: 0.012809 \tValidation Loss: 0.014508\n",
      "Epoch: 1913 \tTraining Loss: 0.012784 \tValidation Loss: 0.014513\n",
      "Epoch: 1914 \tTraining Loss: 0.012778 \tValidation Loss: 0.014515\n",
      "Epoch: 1915 \tTraining Loss: 0.012801 \tValidation Loss: 0.014516\n",
      "Epoch: 1916 \tTraining Loss: 0.012792 \tValidation Loss: 0.014528\n",
      "Epoch: 1917 \tTraining Loss: 0.012785 \tValidation Loss: 0.014518\n",
      "Epoch: 1918 \tTraining Loss: 0.012787 \tValidation Loss: 0.014525\n",
      "Epoch: 1919 \tTraining Loss: 0.012803 \tValidation Loss: 0.014521\n",
      "Epoch: 1920 \tTraining Loss: 0.012793 \tValidation Loss: 0.014525\n",
      "Epoch: 1921 \tTraining Loss: 0.012776 \tValidation Loss: 0.014520\n",
      "Epoch: 1922 \tTraining Loss: 0.012799 \tValidation Loss: 0.014522\n",
      "Epoch: 1923 \tTraining Loss: 0.012792 \tValidation Loss: 0.014511\n",
      "Epoch: 1924 \tTraining Loss: 0.012777 \tValidation Loss: 0.014519\n",
      "Epoch: 1925 \tTraining Loss: 0.012766 \tValidation Loss: 0.014521\n",
      "Epoch: 1926 \tTraining Loss: 0.012786 \tValidation Loss: 0.014516\n",
      "Epoch: 1927 \tTraining Loss: 0.012787 \tValidation Loss: 0.014529\n",
      "Epoch: 1928 \tTraining Loss: 0.012792 \tValidation Loss: 0.014518\n",
      "Epoch: 1929 \tTraining Loss: 0.012786 \tValidation Loss: 0.014528\n",
      "Epoch: 1930 \tTraining Loss: 0.012789 \tValidation Loss: 0.014517\n",
      "Epoch: 1931 \tTraining Loss: 0.012773 \tValidation Loss: 0.014520\n",
      "Epoch: 1932 \tTraining Loss: 0.012779 \tValidation Loss: 0.014525\n",
      "Epoch: 1933 \tTraining Loss: 0.012760 \tValidation Loss: 0.014518\n",
      "Epoch: 1934 \tTraining Loss: 0.012771 \tValidation Loss: 0.014516\n",
      "Epoch: 1935 \tTraining Loss: 0.012760 \tValidation Loss: 0.014528\n",
      "Epoch: 1936 \tTraining Loss: 0.012781 \tValidation Loss: 0.014533\n",
      "Epoch: 1937 \tTraining Loss: 0.012778 \tValidation Loss: 0.014515\n",
      "Epoch: 1938 \tTraining Loss: 0.012773 \tValidation Loss: 0.014518\n",
      "Epoch: 1939 \tTraining Loss: 0.012769 \tValidation Loss: 0.014533\n",
      "Epoch: 1940 \tTraining Loss: 0.012784 \tValidation Loss: 0.014522\n",
      "Epoch: 1941 \tTraining Loss: 0.012755 \tValidation Loss: 0.014515\n",
      "Epoch: 1942 \tTraining Loss: 0.012772 \tValidation Loss: 0.014520\n",
      "Epoch: 1943 \tTraining Loss: 0.012782 \tValidation Loss: 0.014526\n",
      "Epoch: 1944 \tTraining Loss: 0.012766 \tValidation Loss: 0.014519\n",
      "Epoch: 1945 \tTraining Loss: 0.012770 \tValidation Loss: 0.014523\n",
      "Epoch: 1946 \tTraining Loss: 0.012769 \tValidation Loss: 0.014523\n",
      "Epoch: 1947 \tTraining Loss: 0.012749 \tValidation Loss: 0.014523\n",
      "Epoch: 1948 \tTraining Loss: 0.012791 \tValidation Loss: 0.014524\n",
      "Epoch: 1949 \tTraining Loss: 0.012746 \tValidation Loss: 0.014522\n",
      "Epoch: 1950 \tTraining Loss: 0.012775 \tValidation Loss: 0.014525\n",
      "Epoch: 1951 \tTraining Loss: 0.012754 \tValidation Loss: 0.014526\n",
      "Epoch: 1952 \tTraining Loss: 0.012762 \tValidation Loss: 0.014523\n",
      "Epoch: 1953 \tTraining Loss: 0.012783 \tValidation Loss: 0.014539\n",
      "Epoch: 1954 \tTraining Loss: 0.012774 \tValidation Loss: 0.014534\n",
      "Epoch: 1955 \tTraining Loss: 0.012789 \tValidation Loss: 0.014520\n",
      "Epoch: 1956 \tTraining Loss: 0.012771 \tValidation Loss: 0.014519\n",
      "Epoch: 1957 \tTraining Loss: 0.012760 \tValidation Loss: 0.014521\n",
      "Epoch: 1958 \tTraining Loss: 0.012762 \tValidation Loss: 0.014531\n",
      "Epoch: 1959 \tTraining Loss: 0.012762 \tValidation Loss: 0.014520\n",
      "Epoch: 1960 \tTraining Loss: 0.012766 \tValidation Loss: 0.014515\n",
      "Epoch: 1961 \tTraining Loss: 0.012785 \tValidation Loss: 0.014522\n",
      "Epoch: 1962 \tTraining Loss: 0.012767 \tValidation Loss: 0.014529\n",
      "Epoch: 1963 \tTraining Loss: 0.012762 \tValidation Loss: 0.014525\n",
      "Epoch: 1964 \tTraining Loss: 0.012758 \tValidation Loss: 0.014534\n",
      "Epoch: 1965 \tTraining Loss: 0.012774 \tValidation Loss: 0.014535\n",
      "Epoch: 1966 \tTraining Loss: 0.012755 \tValidation Loss: 0.014528\n",
      "Epoch: 1967 \tTraining Loss: 0.012737 \tValidation Loss: 0.014536\n",
      "Epoch: 1968 \tTraining Loss: 0.012766 \tValidation Loss: 0.014534\n",
      "Epoch: 1969 \tTraining Loss: 0.012747 \tValidation Loss: 0.014538\n",
      "Epoch: 1970 \tTraining Loss: 0.012738 \tValidation Loss: 0.014542\n",
      "Epoch: 1971 \tTraining Loss: 0.012749 \tValidation Loss: 0.014528\n",
      "Epoch: 1972 \tTraining Loss: 0.012758 \tValidation Loss: 0.014530\n",
      "Epoch: 1973 \tTraining Loss: 0.012752 \tValidation Loss: 0.014524\n",
      "Epoch: 1974 \tTraining Loss: 0.012739 \tValidation Loss: 0.014526\n",
      "Epoch: 1975 \tTraining Loss: 0.012770 \tValidation Loss: 0.014524\n",
      "Epoch: 1976 \tTraining Loss: 0.012739 \tValidation Loss: 0.014540\n",
      "Epoch: 1977 \tTraining Loss: 0.012758 \tValidation Loss: 0.014534\n",
      "Epoch: 1978 \tTraining Loss: 0.012742 \tValidation Loss: 0.014541\n",
      "Epoch: 1979 \tTraining Loss: 0.012730 \tValidation Loss: 0.014541\n",
      "Epoch: 1980 \tTraining Loss: 0.012736 \tValidation Loss: 0.014530\n",
      "Epoch: 1981 \tTraining Loss: 0.012745 \tValidation Loss: 0.014530\n",
      "Epoch: 1982 \tTraining Loss: 0.012735 \tValidation Loss: 0.014536\n",
      "Epoch: 1983 \tTraining Loss: 0.012737 \tValidation Loss: 0.014535\n",
      "Epoch: 1984 \tTraining Loss: 0.012715 \tValidation Loss: 0.014522\n",
      "Epoch: 1985 \tTraining Loss: 0.012725 \tValidation Loss: 0.014538\n",
      "Epoch: 1986 \tTraining Loss: 0.012759 \tValidation Loss: 0.014528\n",
      "Epoch: 1987 \tTraining Loss: 0.012761 \tValidation Loss: 0.014539\n",
      "Epoch: 1988 \tTraining Loss: 0.012751 \tValidation Loss: 0.014528\n",
      "Epoch: 1989 \tTraining Loss: 0.012714 \tValidation Loss: 0.014542\n",
      "Epoch: 1990 \tTraining Loss: 0.012747 \tValidation Loss: 0.014524\n",
      "Epoch: 1991 \tTraining Loss: 0.012732 \tValidation Loss: 0.014541\n",
      "Epoch: 1992 \tTraining Loss: 0.012729 \tValidation Loss: 0.014539\n",
      "Epoch: 1993 \tTraining Loss: 0.012737 \tValidation Loss: 0.014537\n",
      "Epoch: 1994 \tTraining Loss: 0.012741 \tValidation Loss: 0.014535\n",
      "Epoch: 1995 \tTraining Loss: 0.012740 \tValidation Loss: 0.014538\n",
      "Epoch: 1996 \tTraining Loss: 0.012713 \tValidation Loss: 0.014544\n",
      "Epoch: 1997 \tTraining Loss: 0.012756 \tValidation Loss: 0.014544\n",
      "Epoch: 1998 \tTraining Loss: 0.012729 \tValidation Loss: 0.014542\n",
      "Epoch: 1999 \tTraining Loss: 0.012722 \tValidation Loss: 0.014535\n",
      "Epoch: 2000 \tTraining Loss: 0.012741 \tValidation Loss: 0.014535\n"
     ]
    }
   ],
   "source": [
    "# create a complete CNN\n",
    "# trained pn v1\n",
    "\n",
    "print(model)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 2000\n",
    "# model = model.float()\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            data, target = data.float(), target.float()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    \n",
    "    # At completion of epoch\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), '2020_ML_Assisted_QSE.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "cb2fcdd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'V1')"
      ]
     },
     "execution_count": 773,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log10(train_losses[10:500]), label='Training loss')\n",
    "plt.plot(np.log10(valid_losses[10:500]), label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.title('V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "03707bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('2020_ML_Assisted_QSE.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "b284495f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.014362\n",
      "Loss per Batch: 0.014204\n",
      "Loss per Batch: 0.013942\n",
      "Loss per Batch: 0.014032\n",
      "Loss per Batch: 0.015123\n",
      "Loss per Batch: 0.014069\n",
      "Loss per Batch: 0.013598\n",
      "Loss per Batch: 0.014212\n",
      "Loss per Batch: 0.013726\n",
      "Loss per Batch: 0.013772\n",
      "Loss per Batch: 0.013883\n",
      "Loss per Batch: 0.014951\n",
      "Loss per Batch: 0.013841\n",
      "Loss per Batch: 0.014987\n",
      "Loss per Batch: 0.014456\n",
      "Loss per Batch: 0.014184\n",
      "Loss per Batch: 0.013720\n",
      "Loss per Batch: 0.013625\n",
      "Loss per Batch: 0.014392\n",
      "Loss per Batch: 0.013919\n",
      "Final Test Loss: 0.014150\n",
      "Average Fidelity: 0.861356\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v1\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(pauli, aa)\n",
    "                rho2 = state_recon_bloch_vec(pauli, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "b3fc413d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.015311\n",
      "Loss per Batch: 0.015397\n",
      "Loss per Batch: 0.016250\n",
      "Loss per Batch: 0.015715\n",
      "Loss per Batch: 0.015962\n",
      "Loss per Batch: 0.015560\n",
      "Loss per Batch: 0.015338\n",
      "Loss per Batch: 0.015500\n",
      "Loss per Batch: 0.014302\n",
      "Loss per Batch: 0.015082\n",
      "Loss per Batch: 0.015496\n",
      "Loss per Batch: 0.016135\n",
      "Loss per Batch: 0.016242\n",
      "Loss per Batch: 0.014893\n",
      "Loss per Batch: 0.015495\n",
      "Loss per Batch: 0.015119\n",
      "Loss per Batch: 0.015461\n",
      "Loss per Batch: 0.015142\n",
      "Loss per Batch: 0.014862\n",
      "Loss per Batch: 0.016090\n",
      "Final Test Loss: 0.015468\n",
      "Average Fidelity: 0.851827\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v2\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(pauli, aa)\n",
    "                rho2 = state_recon_bloch_vec(pauli, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "044b2e1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.016293\n",
      "Loss per Batch: 0.015184\n",
      "Loss per Batch: 0.015640\n",
      "Loss per Batch: 0.016611\n",
      "Loss per Batch: 0.016212\n",
      "Loss per Batch: 0.016101\n",
      "Loss per Batch: 0.015034\n",
      "Loss per Batch: 0.016354\n",
      "Loss per Batch: 0.016093\n",
      "Loss per Batch: 0.016125\n",
      "Loss per Batch: 0.016205\n",
      "Loss per Batch: 0.015933\n",
      "Loss per Batch: 0.016253\n",
      "Loss per Batch: 0.015574\n",
      "Loss per Batch: 0.016389\n",
      "Loss per Batch: 0.016141\n",
      "Loss per Batch: 0.015487\n",
      "Loss per Batch: 0.015896\n",
      "Loss per Batch: 0.015825\n",
      "Loss per Batch: 0.016370\n",
      "Final Test Loss: 0.015986\n",
      "Average Fidelity: 0.847331\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v3\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(pauli, aa)\n",
    "                rho2 = state_recon_bloch_vec(pauli, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "87228f00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.016765\n",
      "Loss per Batch: 0.016935\n",
      "Loss per Batch: 0.016017\n",
      "Loss per Batch: 0.016673\n",
      "Loss per Batch: 0.016532\n",
      "Loss per Batch: 0.016408\n",
      "Loss per Batch: 0.016440\n",
      "Loss per Batch: 0.015260\n",
      "Loss per Batch: 0.015649\n",
      "Loss per Batch: 0.016745\n",
      "Loss per Batch: 0.016219\n",
      "Loss per Batch: 0.015702\n",
      "Loss per Batch: 0.016194\n",
      "Loss per Batch: 0.016466\n",
      "Loss per Batch: 0.016826\n",
      "Loss per Batch: 0.015507\n",
      "Loss per Batch: 0.015897\n",
      "Loss per Batch: 0.016804\n",
      "Loss per Batch: 0.016682\n",
      "Loss per Batch: 0.016165\n",
      "Final Test Loss: 0.016294\n",
      "Average Fidelity: 0.845162\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v4\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(pauli, aa)\n",
    "                rho2 = state_recon_bloch_vec(pauli, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "528a6fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per Batch: 0.011168\n",
      "Loss per Batch: 0.011683\n",
      "Loss per Batch: 0.011419\n",
      "Loss per Batch: 0.011394\n",
      "Loss per Batch: 0.010965\n",
      "Loss per Batch: 0.010842\n",
      "Loss per Batch: 0.011105\n",
      "Loss per Batch: 0.011069\n",
      "Loss per Batch: 0.011301\n",
      "Loss per Batch: 0.010599\n",
      "Loss per Batch: 0.011074\n",
      "Loss per Batch: 0.011716\n",
      "Loss per Batch: 0.010902\n",
      "Loss per Batch: 0.010554\n",
      "Loss per Batch: 0.010256\n",
      "Loss per Batch: 0.010956\n",
      "Loss per Batch: 0.011623\n",
      "Loss per Batch: 0.010758\n",
      "Loss per Batch: 0.010478\n",
      "Loss per Batch: 0.011403\n",
      "Final Test Loss: 0.011063\n",
      "Average Fidelity: 0.885972\n"
     ]
    }
   ],
   "source": [
    "# track test loss on v0\n",
    "test_loss = 0.0\n",
    "fidelity = 0.0\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        with torch.no_grad():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            \n",
    "            for ij in range(batch_size):\n",
    "                aa = np.array(output.cpu().numpy()[ij])\n",
    "                bb = np.array(target.cpu().numpy()[ij])\n",
    "                rho1 = state_recon_bloch_vec(pauli, aa)\n",
    "                rho2 = state_recon_bloch_vec(pauli, bb)\n",
    "                rho1 = eig_val_corr(rho1)\n",
    "                rho2 = eig_val_corr(rho2)\n",
    "                fidelity += qt.fidelity(qt.Qobj(rho1), qt.Qobj(rho2))\n",
    "                \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            test_loss += loss.item()*data.size(0)\n",
    "    print('Loss per Batch: {:.6f}'.format(loss))\n",
    "    \n",
    "# calculate average losses\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "fidelity = fidelity/len(test_loader.sampler)\n",
    "\n",
    "# print training/validation statistics \n",
    "print('Final Test Loss: {:.6f}'.format(test_loss))\n",
    "print('Average Fidelity: {:.6f}'.format(fidelity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae66c9c",
   "metadata": {},
   "source": [
    "Trained on v1 for 2000 epochs\n",
    "\n",
    "#v0    \n",
    "Final Test Loss: 0.011063    \n",
    "Average Fidelity: 0.885972\n",
    "\n",
    "#v1   \n",
    "Final Test Loss: 0.014150   \n",
    "Average Fidelity: 0.861356\n",
    "    \n",
    "#v2   \n",
    "Final Test Loss: 0.015468   \n",
    "Average Fidelity: 0.851827\n",
    "    \n",
    "#v3   \n",
    "Final Test Loss: 0.015986   \n",
    "Average Fidelity: 0.847331\n",
    "    \n",
    "#v4   \n",
    "Final Test Loss: 0.016294  \n",
    "Average Fidelity: 0.845162"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
